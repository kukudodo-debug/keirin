import streamlit as st
import pandas as pd
import polars as pl  # Polarsã‚’æ˜ç¤ºçš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import os
import sqlite3
import logic_v2
import db_utils
import scraper # Import Scraper Module
from datetime import datetime, timedelta
import importlib

# Force Reload Modules ensuring fixes are applied without restart
importlib.reload(scraper)
importlib.reload(db_utils)
import logic_v2
importlib.reload(logic_v2)

# ãƒ­ã‚¸ãƒƒã‚¯ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆPolarsãƒ™ãƒ¼ã‚¹ï¼‰ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from advanced_logic import KeirinLogicEngine, apply_advanced_logic

# ==========================================
# ãƒšãƒ¼ã‚¸è¨­å®š
# ==========================================
st.set_page_config(
    page_title="ç«¶è¼ªDeepDive | ãƒ‡ãƒ¼ã‚¿ã§ç†±ç‹‚ã‚’ã¤ã‹ã‚!",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSSã‚¹ã‚¿ã‚¤ãƒ« (Antigravityãƒ†ãƒ¼ãƒ)
st.markdown("""
<style>
    .big-font { font-size: 20px !important; font-weight: bold; }
    .win-rank-1 { background-color: #FFD700; color: black; padding: 2px 5px; border-radius: 3px; }
    .jimoto-tag { background-color: #ffcccc; color: red; font-weight: bold; padding: 2px; border: 1px solid red; border-radius: 4px; }
    .special-badge { background-color: #663399; color: white; padding: 2px 6px; border-radius: 4px; font-weight: bold; font-size: 0.8em; }
    .stDataFrame { border: 1px solid #e0e0e0; border-radius: 5px; }
</style>
""", unsafe_allow_html=True)

# ==========================================
# ãƒ­ã‚¸ãƒƒã‚¯ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ– (ã‚­ãƒ£ãƒƒã‚·ãƒ¥)
# ==========================================
@st.cache_resource
def get_logic_engine():
    """
    Polarsãƒ™ãƒ¼ã‚¹ã®åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–
    ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å ´æ‰€ã‚’æŒ‡å®š (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: data/files)
    """
    # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¦è¿”ã™ã€‚ã“ã®é–¢æ•°ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ãŸã‚ã€
    # ã‚¨ãƒ³ã‚¸ãƒ³å†…ã®é‡ã„ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã¯ä¸€åº¦ã ã‘ã§æ¸ˆã‚€ã€‚
    print("Logic Engine Loading... (Cache Reset v2)")
    return KeirinLogicEngine(data_root_dir='logic_data')

# Cache Clear Trigger (Update this when Logic Class changes)
# version: 2025-12-28-v2

engine = get_logic_engine()

# ==========================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ & DBæ¥ç¶š
# ==========================================
# ==========================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼æ©Ÿèƒ½å®Ÿè£… (æ¤œç´¢ & DB)
# ==========================================

# ==========================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼æ©Ÿèƒ½å®Ÿè£… (æ¤œç´¢ & DB)
# ==========================================

# ==========================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼æ©Ÿèƒ½å®Ÿè£… (ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° & æ¤œç´¢ & DB)
# ==========================================

st.sidebar.image("assets/deepdive_logo.png", use_container_width=True)

# ----------------------------
# 0. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (å‡ºèµ°è¡¨å–å¾—)
# ----------------------------
st.sidebar.markdown("---")
st.sidebar.header("ğŸ“¡ å‡ºèµ°è¡¨å–å¾—")
with st.sidebar.expander("Webã‹ã‚‰å–å¾— (æ¨å¥¨)", expanded=True):
    # Venue List
    active_venues = [
        "å‡½é¤¨","é’æ£®","ã„ã‚ãå¹³","å¼¥å½¦","å‰æ©‹","å–æ‰‹","å®‡éƒ½å®®","å¤§å®®","è¥¿æ­¦åœ’","äº¬ç‹é–£","ç«‹å·","æ¾æˆ¸","å·å´","å¹³å¡š","å°ç”°åŸ","ä¼Šæ±","é™å²¡","åå¤å±‹","å²é˜œ","å¤§å£","è±Šæ©‹","å¯Œå±±","æ¾é˜ª","å››æ—¥å¸‚","ç¦äº•","å¥ˆè‰¯","å‘æ—¥ç”º","å’Œæ­Œå±±","å²¸å’Œç”°","ç‰é‡","åºƒå³¶","é˜²åºœ","é«˜æ¾","å°æ¾å³¶","é«˜çŸ¥","æ¾å±±","å°å€‰","ä¹…ç•™ç±³","æ­¦é›„","ä½ä¸–ä¿","åˆ¥åºœ","ç†Šæœ¬"
    ]
    
    # Date Input (Range)
    # Default: Today
    today = datetime.today()
    target_dates = st.sidebar.date_input("é–‹å‚¬æœŸé–“", [today, today]) # Default tuple
    
    # Multi-Select Venue
    target_venues = st.sidebar.multiselect("ç«¶è¼ªå ´ã‚’é¸æŠ", active_venues, default=["å¹³å¡š", "æ¾æˆ¸"])
    
    if st.sidebar.button("ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹", type="primary"):
        if not target_venues:
            st.sidebar.error("ç«¶è¼ªå ´ã‚’é¸æŠã—ã¦ãã ã•ã„")
        else:
            # Handle Date Range
            if isinstance(target_dates, (list, tuple)):
                if len(target_dates) == 2:
                    s_date, e_date = target_dates
                elif len(target_dates) == 1:
                    s_date = target_dates[0]
                    e_date = target_dates[0]
                else: 
                     s_date = today
                     e_date = today
            else:
                 s_date = target_dates
                 e_date = target_dates
            
            s_str = s_date.strftime("%Y-%m-%d")
            e_str = e_date.strftime("%Y-%m-%d")
            
            st.sidebar.info(f"{s_str} ï½ {e_str} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
            
            # Progress Bar logic is tricky within sidebar, just spinner
            with st.spinner("K-Dreamsã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã„ã¾ã™..."):
                try:
                    all_scraped_data = []
                    
                    # Store in session state structure compatible with uploaded_files
                    # List of dicts: {'label':..., 'df':..., 'meta':...}
                    
                    # Iterate selected venues
                    # Using scraper.fetch_race_data which supports multi-threading for races, 
                    # but we are calling it per venue.
                    for v_name in target_venues:
                        # fetch_race_data returns list of dicts [{'df':..., 'meta':...}]
                        # Note: exclude_ids is optional
                        # Force max_workers=1 to prevent data corruption bug
                        venue_results = scraper.fetch_race_data(v_name, s_str, e_str, max_workers=1)
                        if venue_results:
                            for res in venue_results:
                                df_s = res['df']
                                meta_s = res['meta']
                                refund_s = res.get('refund', {}) # Capture refund data

                                # Format Label: "Place Date 11R (Web)"
                                date_label = meta_s.get('date', '')
                                r_num = meta_s.get('race_num', '?')
                                label = f"{v_name} {date_label} {r_num}R (Web)"
                                
                                # Add to collection
                                all_scraped_data.append({
                                    'label': label,
                                    'df': df_s,
                                    'meta': meta_s,
                                    'refund': refund_s, # Store for DB Save
                                    'filename': f"scraped_{v_name}_{date_label}_{r_num}R.html",
                                    'sort_key': (v_name, int(r_num) if r_num.isdigit() else 0)
                                })
                    
                    if all_scraped_data:
                        st.session_state['scraped_races'] = all_scraped_data
                        st.sidebar.success(f"{len(all_scraped_data)}ãƒ¬ãƒ¼ã‚¹å–å¾—æˆåŠŸï¼")
                        st.rerun() 
                    else:
                        st.sidebar.warning("é–‹å‚¬ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ (ä¸­æ­¢ãƒ»é †å»¶ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™)")
                        
                except Exception as e:
                    st.sidebar.error(f"å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

    # Load from DB Feature (User Request)
    st.sidebar.markdown("---")
    with st.sidebar.expander("ğŸ“‚ ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­è¾¼", expanded=False):
        load_date = st.date_input("é–‹å‚¬æ—¥é¸æŠ", datetime.today())
        

             
        d_str = load_date.strftime("%Yå¹´%mæœˆ%dæ—¥")
        
        # New: Venue Check (User Request 2025-12-29)
        d_str = load_date.strftime("%Yå¹´%mæœˆ%dæ—¥")
        
        if st.button("é–‹å‚¬å ´ã‚’ç¢ºèª"):
             with st.spinner("DBç¢ºèªä¸­..."):
                 venues = db_utils.get_available_venues(d_str)
                 if venues:
                     st.session_state['db_found_venues'] = venues
                     st.session_state['db_check_date'] = d_str
                 else:
                     st.warning(f"{d_str} ã®ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“")
                     if 'db_found_venues' in st.session_state: del st.session_state['db_found_venues']

        # Show MultiSelect if venues found
        target_venues = None
        if 'db_found_venues' in st.session_state and st.session_state.get('db_check_date') == d_str:
             all_v = st.session_state['db_found_venues']
             target_venues = st.multiselect("èª­ã¿è¾¼ã‚€å ´ã‚’é¸æŠ", all_v, default=all_v)

        if st.button("DBã‹ã‚‰èª­ã¿è¾¼ã‚€"):
            with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢ä¸­..."):
                try:
                    # Pass filtered venues if selected
                    loaded_data = db_utils.load_races_as_batch(d_str, target_venues=target_venues)
                    
                    if loaded_data:
                        # Sanitize Loaded Data
                        for r_dat in loaded_data:
                            if 'df' in r_dat and not r_dat['df'].empty:
                                if 'ç«¶èµ°å¾—ç‚¹' in r_dat['df'].columns:
                                    def clean_sc(x):
                                        s = str(x).strip()
                                        import re
                                        # Robust extraction: 2-3 digits, optional dot, optional 1-2 decimals
                                        m = re.search(r'(\d{2,3}(\.\d{1,2})?)', s)
                                        try:
                                            return float(m.group(1)) if m else 0.0
                                        except: return 0.0
                                    r_dat['df']['ç«¶èµ°å¾—ç‚¹'] = r_dat['df']['ç«¶èµ°å¾—ç‚¹'].apply(clean_sc)
                                if 'è»Šç•ª' in r_dat['df'].columns:
                                     r_dat['df']['è»Šç•ª'] = pd.to_numeric(r_dat['df']['è»Šç•ª'], errors='coerce').fillna(0).astype(int)

                        st.session_state['scraped_races'] = loaded_data
                        st.sidebar.success(f"{len(loaded_data)}ãƒ¬ãƒ¼ã‚¹ èª­ã¿è¾¼ã¿æˆåŠŸï¼")
                        st.rerun()
                    else:
                        st.sidebar.warning(f"{d_str} ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                except Exception as ex:
                    st.sidebar.error(f"èª­è¾¼ã‚¨ãƒ©ãƒ¼: {ex}")

    # Save to DB Feature
    if 'scraped_races' in st.session_state and st.session_state['scraped_races']:
        st.sidebar.markdown("---")
        if st.session_state['scraped_races']:
            st.info(f"å–å¾—æ¸ˆã¿ãƒ¬ãƒ¼ã‚¹æ•°: {len(st.session_state['scraped_races'])}ä»¶")
            
            # Force Overwrite Option (User Request)
            overwrite_db = st.checkbox("æ—¢ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã£ã¦ã‚‚ä¸Šæ›¸ãã™ã‚‹ (Force Overwrite)", value=False)
            
            # Sequential Save Button
            if st.button(f"å–å¾—ã—ãŸ{len(st.session_state['scraped_races'])}ãƒ¬ãƒ¼ã‚¹ã‚’DBä¿å­˜"):
                # Sequential Save Logic
                total_races = len(st.session_state["scraped_races"])
                progress_bar = st.progress(0)
                status_text = st.empty()
                success_count = 0
                error_count = 0
                
                msg_container = st.sidebar.container()
                
                with st.spinner("DBã«1ä»¶ãšã¤ä¿å­˜ä¸­..."):
                    for i, r_data in enumerate(st.session_state["scraped_races"]):
                        label = r_data.get("label", f"Race {i+1}")
                        status_text.text(f"ä¿å­˜ä¸­ ({i+1}/{total_races}): {label}")
                        try:
                            if "df" in r_data and not r_data["df"].empty:
                                df_chk = r_data["df"]
                                if "è»Šç•ª" in df_chk.columns:
                                    df_chk["è»Šç•ª"] = pd.to_numeric(df_chk["è»Šç•ª"], errors="coerce").fillna(0).astype(int)
                                for col in ["æœŸåˆ¥", "å¹´é½¢", "æ ç•ª"]:
                                    if col in df_chk.columns:
                                        df_chk[col] = pd.to_numeric(df_chk[col], errors="coerce").fillna(0).astype(int)
                                if "è„šè³ª" in df_chk.columns:
                                    df_chk["è„šè³ª"] = df_chk["è„šè³ª"].fillna("").astype(str)
                                if "ç«¶èµ°å¾—ç‚¹" in df_chk.columns:
                                    df_chk["ç«¶èµ°å¾—ç‚¹"] = pd.to_numeric(df_chk["ç«¶èµ°å¾—ç‚¹"], errors="coerce").fillna(0.0)
                            c, msg = db_utils.save_race_data([r_data], overwrite=overwrite_db)
                            if c > 0: success_count += c
                        except Exception as e:
                            error_count += 1
                            msg_container.error(f"Error {label}: {e}")
                        progress_bar.progress((i + 1) / total_races)
                status_text.text("å®Œäº†ï¼")
                progress_bar.progress(1.0)
                if success_count > 0: st.sidebar.success(f"{success_count}ãƒ¬ãƒ¼ã‚¹ ä¿å­˜å®Œäº†ï¼")
                elif error_count > 0: st.sidebar.warning(f"{error_count}ä»¶ã®ã‚¨ãƒ©ãƒ¼ã‚ã‚Š")
                else: st.sidebar.info("æ–°è¦ä¿å­˜ãªã—")
           
        # Fallback: Load from Memory (User Request)
        if st.session_state['scraped_races']:
             st.sidebar.markdown("---")
             st.sidebar.markdown("**ãŠå›°ã‚Šã®å ´åˆ:**")
             if st.sidebar.button("ç›´å‰ã«å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º (DBä»‹ã•ãš)"):
                 st.session_state['scraped_races'] = st.session_state['scraped_races'] # Trigger rerun logic?
                 # Actually, logic checks 'scraped_races'. Simple rerun might do it.
                 # But we need to ensure the main area renders it.
                 # Main area renders `st.session_state['scraped_races']` if present.
                 st.sidebar.success("ç›´å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¾ã™ï¼")
                 st.rerun()

    # --- Batch Prediction Button (New Feature) ---
    if 'scraped_races' in st.session_state and st.session_state['scraped_races']:
        st.sidebar.markdown("---")
        if st.sidebar.button("ğŸš€ å…¨ãƒ¬ãƒ¼ã‚¹äºˆæƒ³ï¼†ä¿å­˜ (æ¨™æº–è©³ç´°)"):
            r_list = st.session_state['scraped_races']
            total_r = len(r_list)
            
            # Progress UI
            prog_bar = st.sidebar.progress(0)
            status_txt = st.sidebar.empty()
            saved_count = 0
            
            for i, r_dat in enumerate(r_list):
                if 'df' not in r_dat or r_dat['df'].empty: continue
                
                df_race = r_dat['df']
                meta = r_dat['meta']
                place_name = meta.get('place')
                race_num = meta.get('race_num')
                
                status_txt.text(f"äºˆæƒ³ä¸­ ({i+1}/{total_r}): {place_name} {race_num}R")
                
                try:
                    # 1. Scoring (Full Pipeline)
                    # Support Date/Place if missing
                    if 'ç«¶è¼ªå ´' not in df_race.columns and place_name: df_race['ç«¶è¼ªå ´'] = place_name
                    if 'æ—¥ä»˜' not in df_race.columns and meta.get('date'): df_race['æ—¥ä»˜'] = meta.get('date')
                    if 'ãƒ¬ãƒ¼ã‚¹ç•ªå·' not in df_race.columns and race_num: df_race['ãƒ¬ãƒ¼ã‚¹ç•ªå·'] = race_num

                    # Clean Score (Robust)
                    if 'ç«¶èµ°å¾—ç‚¹' in df_race.columns:
                        def clean_input_score_batch(x):
                            s = str(x).strip()
                            import re
                            m = re.search(r'(\d{2,3}(\.\d{1,2})?)', s)
                            try: return float(m.group(1)) if m else 0.0
                            except: return 0.0
                        df_race['ç«¶èµ°å¾—ç‚¹'] = df_race['ç«¶èµ°å¾—ç‚¹'].apply(clean_input_score_batch)

                    if 'è»Šç•ª' in df_race.columns:
                         df_race['è»Šç•ª'] = pd.to_numeric(df_race['è»Šç•ª'], errors='coerce').fillna(0).astype(int)

                    # Full Features
                    df_target = db_utils.run_global_features(df_race)
                    df_target = db_utils.run_race_features(df_target)
                    
                    # Use AI Logic (V3) for Unified Prediction
                    df_scored = logic_v2.calculate_ai_score(df_target)
                    
                    # Final Score uses ai_score from classic (no separate bonus)
                    df_scored['final_score'] = pd.to_numeric(df_scored.get('ai_score', 0), errors='coerce').fillna(0.0)
                    df_scored['ai_bonus'] = 0.0

                    # 2. Strategy (AI Logic V3)
                    strategy_data = logic_v2.generate_betting_strategy(df_scored, score_col='final_score')
                    
                    # 3. Check for Suji-Fix (æ¿€ç†±) using same check (simplified)
                    race_type_for_exclusion = strategy_data.get('type', 'standard')
                    
                    # Skip saving suji_fix (æ¿€ç†±) races
                    if race_type_for_exclusion == 'suji_fix':
                        print(f"Skipping suji_fix race: {place_name} {race_num}R")
                        continue
                    
                    # 4. Save
                    d_raw = meta.get('date', datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'))
                    # Clean Date
                    d_clean = d_raw.replace('-', 'å¹´').replace('/', 'å¹´')
                    if 'å¹´' not in d_clean:
                         try: d_clean = datetime.strptime(d_raw, "%Y-%m-%d").strftime("%Yå¹´%mæœˆ%dæ—¥")
                         except: pass
                    
                    # Fix 1RR issue
                    r_num_str = str(race_num)
                    if not r_num_str.endswith('R'):
                        r_num_str += 'R'
                    
                    # Generate Hash ID
                    import hashlib
                    raw_str = f"{d_clean}{place_name}{r_num_str}"
                    race_id = hashlib.md5(raw_str.encode()).hexdigest()

                    st_title = strategy_data.get('title', 'æ¨™æº–')
                    st_reason = strategy_data.get('reason', '')
                    
                    pred_data = {
                        "race_id": race_id,
                        "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        "place": place_name,
                        "race_num": r_num_str,
                        "date": d_clean,
                        "prediction_text": f"ã€{st_title}ã€‘{st_reason} (ä¸€æ‹¬)",
                        "tickets": strategy_data.get('tickets', []),
                        "structured_bets": strategy_data.get('structured_bets', []),  # Added for stats calc
                        "strategy_title": st_title,
                        "strategy_type": "classic", # Changed to Classic
                        "race_type": strategy_data.get('type', 'standard'),
                        "ai_indices": df_scored[['è»Šç•ª', 'final_score', 'é¸æ‰‹å', 'ai_tag']].to_dict('records') if 'final_score' in df_scored.columns else []
                    }
                    
                    if db_utils.save_prediction(pred_data):
                        saved_count += 1
                        
                except Exception as e:
                    print(f"Batch Error {place_name} {race_num}R: {e}")
                    
                prog_bar.progress((i + 1) / total_r)
            
            status_txt.text("å®Œäº†!")
            st.sidebar.success(f"âœ… {saved_count} ãƒ¬ãƒ¼ã‚¹ã®äºˆæƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")

# 1. éå»ãƒ¬ãƒ¼ã‚¹æ¤œç´¢ (DB)

if os.path.exists(db_utils.DB_PATH):
    # A. Search Scope
    with st.sidebar.expander("1. æ¤œç´¢å¯¾è±¡ãƒ»æœŸé–“", expanded=True):
        # Define Special Categories
        ALL_PLACES = "å…¨ç«¶è¼ªå ´"
        BANK_33 = "33ãƒãƒ³ã‚¯"
        BANK_500 = "500ãƒãƒ³ã‚¯"
        
        # Lists
        list_33 = ["å‰æ©‹", "æ¾æˆ¸", "å°ç”°åŸ", "ä¼Šæ±", "å¥ˆè‰¯", "é˜²åºœ", "å¯Œå±±"]
        list_500 = ["å®‡éƒ½å®®", "å¤§å®®", "é«˜çŸ¥"]
        
        # Mix options
        base_opts = [ALL_PLACES, BANK_33, BANK_500]
        venue_opts = base_opts + list(db_utils.TRACK_PREFECTURE_MAP.keys())
        
        venue_sb = st.selectbox("ç«¶è¼ªå ´", venue_opts, index=0, key="sb_venue")
        
        # Year Multiselect (2016-2025)
        years = list(range(2016, 2026))
        selected_years = st.multiselect("å¯¾è±¡å¹´åº¦", years, default=[2023, 2024, 2025], key="sb_years")
    
    # B. Player & Line Filters
    with st.sidebar.expander("2. é¸æ‰‹ãƒ»ãƒ©ã‚¤ãƒ³æ¡ä»¶", expanded=False):
        # Player Name
        search_name = st.text_input("é¸æ‰‹å (éƒ¨åˆ†ä¸€è‡´)", key="sb_name")
        
        # Line Info
        f_longest = st.checkbox("æœ€é•·ãƒ©ã‚¤ãƒ³ã®é¸æ‰‹ã®ã¿", key="sb_longest")
        
        # Line Length Slider
        # 1 (Tanki) to 5+
        f_line_len = st.slider("ãƒ©ã‚¤ãƒ³é•· (0=æŒ‡å®šãªã—)", 0, 5, 0, key="sb_len")
        
        # Line Position
        f_line_pos = st.slider("ãƒ©ã‚¤ãƒ³å†…ä½ç½® (0=æŒ‡å®šãªã—)", 0, 5, 0, key="sb_pos")
        
        # Line Strength (Head/Second)
        # Assuming db_utils has 'line_strength_head' (Str) -> "å¼·", "ä¸­", "å¼±"
        st.caption("ãƒ©ã‚¤ãƒ³å¼·åº¦åˆ¤å®š")
        f_str_head = st.multiselect("å…ˆè¡Œå¼·åº¦", ["å¼·", "ä¸­", "å¼±", "ç„¡"], default=[], key="sb_str_h")
        f_str_sec  = st.multiselect("ç•ªæ‰‹å¼·åº¦", ["å¼·", "ä¸­", "å¼±", "ç„¡"], default=[], key="sb_str_s")
        
        f_jimoto = st.checkbox("åœ°å…ƒé¸æ‰‹ã®ã¿ (Home)", key="sb_jimoto")

    # C. Tactic & Ability Filters
    with st.sidebar.expander("3. æˆ¦æ³•ãƒ»èƒ½åŠ›å€¤", expanded=False):
        # "Most in Race" Flags (is_top_nige, etc.)
        st.caption("ãƒ¬ãƒ¼ã‚¹å†…No.1 (æœ€å¤§å€¤ã‚’æŒã¤é¸æ‰‹)")
        c1, c2 = st.columns(2)
        f_top_nige = c1.checkbox("é€ƒã’æœ€å¤š", key="sb_t_nige")
        f_top_maku = c2.checkbox("æ²ã‚Šæœ€å¤š", key="sb_t_maku")
        f_top_sashi = c1.checkbox("å·®ã—æœ€å¤š", key="sb_t_sashi")
        
        # Fav Tactic
        st.caption("å¾—æ„æˆ¦æ³• (åŸºæœ¬æˆ¦æ³•)")
        f_tactics = st.multiselect("æˆ¦æ³•ã‚¿ã‚¤ãƒ—", ["é€ƒ", "æ²", "å·®", "ãƒ"], default=[], key="sb_tac")

    # D. Action
    if st.sidebar.button("è©³ç´°æ¤œç´¢å®Ÿè¡Œ", type="primary"):
        if not selected_years:
            st.sidebar.error("å¹´åº¦ã‚’é¸æŠã—ã¦ãã ã•ã„")
        else:
            try:
                # 1. Calc Date Range from Years
                min_y = min(selected_years)
                max_y = max(selected_years)
                s_date = f"{min_y}-01-01"
                e_date = f"{max_y}-12-31"
                
                with st.spinner("DBæ¤œç´¢ä¸­..."):
                    # Resolve Venue Param
                    if venue_sb == "å…¨ç«¶è¼ªå ´":
                        target_venue = None
                    elif venue_sb == "33ãƒãƒ³ã‚¯":
                        target_venue = ["å‰æ©‹", "æ¾æˆ¸", "å°ç”°åŸ", "ä¼Šæ±", "å¥ˆè‰¯", "é˜²åºœ", "å¯Œå±±"]
                    elif venue_sb == "500ãƒãƒ³ã‚¯":
                        target_venue = ["å®‡éƒ½å®®", "å¤§å®®", "é«˜çŸ¥"]
                    else:
                        target_venue = venue_sb

                    # Load Raw Data
                    df_res = db_utils.load_races_from_db(target_venue, s_date, e_date)
                
                if df_res.empty:
                    st.sidebar.warning("ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                else:
                    # 2. Filter Process
                    # Filter by Year Exact (since range might include unselected middle years)
                    if 'date_dt' not in df_res.columns:
                        # Re-parse if needed, or assume 'æ—¥ä»˜' string
                        # db_utils load returns 'æ—¥ä»˜' as string "YYYYå¹´MMæœˆDDæ—¥" (converted in logic?) 
                        # load_races_from_db returns 'æ—¥ä»˜' as "YYYYå¹´MMæœˆDDæ—¥".
                        # Let's convert to year.
                        def get_year(s):
                            try: return int(s[:4]) # "2023å¹´..."
                            except: return 0
                        df_res['year_temp'] = df_res['æ—¥ä»˜'].apply(get_year)
                        df_res = df_res[df_res['year_temp'].isin(selected_years)]
                    
                    # Name Filter
                    if search_name:
                        df_res = df_res[df_res['é¸æ‰‹å'].astype(str).str.contains(search_name)]
                    
                    # Line Filters
                    if f_longest and 'is_longest_line' in df_res.columns:
                        df_res = df_res[df_res['is_longest_line'] == 1]
                        
                    if f_line_len > 0 and 'line_length' in df_res.columns:
                        df_res = df_res[df_res['line_length'] == f_line_len]
                        
                    if f_line_pos > 0 and 'line_pos' in df_res.columns:
                        df_res = df_res[df_res['line_pos'] == f_line_pos]

                    # Strength
                    if f_str_head and 'line_strength_head' in df_res.columns:
                        df_res = df_res[df_res['line_strength_head'].isin(f_str_head)]
                    if f_str_sec and 'line_strength_second' in df_res.columns:
                        df_res = df_res[df_res['line_strength_second'].isin(f_str_sec)]

                    # Jimoto
                    if f_jimoto and 'is_jimoto' in df_res.columns:
                        df_res = df_res[df_res['is_jimoto'] == 1]

                    # Tactics High
                    if f_top_nige and 'is_top_nige' in df_res.columns:
                        df_res = df_res[df_res['is_top_nige'] == 1]
                    if f_top_maku and 'is_top_makuri' in df_res.columns:
                        df_res = df_res[df_res['is_top_makuri'] == 1] 
                    if f_top_sashi and 'is_top_sashi' in df_res.columns:
                         df_res = df_res[df_res['is_top_sashi'] == 1]
                    
                    # Fav Tactic (String Match)
                    if f_tactics and 'fav_tactic' in df_res.columns:
                         df_res = df_res[df_res['fav_tactic'].isin(f_tactics)]

                    # Result
                    st.session_state['search_result_db'] = df_res
                    st.sidebar.success(f"æ¤œç´¢å®Œäº†: {len(df_res)}ä»¶")
                    
                    # Player Stats Summary (If filtered by name and results exist)
                    if search_name and not df_res.empty:
                        st.session_state['player_stats_summary'] = True
                    else:
                        st.session_state.pop('player_stats_summary', None)

            except Exception as e:
                st.sidebar.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

    # E. Settings & Help
    st.sidebar.markdown("---")
    st.sidebar.header("âš™ï¸ è¨­å®šãƒ»ãƒ˜ãƒ«ãƒ—")
    
    # API Key Persistence
    API_KEY_FILE = "api_key_secret.txt"
    loaded_key = ""
    if os.path.exists(API_KEY_FILE):
        try:
            with open(API_KEY_FILE, "r") as f:
                loaded_key = f.read().strip()
        except: pass

    api_key_input = st.sidebar.text_input("Gemini API Key", value=loaded_key, type="password", help="AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¿…è¦ã§ã™")
    
    # Save if changed
    if api_key_input != loaded_key:
        with open(API_KEY_FILE, "w") as f:
            f.write(api_key_input)
        st.sidebar.success("APIã‚­ãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    
    with st.sidebar.expander("ğŸ“š ç”¨èªãƒ»ãƒ­ã‚¸ãƒƒã‚¯è§£èª¬"):
        st.markdown("""
        ### ğŸ‘‘ ãƒ­ã‚¸ãƒƒã‚¯ V3 (New)
        *   **ğŸŒŸ åœ§å€’çš„æ²ã‚Š (Dom Makuri)**: æ²ã‚Šå›æ•°ãŒåœ§å€’çš„(5å›ä»¥ä¸Šã‹ã¤ä»–ã‚’åœ§å€’)ã€‚SSç´šã®ä¿¡é ¼åº¦ã€‚
        *   **ğŸƒ åœ§å€’çš„é€ƒã’ (Dom Nige)**: çŸ­èµ°è·¯ã§åœ§å€’çš„ãªé€ƒã’å›æ•°ã‚’æŒã¤é¸æ‰‹ã€‚æŠ¼ã—åˆ‡ã‚Šæ¿ƒåšã€‚
        *   **ğŸš€ B-Top (Back Leader)**: ãƒãƒƒã‚¯å›æ•°ãƒˆãƒƒãƒ—ã®é¸æ‰‹ã€‚çŸ­èµ°è·¯ã§ã¯ãƒ©ã‚¤ãƒ³æ±ºç€ã€é•·èµ°è·¯ã§ã¯é€£å¯¾ç‡ãŒé«˜ã„ã€‚
        *   **ğŸ›¡ï¸ æ¿€æˆ¦åŒº (Conflict)**: é€ƒã’é¸æ‰‹ãŒ3åä»¥ä¸Šã„ã‚‹ãƒ¬ãƒ¼ã‚¹ã€‚æ½°ã—åˆã„ã«ã‚ˆã‚‹å·®ã—æœ‰åˆ©ã‚„ã€æœ€å¼·é€ƒã’ã®ç‹¬èµ°ã‚’åˆ¤å®šã€‚

        ### ğŸ¯ äºˆæƒ³ã‚¿ã‚¤ãƒ— (Race Type)
        *   **ğŸ° é‰„æ¿éŠ€è¡Œ (Teppan)**: 1å¼·ã€ã¾ãŸã¯ãƒ©ã‚¤ãƒ³ãŒå¼·åŠ›ã§ã‚¹ã‚¸æ±ºç€ãŒæ¿ƒåšãªãƒ¬ãƒ¼ã‚¹ã€‚ç‚¹æ•°ã‚’çµã£ã¦åšå¼µã‚Šæ¨å¥¨ã€‚
        *   **âš”ï¸ æœ‰åŠ›ã‚¹ã‚¸ (Suji-Lead)**: ãƒ©ã‚¤ãƒ³ã§ã®æ±ºç€ãŒæœ‰åŠ›ã ãŒã€ãƒ’ãƒ¢è’ã‚Œã®å¯èƒ½æ€§ã‚‚ã‚ã‚‹ãƒ¬ãƒ¼ã‚¹ã€‚
        *   **âš¡ ãƒ©ã‚¤ãƒ³ãƒ–ãƒ¬ã‚¤ã‚«ãƒ¼ (Breaker)**: ã‚¹ã‚¸æ±ºç€ãŒå´©ã‚Œã‚„ã™ãã€åˆ¥ç·šã‚„å˜é¨ãŒçµ¡ã‚€æ··æˆ¦ãƒ¬ãƒ¼ã‚¹ã€‚
        *   **ğŸ’° ä¸€æ’ƒå›å (Snipe)**: æœŸå¾…å€¤ã®é«˜ã„ç©´é¸æ‰‹(AIé¸å‡º)ãŒã„ã‚‹ãƒ¬ãƒ¼ã‚¹ã€‚

        ### ğŸ“Š é¸æ‰‹ç‰¹æ€§ (Player Stats)
        *   **âš ï¸ ãƒ©ã‚¤ãƒ³ä¹–é›¢**: è‡ªåˆ†ãŒå¥½èµ°ã—ã¦ã‚‚ç•ªæ‰‹ãŒåƒåˆ‡ã‚Œã‚„ã™ã„é¸æ‰‹ã€‚
        *   **ğŸ”„ æ··æˆ¦æµ®ä¸Š**: å±•é–‹ãŒç¸ºã‚ŒãŸæ™‚ã«æµ®ä¸Šã™ã‚‹ç©´å€™è£œã€‚
        *   **ğŸ—¡ï¸ å·®ã—é€†è»¢**: ç•ªæ‰‹ã‹ã‚‰ã‚­ãƒƒãƒãƒªå·®ã—åˆ‡ã‚‹ã‚¿ã‚¤ãƒ—ã€‚
        *   **ğŸ’’ ç›¸æ€§è‰¯**: ã“ã®ãƒãƒ³ã‚¯ã§ã®é€£å¯¾ç‡ãŒéå¸¸ã«é«˜ã„é¸æ‰‹ã€‚

        ### ğŸ“ ãƒãƒ³ã‚¯ç‰¹å¾´ (Specs)
        *   **çŸ­ç›´ç·š (<50m)**: é€ƒã’æœ‰åˆ© (å‰æ©‹,å°ç”°åŸãªã©)
        *   **é•·ç›´ç·š (>58m)**: æ²ã‚Šãƒ»å·®ã—æœ‰åˆ© (å¤§å®®,æ­¦é›„ãªã©)
        """)

# ==========================================
# ã‚¿ãƒ–æ§‹æˆ
# ==========================================
# ==========================================
# ã‚¿ãƒ–æ§‹æˆ
# ==========================================
st.title("ç«¶è¼ªDeepDiveï½œãƒ‡ãƒ¼ã‚¿ã§ç†±ç‹‚ã‚’ã¤ã‹ã‚!")
tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“‹ å‡ºèµ°è¡¨ãƒ»AIäºˆæƒ³", "ğŸ•µï¸ é¸æ‰‹ãƒ»ãƒ­ã‚¸ãƒƒã‚¯æ¤œç´¢", "ğŸ“ˆ ã‚¨ãƒ³ã‚¸ãƒ³çŠ¶æ…‹", "âš™ï¸ è¨­å®š", "ğŸ“œ AIçš„ä¸­å±¥æ­´"])

# Display Search Results from Sidebar (Global Area)
if 'search_result_db' in st.session_state:
    res_df = st.session_state['search_result_db']
    
    # é¸æ‰‹æˆç¸¾ã‚µãƒãƒªãƒ¼è¡¨ç¤º (åå‰æ¤œç´¢æ™‚)
    if st.session_state.get('player_stats_summary'):
        st.info(f"ğŸ“Š é¸æ‰‹æˆç¸¾ã‚µãƒãƒªãƒ¼ (å¯¾è±¡æœŸé–“: {len(res_df)}èµ°)")
        
        # é›†è¨ˆ
        # Win stats
        if 'ç€é †_val' in res_df.columns:
            total = len(res_df)
            w1 = len(res_df[res_df['ç€é †_val'] == 1])
            w2 = len(res_df[res_df['ç€é †_val'] <= 2])
            w3 = len(res_df[res_df['ç€é †_val'] <= 3])
            
            w1_rate = w1 / total if total > 0 else 0
            w2_rate = w2 / total if total > 0 else 0
            w3_rate = w3 / total if total > 0 else 0
            
            # S/B Stats (Mean of Pre-Race counts)
            # Note: Scraped 'S'/'B' are usually period totals held by player, NOT "Took S in this race".
            # So we show Mean (Average holding)
            # Ensure numeric calc
            num_cols = ['S', 'B', 'é€ƒ', 'æ²', 'å·®', 'ãƒ', 'ç«¶èµ°å¾—ç‚¹']
            for c in num_cols:
                if c in res_df.columns:
                    res_df[c] = pd.to_numeric(res_df[c], errors='coerce').fillna(0)

            s_mean = 0
            b_mean = 0
            
            try:
                if 'S' in res_df.columns:
                   s_mean = pd.to_numeric(res_df['S'], errors='coerce').fillna(0).mean()
                if 'B' in res_df.columns:
                   b_mean = pd.to_numeric(res_df['B'], errors='coerce').fillna(0).mean()
            except: pass
            

            # Fav Tactic (Mode)
            fav_tac = "ä¸æ˜"
            if 'è„šè³ª' in res_df.columns:
                fav_tac = res_df['è„šè³ª'].mode()[0] if not res_df['è„šè³ª'].mode().empty else "ä¸æ˜"
            
            # Ability Stats (Mean)
            try:
                a_nige = pd.to_numeric(res_df['é€ƒ'], errors='coerce').fillna(0).mean() if 'é€ƒ' in res_df.columns else 0
                a_maku = pd.to_numeric(res_df['æ²'], errors='coerce').fillna(0).mean() if 'æ²' in res_df.columns else 0
                a_sashi = pd.to_numeric(res_df['å·®'], errors='coerce').fillna(0).mean() if 'å·®' in res_df.columns else 0
                a_mark = pd.to_numeric(res_df['ãƒ'], errors='coerce').fillna(0).mean() if 'ãƒ' in res_df.columns else 0
            except Exception as e:
                st.warning(f"Stat Calc Error: {e}")
                a_nige, a_maku, a_sashi, a_mark = 0, 0, 0, 0

            # Display Metrics
            m1, m2, m3, m4, m5 = st.columns(5)
            m1.metric("å‹ç‡", f"{w1_rate:.1%}", f"{w1}å‹")
            m2.metric("2é€£å¯¾ç‡", f"{w2_rate:.1%}", f"{w2}å›")
            m3.metric("3é€£å¯¾ç‡", f"{w3_rate:.1%}", f"{w3}å›")
            m4.metric("å¹³å‡Sä¿æŒ", f"{s_mean:.1f}å›", "ç›´è¿‘å¹³å‡")
            m5.metric("å¹³å‡Bä¿æŒ", f"{b_mean:.1f}å›", "ç›´è¿‘å¹³å‡")
            
            st.caption(f"**è„šè³ªå‚¾å‘ (å¹³å‡å›æ•°)**: é€ƒ:{a_nige:.1f}  æ²:{a_maku:.1f}  å·®:{a_sashi:.1f}  ãƒ:{a_mark:.1f}  (æœ€å¤šè„šè³ª: {fav_tac})")
            st.caption("â€» S/Bä¿æŒæ•°ã¯ã€å„ãƒ¬ãƒ¼ã‚¹å‡ºå ´æ™‚ç‚¹ã§ã®å‡ºèµ°è¡¨ãƒ‡ãƒ¼ã‚¿ï¼ˆæœŸåˆ¥åˆè¨ˆï¼‰ã®å¹³å‡å€¤ã§ã™ã€‚")

    st.dataframe(res_df)
    if st.button("æ¤œç´¢çµæœã‚’é–‰ã˜ã‚‹"):
        del st.session_state['search_result_db']
        if 'player_stats_summary' in st.session_state:
             del st.session_state['player_stats_summary']
        st.rerun()

# ------------------------------------------
# Tab 1: å‡ºèµ°è¡¨ãƒ»AIäºˆæƒ³ (ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½)
# ------------------------------------------
with tab1:
    st.header("å‡ºèµ°è¡¨è§£æ")
    
    uploaded_files = st.file_uploader("æ¥½å¤©Kãƒ‰ãƒªãƒ¼ãƒ ã‚¹ã®å‡ºèµ°è¡¨(HTML)ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰", type=['html', 'htm'], accept_multiple_files=True)
    
    # Merge Uploaded Files + Scraped Data
    all_race_data = []

    # A) Uploaded Files
    if uploaded_files:
        for f in uploaded_files:
            content = f.read().decode("utf-8", errors="ignore")
            # Use direct HTML cell parser for accurate column extraction
            df_curr, meta_curr = logic_v2.parse_kdreams_direct(content)
            
            if not df_curr.empty:
                # Meta info for label
                r_num = "??R"
                if 'ãƒ¬ãƒ¼ã‚¹ç•ªå·' in df_curr.columns:
                    r_num = f"{df_curr['ãƒ¬ãƒ¼ã‚¹ç•ªå·'].iloc[0]}R"
                elif meta_curr.get('race_num'):
                     r_num = f"{meta_curr['race_num']}R"
                     
                p_name = meta_curr.get('place', 'ä¸æ˜')
                r_class = meta_curr.get('race_class', '')
                
                label = f"{p_name} {r_num} {r_class}"
                
                # Get integer race num for sorting
                r_num_int = 0
                if 'ãƒ¬ãƒ¼ã‚¹ç•ªå·' in df_curr.columns:
                     try: r_num_int = int(df_curr['ãƒ¬ãƒ¼ã‚¹ç•ªå·'].iloc[0])
                     except: pass
                elif meta_curr.get('race_num'):
                     try: r_num_int = int(meta_curr.get('race_num'))
                     except: pass

                all_race_data.append({
                    'label': label,
                    'df': df_curr,
                    'meta': meta_curr,
                    'filename': f.name,
                    'sort_key': (p_name, r_num_int)
                })

    # B) Scraped Races (Stored in Session State)
    if 'scraped_races' in st.session_state:
        for scraped in st.session_state['scraped_races']:
            all_race_data.append(scraped)
            
    race_data_list = all_race_data

    if not race_data_list:
        st.error("æœ‰åŠ¹ãªãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        # --- Batch Analysis Section (User Request) ---
        with st.expander("ğŸ“‹ å…¨ãƒ¬ãƒ¼ã‚¹ AIåˆ†æã‚µãƒãƒªãƒ¼ (ä¸€æ‹¬äºˆæƒ³)", expanded=False):
            st.info(f"èª­ã¿è¾¼ã¿æ¸ˆã¿ãƒ¬ãƒ¼ã‚¹æ•°: {len(race_data_list)}ä»¶")
            
            # Check for cached summary
            if 'batch_analysis_summary' in st.session_state and st.session_state['batch_analysis_summary'] is not None:
                 st.info("å‰å›ã®åˆ†æçµæœã‚’è¡¨ç¤ºã—ã¾ã™")
                 st.dataframe(st.session_state['batch_analysis_summary'], use_container_width=True)
                 if st.button("åˆ†æçµæœã‚’ã‚¯ãƒªã‚¢"):
                     st.session_state['batch_analysis_summary'] = None
                     st.rerun()

            if st.button("å…¨ãƒ¬ãƒ¼ã‚¹ã‚’ä¸€æ‹¬åˆ†æã™ã‚‹"):
                summary_rows = []
                progress_bar = st.progress(0)
                
                for i, r_data in enumerate(race_data_list):
                    progress_bar.progress((i + 1) / len(race_data_list))
                    
                    df_target = r_data['df']
                    meta_target = r_data['meta']
                    try:
                        # 1. Advanced Logic for Prediction
                        p_name = meta_target.get('place', '')
                        r_cls = meta_target.get('race_class', 'Aç´š')
                        # 0. Pre-process Features (Must be same as single view)
                        if 'ç«¶è¼ªå ´' not in df_target.columns and p_name: df_target['ç«¶è¼ªå ´'] = p_name
                        if 'æ—¥ä»˜' not in df_target.columns and meta_target.get('date'): df_target['æ—¥ä»˜'] = meta_target.get('date')
                        if 'ãƒ¬ãƒ¼ã‚¹ç•ªå·' not in df_target.columns and meta_target.get('race_num'): df_target['ãƒ¬ãƒ¼ã‚¹ç•ªå·'] = meta_target.get('race_num')
                        
                        # Sanitize Input DataFrame Types
                        if 'ç«¶èµ°å¾—ç‚¹' in df_target.columns:
                            # Protect against double concatenation (85.1285.12) or list-string
                            def clean_input_score(x):
                                s = str(x).strip()
                                import re
                                # Match 2-3 digits, optional dot, optional 1-2 decimals
                                m = re.search(r'(\d{2,3}(\.\d{1,2})?)', s)
                                if m: 
                                    try: return float(m.group(1))
                                    except: return 0.0
                                return 0.0
                            df_target['ç«¶èµ°å¾—ç‚¹'] = df_target['ç«¶èµ°å¾—ç‚¹'].apply(clean_input_score)
                        
                        if 'è»Šç•ª' in df_target.columns:
                             df_target['è»Šç•ª'] = pd.to_numeric(df_target['è»Šç•ª'], errors='coerce').fillna(0).astype(int)
                        
                        df_target = db_utils.run_global_features(df_target)
                        df_target = db_utils.run_race_features(df_target)
                        
                        # Use CLASSIC Logic for Unified Prediction
                        df_scored = logic_v2.calculate_classic_score(df_target)

                        # Legacy Metrics for "Trend" (User Request: é‰„æ¿/æ··æˆ¦ etc.)
                        legacy_metrics = logic_v2.calculate_advanced_metrics(df_target)
                        trend_signals = legacy_metrics.get('signals', [])
                        trend_str = " ".join(trend_signals) if trend_signals else "-"
                        
                        # Final Score uses ai_score from classic (no separate bonus)
                        df_scored['final_score'] = pd.to_numeric(df_scored.get('ai_score', 0), errors='coerce').fillna(0.0)
                        df_scored['ai_bonus'] = 0.0

                        if 'final_score' in df_scored.columns:
                            top_row = df_scored.sort_values('final_score', ascending=False).iloc[0]
                            top_name = top_row['é¸æ‰‹å']
                            top_score = top_row['final_score']
                            
                            confidence = "â—" if top_score >= 80 else "â—‹"
                            if top_score >= 85: confidence = "â˜…"
                            
                            top3_df = df_scored.sort_values('final_score', ascending=False).head(3)
                            top3_nums = top3_df['è»Šç•ª'].tolist()
                            pred_str = "-".join(map(str, top3_nums))
                            
                            
                            # --- Automatic History Save (Classic Logic) ---
                            strategy_data = logic_v2.generate_classic_strategy(df_scored, score_col='final_score')
                            
                            # Check for Suji-Fix (æ¿€ç†±) using hybrid check for exclusion
                            hybrid_check = logic_v2.generate_betting_strategy(df_scored, score_col='final_score')
                            race_type_for_exclusion = hybrid_check.get('type', 'standard')

                            # --- AUTO SAVE TO HISTORY ---
                            # RELAXED: Save if tickets exist OR type is valid, BUT skip suji_fix
                            is_valid = strategy_data.get('type') not in ['error', 'skip']
                            if strategy_data.get('tickets'): is_valid = True
                            
                            # Exclude L-Class / Girls Keirin (User Request)
                            if 'Lç´š' in r_cls or 'ã‚¬ãƒ¼ãƒ«ã‚º' in r_cls:
                                is_valid = False
                                
                            # Skip suji_fix (æ¿€ç†±) races - REMOVED per user request
                            # if race_type_for_exclusion == 'suji_fix':
                            #     is_valid = False

                            if is_valid:
                                try:
                                    p_name = meta.get('place', '')
                                    r_num = meta.get('race_num', '??R')
                                    d_raw = meta.get('date', '')
                                    d_clean = d_raw.replace('-', 'å¹´').replace('/', 'å¹´')
                                    if 'å¹´' not in d_clean: 
                                         try: d_clean = datetime.strptime(d_raw, "%Y-%m-%d").strftime("%Yå¹´%mæœˆ%dæ—¥")
                                         except: pass
                                    
                                    # Fix 1RR issue
                                    r_num_str = str(r_num)
                                    if not r_num_str.endswith('R'):
                                        r_num_str += 'R'
                                    
                                    # Generate Hash ID
                                    import hashlib
                                    raw_str = f"{d_clean}{p_name}{r_num_str}"
                                    race_id = hashlib.md5(raw_str.encode()).hexdigest()

                                    st_title = strategy_data.get('title', 'æ¨™æº–')
                                    st_reason = strategy_data.get('reason', '')
                                    
                                    pred_dict = {
                                        "race_id": race_id,
                                        "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                        "place": p_name,
                                        "race_num": r_num_str,
                                        "date": d_clean,
                                        "prediction_text": f"ã€{st_title}ã€‘{st_reason} (ä¸€æ‹¬)",
                                        "tickets": strategy_data.get('tickets', []),
                                        "strategy_title": st_title,
                                        "strategy_type": "classic", # Changed to Classic
                                        "race_type": strategy_data.get('type', 'standard'),
                                        "ai_indices": df_scored[['è»Šç•ª', 'final_score', 'é¸æ‰‹å', 'ai_tag']].to_dict('records') if 'final_score' in df_scored.columns else []
                                    }
                                    res = db_utils.save_prediction(pred_dict)
                                except Exception as e_s: print(f"Save Error: {e_s}")

                                
                            # Calculate Gap and Bonus
                            score_gap = 0.0
                            max_bonus_val = 0.0
                            
                            try:
                                # Bonus of the Top Pick (Honmei)
                                max_bonus_val = float(top_row.get('ai_bonus', 0.0))
                                
                                # Score Gap (1st - 2nd)
                                if len(df_scored) >= 2:
                                    # df_scored is not sorted in place above, top_row is from sorted copy? No, line 676: df_scored.sort_values...iloc[0]
                                    # But df_scored itself is not sorted.
                                    # top3_df is consistent.
                                    s1 = float(top3_df.iloc[0]['final_score'])
                                    s2 = float(top3_df.iloc[1]['final_score'])
                                    score_gap = s1 - s2
                            except: pass
                                
                            summary_rows.append({
                                "ãƒ¬ãƒ¼ã‚¹": r_data['label'],
                                "ãƒ¬ãƒ¼ã‚¹å‚¾å‘": trend_str, 
                                "æœ¬å‘½é¸æ‰‹": top_name,
                                "æœ€å¤§åŠ ç‚¹": f"{max_bonus_val:+.1f}",
                                "æŒ‡æ•°å·®(1-2ä½)": f"{score_gap:.1f}",
                                "ç¢ºåº¦": confidence
                            })
                    except Exception as e:
                        print(f"Batch Error: {e}")
                        st.error(f"Error processing {r_data['label']}: {e}")
                        pass
                
                progress_bar.empty()
                if summary_rows:
                    st.success(f"{len(summary_rows)}ãƒ¬ãƒ¼ã‚¹ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                    
                    # Order columns nicely
                    df_summary = pd.DataFrame(summary_rows)
                    cols = ["ãƒ¬ãƒ¼ã‚¹", "ãƒ¬ãƒ¼ã‚¹å‚¾å‘", "æœ¬å‘½é¸æ‰‹", "æœ€å¤§åŠ ç‚¹", "æŒ‡æ•°å·®(1-2ä½)", "ç¢ºåº¦"]
                    # Ensure cols exist (in case empty)
                    df_summary = df_summary[cols]
                    
                    st.session_state['batch_analysis_summary'] = df_summary
                    st.dataframe(df_summary, use_container_width=True)
                else:
                    st.warning("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

        # 2. Race Selection UI
        # Sort by Venue, then Race Number
        race_data_list.sort(key=lambda x: x['sort_key'])
        
        st.markdown("---")
        
        # --- Button Grid Logic ---
        # 1. Initialize State
        if 'selected_race_label' not in st.session_state:
            st.session_state['selected_race_label'] = race_data_list[0]['label']
        
        # 2. Group by Venue
        from itertools import groupby
        
        # Ensure sorted by Venue for grouping
        race_data_list.sort(key=lambda x: (x['sort_key'][0], x['sort_key'][1]))
        
        # 3. Render Buttons
        st.write("â–¼ åˆ†æã™ã‚‹ãƒ¬ãƒ¼ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„")
        
        for venue, items in groupby(race_data_list, key=lambda x: x['sort_key'][0]):
            st.subheader(f"ğŸŸï¸ {venue}")
            # Create columns for buttons (e.g. 6 per row)
            items_list = list(items)
            cols = st.columns(6)
            for idx, item in enumerate(items_list):
                c = cols[idx % 6]
                r_num_str = f"{item['sort_key'][1]}R"
                label = item['label']
                filename = item.get('filename', str(idx))  # Unique per file
                
                # Style active button
                is_active = (st.session_state['selected_race_label'] == label)
                if c.button(f"{r_num_str}", key=f"btn_{label}_{filename}", type="primary" if is_active else "secondary"):
                    st.session_state['selected_race_label'] = label
                    st.rerun()

        # 4. Get Selected Data
        selected_label = st.session_state['selected_race_label']
        # Fallback if selection missing from current list (re-upload etc)
        target_data = next((d for d in race_data_list if d['label'] == selected_label), None)
        if not target_data and race_data_list:
            target_data = race_data_list[0]
            st.session_state['selected_race_label'] = target_data['label']
        
        if target_data:
            # IMPORTANT: creating a copy is essential to prevent mutating cached objects in session_state
            df_race = target_data['df'].copy()
            meta = target_data['meta']
            place_name = meta.get('place', '')
            race_class = meta.get('race_class', 'Aç´š')
            
            st.success(f"ğŸ“ {selected_label} - è§£æä¸­...")
            st.caption(f"File: {target_data['filename']}")
            
            # DEBUG: Dump DF for inspection
            try:
                df_race.to_csv("debug_race_df.csv", index=False)
            except: pass
            
            # --- Continue Analysis below ---
            
            # 1. åŸºæœ¬ç‰¹å¾´é‡ã®ç”Ÿæˆ (åœ°å…ƒåˆ¤å®šãªã©)
            if 'ç«¶è¼ªå ´' not in df_race.columns and place_name:
                df_race['ç«¶è¼ªå ´'] = place_name
            if 'æ—¥ä»˜' not in df_race.columns and meta.get('date'):
                df_race['æ—¥ä»˜'] = meta.get('date')
            if 'ãƒ¬ãƒ¼ã‚¹ç•ªå·' not in df_race.columns and meta.get('race_num'):
                df_race['ãƒ¬ãƒ¼ã‚¹ç•ªå·'] = meta.get('race_num')
                
            # --- CRITICAL FIX: Ensure 'ç«¶èµ°å¾—ç‚¹' Column Name Consistency ---
            # Search for any column containing 'ç«¶èµ°å¾—ç‚¹' (handling whitespace/unicode)
            score_col_candidates = [c for c in df_race.columns if 'ç«¶èµ°å¾—ç‚¹' in str(c)]
            if score_col_candidates and 'ç«¶èµ°å¾—ç‚¹' not in df_race.columns:
                 # Rename best candidate
                 df_race.rename(columns={score_col_candidates[0]: 'ç«¶èµ°å¾—ç‚¹'}, inplace=True)
            elif not score_col_candidates and 'å¾—ç‚¹' in str(df_race.columns):
                 # Last resort: look for 'å¾—ç‚¹' if unique
                 score_c = [c for c in df_race.columns if 'å¾—ç‚¹' in str(c)]
                 if len(score_c) == 1:
                     df_race.rename(columns={score_c[0]: 'ç«¶èµ°å¾—ç‚¹'}, inplace=True)
                     
            if 'ç«¶èµ°å¾—ç‚¹' in df_race.columns:
                 # Ensure float
                 df_race['ç«¶èµ°å¾—ç‚¹'] = pd.to_numeric(df_race['ç«¶èµ°å¾—ç‚¹'], errors='coerce').fillna(0.0)
            # -----------------------------------------------------------------
                
            try:
                df_race = db_utils.run_global_features(df_race)
                df_race = db_utils.run_race_features(df_race) # Add Specialist Flags
            except Exception as e:
                st.warning(f"ç‰¹å¾´é‡ç”Ÿæˆã‚¹ã‚­ãƒƒãƒ—: {e}")
            
            # 2. AIã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° (AI Logic V3)
            try:
                df_scored = logic_v2.calculate_ai_score(df_race)
                
                with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°: ãƒ­ã‚¸ãƒƒã‚¯æŠ•å…¥å‰ã®ãƒ‡ãƒ¼ã‚¿ç¢ºèª"):
                    st.write(f"ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(df_scored)}")
                    if 'ãƒ©ã‚¤ãƒ³' in df_scored.columns:
                        st.write("â–¼ ãƒ©ã‚¤ãƒ³åˆ—ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ (Top 9)")
                        st.table(df_scored[['è»Šç•ª', 'é¸æ‰‹å', 'ãƒ©ã‚¤ãƒ³']].head(9))
                        st.write("Unique Lines:", df_scored['ãƒ©ã‚¤ãƒ³'].unique())
                    else:
                        st.error("âš ï¸ 'ãƒ©ã‚¤ãƒ³'ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼")
                    
                    st.write("Engine Stats Cache Keys:", list(engine.stats_cache.keys()))
                
                # 3. â˜… æ‹¡å¼µAIãƒ­ã‚¸ãƒƒã‚¯ã®é©ç”¨ (Polarsã‚¨ãƒ³ã‚¸ãƒ³æ´»ç”¨) â˜…
                if place_name:
                    # Debug: Show logic is attempting to run
                    # --- Debug RAW Column Names ---
                    with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°: ç”Ÿã‚«ãƒ©ãƒ åä¸€è¦§"):
                        st.write("ã‚«ãƒ©ãƒ æ•°:", len(df_scored.columns))
                        # Show columns with INDEX
                        col_list = list(df_scored.columns)
                        for i, c in enumerate(col_list):
                            st.write(f"  [{i}]: {c}")
                        
                        # Show first row data with index
                        if len(df_scored) > 0:
                            st.write("--- æœ€åˆã®è¡Œã®ãƒ‡ãƒ¼ã‚¿ ---")
                            sample = df_scored.iloc[0]
                            for i, c in enumerate(col_list):
                                st.write(f"  [{i}] {c} = {sample[c]}")
                        
                    # --- Debug Tactic Flags ---
                    with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°: é€ƒ/æ²/å·® MAXåˆ¤å®š"):
                        if 'é€ƒ' in df_scored.columns:
                            st.write("é€ƒ åˆ—ã®å€¤:")
                            st.table(df_scored[['è»Šç•ª', 'é¸æ‰‹å', 'é€ƒ']].astype(str))
                        if 'æ²' in df_scored.columns:
                            st.write("æ² åˆ—ã®å€¤:")
                            st.table(df_scored[['è»Šç•ª', 'é¸æ‰‹å', 'æ²']].astype(str))
                        if 'å·®' in df_scored.columns:
                            st.write("å·® åˆ—ã®å€¤:")
                            st.table(df_scored[['è»Šç•ª', 'é¸æ‰‹å', 'å·®']].astype(str))
                        
                        if 'is_top_nige' in df_scored.columns:
                            nige_top = df_scored[df_scored['is_top_nige'] == 1]['é¸æ‰‹å'].tolist()
                            st.info(f"é€ƒNO1: {nige_top}")
                        if 'is_top_makuri' in df_scored.columns:
                            mak_top = df_scored[df_scored['is_top_makuri'] == 1]['é¸æ‰‹å'].tolist()
                            st.info(f"æ²NO1: {mak_top}")
                        if 'is_top_sashi' in df_scored.columns:
                            sashi_top = df_scored[df_scored['is_top_sashi'] == 1]['é¸æ‰‹å'].tolist()
                            st.info(f"å·®NO1: {sashi_top}")

                    # st.toast(f"Applying Logic for {place_name} ({race_class})") # Optional toast
                    df_scored = apply_advanced_logic(df_scored, engine, place_name, race_class)
                else:
                    st.error("âš ï¸ ç«¶è¼ªå ´åï¼ˆplace_nameï¼‰ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    st.write(f"Meta Info: {meta}")
                    
                # POST-LOGIC DEBUG
                with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°: ãƒ­ã‚¸ãƒƒã‚¯é©ç”¨å¾Œã®è©³ç´° (Stats Status)"):
                        st.write("Loaded Stats Cache Keys:", list(engine.stats_cache.keys()))
                        if 'bonus_reasons' in df_scored.columns:
                            st.write("ãƒœãƒ¼ãƒŠã‚¹ç†ç”±ã‚µãƒ³ãƒ—ãƒ«:", df_scored[['é¸æ‰‹å', 'bonus_reasons']].head(5))
                        else:
                            st.write("âš ï¸ bonus_reasons ã‚«ãƒ©ãƒ ãªã—")
                        
                        if 'line_len_temp' in df_scored.columns:
                            st.write("ãƒ©ã‚¤ãƒ³é•·(Temp):", df_scored['line_len_temp'].head())

                # Final Score uses ai_score + advanced bonus
                # Ensure bonus_score is numeric
                df_scored['ai_bonus'] = pd.to_numeric(df_scored.get('bonus_score', 0), errors='coerce').fillna(0.0)
                
                # Base Score (Classic)
                classic_score = pd.to_numeric(df_scored.get('ai_score', 0), errors='coerce').fillna(0.0)
                
                # Final = Classic + Advanced Bonus
                df_scored['final_score'] = classic_score + df_scored['ai_bonus']

    
                # --- è¡¨ç¤º ---
                # Bank Info (New)
                # Returns: (spec_str, desc, fav)
                spec_str, b_desc, b_fav = db_utils.get_bank_characteristics(place_name)
                st.info(f"**ğŸŸï¸ ãƒãƒ³ã‚¯ç‰¹å¾´: {spec_str}**\n\n{b_desc}\n\nğŸ‘‰ **æœ‰åˆ©ãªæˆ¦æ³•: {b_fav}**")

                # Generate Strategy for Display (AI Logic V3)
                strategy_data = {}
                try:
                    strategy_data = logic_v2.generate_betting_strategy(df_scored, score_col='final_score')
                    structured_bets = strategy_data.get('tickets', [])
                except Exception as e:
                    st.warning(f"æˆ¦ç•¥ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                    structured_bets = []

                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.subheader("ğŸ”¥ ãƒ‡ãƒ¼ã‚¿ã§ç†±ç‹‚ã‚’ã¤ã‹ã‚!")
                    
                    # Time Info
                    time_info = []
                    if meta.get('deadline'): time_info.append(f"â° ç· åˆ‡: **{meta['deadline']}**")
                    if meta.get('start_time'): time_info.append(f"ğŸ”« ç™ºèµ°: **{meta['start_time']}**")
                    
                    if time_info:
                        st.markdown(" ".join(time_info))
                    
                    # Line Info (New)
                    if meta.get('lines_parsed'):
                         st.info(f"ğŸš€ **ä¸¦ã³äºˆæƒ³**: {meta['lines_parsed']}")
                    
                    # è¡¨ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢
                    display_df = df_scored.copy()
                    
                    # Velodrome to Prefecture mapping
                    velodrome_pref = {
                        "å‡½é¤¨": "åŒ—æµ·é“", "é’æ£®": "é’æ£®", "ã„ã‚ãå¹³": "ç¦å³¶", 
                        "å¼¥å½¦": "æ–°æ½Ÿ", "å‰æ©‹": "ç¾¤é¦¬", "å–æ‰‹": "èŒ¨åŸ", "å®‡éƒ½å®®": "æ ƒæœ¨",
                        "å¤§å®®": "åŸ¼ç‰", "è¥¿æ­¦åœ’": "åŸ¼ç‰", "äº¬ç‹é–£": "æ±äº¬", "ç«‹å·": "æ±äº¬",
                        "æ¾æˆ¸": "åƒè‘‰", "åƒè‘‰": "åƒè‘‰", "å·å´": "ç¥å¥ˆå·", "å¹³å¡š": "ç¥å¥ˆå·",
                        "å°ç”°åŸ": "ç¥å¥ˆå·", "ä¼Šæ±": "é™å²¡", "é™å²¡": "é™å²¡",
                        "åå¤å±‹": "æ„›çŸ¥", "è±Šæ©‹": "æ„›çŸ¥", "å²é˜œ": "å²é˜œ", "å¤§å£": "å²é˜œ",
                        "æ¾é˜ª": "ä¸‰é‡", "å››æ—¥å¸‚": "ä¸‰é‡", "å¯Œå±±": "å¯Œå±±", "ç¦äº•": "ç¦äº•",
                        "å¥ˆè‰¯": "å¥ˆè‰¯", "å‘æ—¥ç”º": "äº¬éƒ½", "å’Œæ­Œå±±": "å’Œæ­Œå±±",
                        "å²¸å’Œç”°": "å¤§é˜ª", "ç‰é‡": "å²¡å±±", "åºƒå³¶": "åºƒå³¶", "é˜²åºœ": "å±±å£",
                        "é«˜æ¾": "é¦™å·", "å°æ¾å³¶": "å¾³å³¶", "é«˜çŸ¥": "é«˜çŸ¥", "æ¾å±±": "æ„›åª›",
                        "å°å€‰": "ç¦å²¡", "ä¹…ç•™ç±³": "ç¦å²¡", "æ­¦é›„": "ä½è³€", "ä½ä¸–ä¿": "é•·å´",
                        "åˆ¥åºœ": "å¤§åˆ†", "ç†Šæœ¬": "ç†Šæœ¬"
                    }
                    venue_pref = velodrome_pref.get(place_name, "")
                    
                    # AIã‚¿ã‚°ã®è£…é£¾ (Antigravityç†ç”±ãŒã‚ã‚Œã°è¿½åŠ )
                    def format_tags(row):
                        tags = str(row.get('ai_tag', ''))
                        reason = str(row.get('bonus_reasons', '')) # logic_polars uses 'bonus_reasons'
                        if reason and reason != 'nan':
                            tags += reason # brackets already included in bonus_reasons
                        
                        # Check if local player (same prefecture as velodrome)
                        player_pref = str(row.get('åºœçœŒ', '')).strip()
                        is_local = False
                        if venue_pref and player_pref:
                            # Handle variations: "ç¥å¥ˆå·" vs "ç¥å¥ˆå·çœŒ", etc.
                            if venue_pref in player_pref or player_pref in venue_pref:
                                is_local = True
                        
                        if row.get('is_jimoto') == 1 or is_local:
                            tags = "ğŸ åœ°å…ƒ " + tags
                        return tags.strip()
    
                    display_df['åˆ†æã‚³ãƒ¡ãƒ³ãƒˆ'] = display_df.apply(format_tags, axis=1)
                    
                    # è¡¨ç¤ºã‚«ãƒ©ãƒ 
                    cols = ['è»Šç•ª', 'é¸æ‰‹å', 'åºœçœŒ', 'final_score', 'ai_bonus', 'åˆ†æã‚³ãƒ¡ãƒ³ãƒˆ', 'ç«¶èµ°å¾—ç‚¹', 'è„šè³ª']
                    # å­˜åœ¨ç¢ºèª
                    cols = [c for c in cols if c in display_df.columns]
                    
                    # Ensure numeric for formatting
                    numeric_cols = ['final_score', 'ai_bonus', 'ç«¶èµ°å¾—ç‚¹', 'S', 'B', 'é€ƒ', 'æ²', 'å·®', 'ãƒ']
                    for nc in numeric_cols:
                        if nc in display_df.columns:
                            display_df[nc] = pd.to_numeric(display_df[nc], errors='coerce').fillna(0.0)

                    st.dataframe(
                        display_df[cols].style
                        .background_gradient(subset=['final_score'], cmap="Purples") # è‰²è¨­å®š
                        .format({
                             'final_score': '{:.1f}', 
                             'ai_bonus': '{:+.1f}',
                             'ç«¶èµ°å¾—ç‚¹': '{:.2f}'}, na_rep="0.0"), 
                        use_container_width=True,
                        height=400
                    )
                    
                    # --- äºˆæ¸¬ç‡ãƒ†ãƒ¼ãƒ–ãƒ« (Second Table) ---
                    st.markdown("---")
                    st.subheader("ğŸ“Š äºˆæ¸¬å‹ç‡ãƒ»é€£å¯¾ç‡ãƒ†ãƒ¼ãƒ–ãƒ«")
                    
                    # Calculate prediction rates based on data logic
                    pred_df = df_scored.copy()
                    
                    # Calculate comprehensive strength score for each player
                    # Base: normalized final_score
                    if 'final_score' in pred_df.columns:
                        base_score = pd.to_numeric(pred_df['final_score'], errors='coerce').fillna(100)
                    elif 'ç«¶èµ°å¾—ç‚¹' in pred_df.columns:
                        base_score = pd.to_numeric(pred_df['ç«¶èµ°å¾—ç‚¹'], errors='coerce').fillna(100)
                    else:
                        base_score = pd.Series([100] * len(pred_df))
                    
                    # Apply bonus multipliers based on data-driven factors
                    # CAUTION: final_score already includes AI bonuses. 
                    # Adding multipliers here creates double-counting and flips ranking.
                    # To ensure consistency between AI Analysis (final_score) and Prediction Table,
                    # we do NOT add further multipliers if using final_score.
                    
                    multiplier = pd.Series([1.0] * len(pred_df), index=pred_df.index)
                    
                    # (Legacy multipliers commented out to fix inconsistency)
                    # if 'is_top_nige' in pred_df.columns: multiplier += pred_df['is_top_nige'].fillna(0) * 0.30
                    # if 'is_top_makuri' in pred_df.columns: multiplier += pred_df['is_top_makuri'].fillna(0) * 0.32
                    
                    # Factor 2: Line position advantage (Small adjustment ok?)
                    # If we remove all, we trust final_score 100%.
                    # Let's keep small adjustments if needed, but for now, prioritize consistency.
                    
                    # Calculate weighted strength score
                    # Use final_score (base_score) directly to maintain ranking order
                    strength = base_score * 1.0 # multiplier (disabled)
                    
                    # === äºˆæ¸¬å‹ç‡ (Sum = 100%) ===
                    # Advanced Logic: Force Top 1 Win Rate based on Class/Venue historical data
                    # (Derived from analyze_top_score_rates.py)
                    
                    import numpy as np
                    
                    # 1. Determine Target Rate for Top 1 Score Player
                    target_top1_rate = 39.3 # Default global average
                    
                    # Check Class (if available)
                    race_class = ""
                    # Try to infer class from race_name in meta or other columns
                    # Simplification: Use heuristics or default
                    # If we had 'ç´šç­' column, we would use it. 
                    # Here we might need to rely on typical performance if class is unknown.
                    # But actually we can differentiate by Score itself?
                    # High Score (Girls > 50 but scale is different). 
                    # Let's use venue-specific adjustments if available.
                    
                    venue_adjustments = {
                        "å¹³å¡š": 2.0, "æ­¦é›„": 1.7, "å‰æ©‹": 0.7, "é˜²åºœ": -0.3, "ç‰é‡": -0.7, "æ¾é˜ª": -1.7
                    }
                    adj = venue_adjustments.get(place_name, 0.0)
                    target_top1_rate += adj

                    # Check for Girls Keirin (L-Class) - usually 7 cars, L codes
                    is_girls = False
                    if len(pred_df) <= 7:
                         # Heuristic: Check if 'L' is in any class code if available, or just check race metadata
                         # If race_name contains 'ã‚¬ãƒ¼ãƒ«ã‚º'
                         r_name = meta.get('race_name', '')
                         if 'ã‚¬ãƒ¼ãƒ«ã‚º' in r_name or 'Lç´š' in r_name:
                             is_girls = True
                             target_top1_rate = 63.7 # From analysis
                    
                    # Check for Challenge (A3) - usually lower scores?
                    # A3 average is 43.6%
                    # S-Class average is ~35%
                    # If not girls, try to guess class by score average?
                    if not is_girls:
                        avg_score = pred_df['ç«¶èµ°å¾—ç‚¹'].mean() if 'ç«¶èµ°å¾—ç‚¹' in pred_df.columns else 80
                        if avg_score < 80: # Challenge likely
                            target_top1_rate = 43.6
                        elif avg_score > 100: # S-Class likely
                            target_top1_rate = 35.0
                    
                    # 2. Calculate initial power-law distribution
                    # Use Power 3.9 as established baseline
                    strength_powered = np.power(strength, 3.9)
                    
                    # 3. Identify Top 1 Player (Based on calculated strength/final AI score)
                    # Use 'strength' which includes final_score + bonuses
                    top_idx = strength.idxmax()
                    
                    # Calculate raw distribution first
                    total_p = strength_powered.sum()
                    if total_p > 0:
                        raw_probs = strength_powered / total_p
                    else:
                        raw_probs = pd.Series([1/len(pred_df)]*len(pred_df), index=pred_df.index)
                    
                    # 4. Apply Target Rate using "Force & Distribute"
                    # We want AI's Top Pick to have `target_top1_rate`.
                    # Valid only if target is reasonable (e.g. < 90%)
                    if 0 < target_top1_rate < 90:
                        top_prob_target = target_top1_rate / 100.0
                        
                        probs = raw_probs.copy()
                        
                        # Set Top 1
                        probs[top_idx] = top_prob_target
                        
                        # Normalize others
                        others_mask = probs.index != top_idx
                        sum_others = probs[others_mask].sum()
                        
                        if sum_others > 0:
                            target_others = 1.0 - top_prob_target
                            probs[others_mask] = probs[others_mask] / sum_others * target_others
                        
                        pred_df['äºˆæ¸¬å‹ç‡'] = (probs * 100).round(1)
                    else:
                         pred_df['äºˆæ¸¬å‹ç‡'] = (raw_probs * 100).round(1)
                    
                    # === é€£å¯¾æœŸå¾… (Individual %) ===
                    # Optimized multiplier: 1.97x based on historical analysis (was 1.8x)
                    pred_df['é€£å¯¾æœŸå¾…'] = (pred_df['äºˆæ¸¬å‹ç‡'] * 1.97).clip(upper=95).round(1)
                    
                    # === 3ç€å†…æœŸå¾… (Individual %) ===
                    # Optimized multiplier: 2.76x based on historical analysis (was 2.5x)
                    pred_df['3ç€å†…æœŸå¾…'] = (pred_df['äºˆæ¸¬å‹ç‡'] * 2.76).clip(upper=99).round(1)
                    
                    # Prepare display columns
                    pred_cols = ['è»Šç•ª', 'é¸æ‰‹å', 'ç«¶èµ°å¾—ç‚¹']
                    
                    # Ensure numeric for display columns
                    if 'ç«¶èµ°å¾—ç‚¹' in pred_df.columns:
                        pred_df['ç«¶èµ°å¾—ç‚¹'] = pd.to_numeric(pred_df['ç«¶èµ°å¾—ç‚¹'], errors='coerce').fillna(0.0)

                    # Add tactic columns if available
                    for tc in ['S', 'B', 'é€ƒ', 'æ²', 'å·®', 'ãƒ']:
                        if tc in pred_df.columns:
                            pred_df[tc] = pd.to_numeric(pred_df[tc], errors='coerce').fillna(0)
                        if tc in pred_df.columns:
                            pred_cols.append(tc)
                    
                    pred_cols.extend(['äºˆæ¸¬å‹ç‡', 'é€£å¯¾æœŸå¾…', '3ç€å†…æœŸå¾…'])
                    
                    # Filter to existing columns
                    pred_cols = [c for c in pred_cols if c in pred_df.columns]
                    
                    # Sort by äºˆæ¸¬å‹ç‡ descending
                    pred_display = pred_df[pred_cols].sort_values('äºˆæ¸¬å‹ç‡', ascending=False)
                    
                    # Format and display
                    st.dataframe(
                        pred_display.style
                        .background_gradient(subset=['äºˆæ¸¬å‹ç‡'], cmap="Greens")
                        .format({
                            'ç«¶èµ°å¾—ç‚¹': '{:.2f}',
                            'äºˆæ¸¬å‹ç‡': '{:.1f}%',
                            'é€£å¯¾æœŸå¾…': '{:.1f}%',
                            '3ç€å†…æœŸå¾…': '{:.1f}%'
                        }),
                        use_container_width=True,
                        height=350
                    )
                
                with col2:
                    st.subheader("ğŸ¯ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰äºˆæƒ³")
                    
                    # --- 6. Structured Tickets ---
                    if structured_bets:
                       # Display Strategy Title & Alert within Col2
                       st.markdown(f"**æˆ¦ç•¥: {strategy_data.get('title', 'æ¨™æº–')}**")
                       
                       # Display Confidence & EV
                       conf = strategy_data.get('confidence_level', 'ä¸­')
                       ev_comment = strategy_data.get('ev_comment', '')
                       pseudo_ev = strategy_data.get('pseudo_ev', 0)
                       rec_pts = strategy_data.get('recommended_points', {})
                       
                       # Confidence badge color
                       conf_color = "ğŸŸ¢" if conf == "é«˜" else "ğŸŸ¡" if conf == "ä¸­" else "ğŸ”´"
                       st.markdown(f"**ä¿¡é ¼åº¦**: {conf_color} {conf}ã€€|ã€€**æœŸå¾…å€¤ (æ¨å®š)**: {pseudo_ev:+.2f}")
                       st.caption(ev_comment)
                       
                       # Recommended points
                       pts_str = " / ".join([f"{k}: {v}ç‚¹" for k, v in rec_pts.items()])
                       st.info(f"ğŸ’¡ **æ¨å¥¨ç‚¹æ•°**: {pts_str}")
                       
                       strategy_type = strategy_data.get('type', 'standard')
                       if strategy_type in ['snipe', 'chaos']:
                           st.warning("âš ï¸ ç©´æ°—é…ã‚ã‚Šï¼ç‚¹æ•°ã¯åºƒã‚ã«")
                       
                       # Helper to display tickets cleanly
                       st.markdown("---")
                       for t in strategy_data.get('tickets', []):
                           st.write(f"- {t}")
                       
                       # ==========================================
                       # AUTO SAVE (Single Race View) - Classic Logic
                       # ==========================================
                       # Check for Suji-Fix (æ¿€ç†±) using hybrid check for exclusion
                       hybrid_check = logic_v2.generate_betting_strategy(df_scored, score_col='final_score')
                       race_type_for_exclusion = hybrid_check.get('type', 'standard')
                       
                       # Skip suji_fix (æ¿€ç†±) races from saving
                       if race_type_for_exclusion != 'suji_fix':
                           try:
                               p_name = meta.get('place', '')
                               r_num = meta.get('race_num', '??R')
                               d_raw = meta.get('date', '')
                               d_clean = d_raw.replace('-', 'å¹´').replace('/', 'å¹´')
                               if 'å¹´' not in d_clean: 
                                   try: d_clean = datetime.strptime(d_raw, '%Y-%m-%d').strftime('%Yå¹´%mæœˆ%dæ—¥')
                                   except: pass
                               
                               # Generate Hash ID if race_id missing
                               race_id = meta.get('race_id')
                               if not race_id:
                                    import hashlib
                                    raw_str = f"{d_clean}{p_name}{r_num}"
                                    race_id = hashlib.md5(raw_str.encode()).hexdigest()

                               # --- Classic Strategy ---
                               st_title = strategy_data.get('title', 'æ¨™æº–')
                               st_type = strategy_data.get('type', 'standard')
                               st_reason = strategy_data.get('reason', '')
                               
                               pred_data_classic = {
                                   'race_id': race_id, 
                                   'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                   'place': p_name,
                                   'race_num': r_num,
                                   'date': d_clean,
                                   'prediction_text': f'ã€{st_title}ã€‘{st_reason}',
                                   'tickets': strategy_data.get('tickets', []),
                                   'structured_bets': strategy_data.get('structured_bets', []),  # Added for stats calc
                                   'strategy_title': st_title,
                                   'strategy_type': 'classic',  # Changed to Classic
                                   'race_type': st_type,
                                   'ai_indices': df_scored[['è»Šç•ª', 'final_score', 'ai_bonus', 'é¸æ‰‹å']].fillna(0).to_dict('records') if 'final_score' in df_scored.columns else []
                               }
                               
                               if pred_data_classic['tickets']:
                                   db_utils.save_prediction(pred_data_classic)
                                       
                           except Exception as e_save:
                               print(f"Auto Save Error: {e_save}")
                       else:
                           # Display message for skipped suji_fix race
                           st.info("ã“ã®ãƒ¬ãƒ¼ã‚¹ã¯æ¿€ç†±(suji_fix)ã®ãŸã‚ã€äºˆæƒ³å±¥æ­´ã¸ã®ä¿å­˜ã¯è¡Œã„ã¾ã›ã‚“ã€‚")
                       
                       # --- Manual Odds Input (Optional) ---
                       st.markdown("---")
                       with st.expander("ğŸ“Š ã‚ªãƒƒã‚ºæ‰‹å‹•å…¥åŠ›ï¼ˆä»»æ„ï¼‰", expanded=False):
                           st.caption("kdreamsã‹ã‚‰ã‚ªãƒƒã‚ºã‚’ã‚³ãƒ”ãƒšã—ã¦æœŸå¾…å€¤ã‚’è¨ˆç®—ã§ãã¾ã™")
                           st.caption("å½¢å¼ä¾‹: `5-2: 7.5` ã¾ãŸã¯ `1-2-3: 25.0`")
                           
                           odds_input_key = f"odds_input_{selected_label}"
                           odds_text = st.text_area(
                               "ã‚ªãƒƒã‚ºè²¼ã‚Šä»˜ã‘",
                               height=100,
                               key=odds_input_key,
                               placeholder="ä¾‹:\n5-2: 7.5\n1-2-3: 25.0\n1=2=3: 8.5"
                           )
                           
                           if odds_text.strip():
                                # Parse pasted odds
                                import re
                                parsed_odds = {}
                                for line in odds_text.strip().split('\n'):
                                    line = line.strip()
                                    if not line:
                                        continue
                                    
                                    # Format 1: "7-9-4: 35.5" or "7-9-4 35.5" (combo and odds separated)
                                    match1 = re.match(r'^[\d]*[\s\t]*(\d+[-=]\d+(?:[-=]\d+)?)[\:\s\t]+(\d+\.?\d*)$', line)
                                    if match1:
                                        combo = match1.group(1)
                                        odds = float(match1.group(2))
                                        parsed_odds[combo] = odds
                                        continue
                                    
                                    # Format 2: "7-9-435.5" (3é€£å˜ with odds directly after, e.g. from kdreams copy)
                                    # Pattern: single digit X-X-X followed by decimal number (keirin car numbers are 1-9)
                                    match2 = re.match(r'^[\d]*[\s\t]*(\d)-(\d)-(\d)(\d+\.\d+)$', line)
                                    if match2:
                                        c1, c2, c3, odds_str = match2.groups()
                                        combo = f"{c1}-{c2}-{c3}"
                                        odds = float(odds_str)
                                        parsed_odds[combo] = odds
                                        continue
                                    
                                    # Format 3: "5-235.5" (2è»Šå˜ with odds directly after)
                                    match3 = re.match(r'^[\d]*[\s\t]*(\d)-(\d)(\d+\.\d+)$', line)
                                    if match3:
                                        c1, c2, odds_str = match3.groups()
                                        combo = f"{c1}-{c2}"
                                        odds = float(odds_str)
                                        parsed_odds[combo] = odds
                                        continue
                                
                                if parsed_odds:
                                    st.success(f"âœ… {len(parsed_odds)}ä»¶ã®ã‚ªãƒƒã‚ºã‚’è§£æã—ã¾ã—ãŸ")
                                    
                                    # Extract all combos from strategy_data tickets
                                    # Tickets format: "3é€£å˜ (ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³): 2,9 - 2,9,4 - 9,4,7,8"
                                    all_ai_combos = []
                                    tickets = strategy_data.get('tickets', [])
                                    
                                    for ticket in tickets:
                                        if '3é€£å˜' in ticket and 'ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³' in ticket:
                                            # Parse formation: "2,9 - 2,9,4 - 9,4,7,8"
                                            try:
                                                parts = ticket.split(':')[1].strip().split(' - ')
                                                if len(parts) == 3:
                                                    pos1 = [x.strip() for x in parts[0].split(',')]
                                                    pos2 = [x.strip() for x in parts[1].split(',')]
                                                    pos3 = [x.strip() for x in parts[2].split(',')]
                                                    # Generate all combinations
                                                    for p1 in pos1:
                                                        for p2 in pos2:
                                                            for p3 in pos3:
                                                                if p1 != p2 and p2 != p3 and p1 != p3:
                                                                    all_ai_combos.append(f"{p1}-{p2}-{p3}")
                                            except:
                                                pass
                                    
                                    # Also add top 3 as fallback
                                    sorted_df = df_scored.sort_values('final_score', ascending=False)
                                    top_cars = [str(sorted_df['è»Šç•ª'].iloc[i]) for i in range(min(3, len(sorted_df)))]
                                    if len(top_cars) >= 3:
                                        all_ai_combos.append(f"{top_cars[0]}-{top_cars[1]}-{top_cars[2]}")
                                    
                                    # Find matches between AI combos and parsed odds
                                    matched_combos = []
                                    for combo in all_ai_combos:
                                        if combo in parsed_odds:
                                            matched_combos.append((combo, parsed_odds[combo]))
                                    
                                    # Display matched combos with EV
                                    if matched_combos:
                                        st.markdown("**ğŸ¯ AIæ¨å¥¨ Ã— ã‚ªãƒƒã‚ºç…§åˆçµæœ:**")
                                        base_win_rate = strategy_data.get('top_win_rate', 15)  # Base rate for primary
                                        
                                        for i, (combo, odds) in enumerate(sorted(matched_combos, key=lambda x: x[1])):
                                            # Adjust win rate based on position (lower odds = higher rate)
                                            adjusted_rate = base_win_rate * (1 - i * 0.15)  # Decay for lower priority
                                            ev = (adjusted_rate / 100) * odds - 1
                                            ev_color = "ğŸŸ¢" if ev > 0 else "ğŸ”´"
                                            st.write(f"  {ev_color} **{combo}**: {odds}å€ â†’ æœŸå¾…å€¤ {ev:+.2f}")
                                    else:
                                        st.info("AIæ¨å¥¨è²·ã„ç›®ã¨ä¸€è‡´ã™ã‚‹ã‚ªãƒƒã‚ºãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                                        st.caption(f"AIæ¨å¥¨: {', '.join(all_ai_combos[:5])}...")
                                    
                                    # Show all parsed odds
                                    st.write("**è§£ææ¸ˆã¿ã‚ªãƒƒã‚º:**")
                                    for combo, odds in sorted(parsed_odds.items(), key=lambda x: x[1])[:10]:
                                        st.write(f"  {combo}: {odds}å€")


                       # Optional: JSON debug (collapsed)
                       with st.expander("ğŸ” ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ (JSON)"):
                           st.json(structured_bets)
                   
                # --- NEW: Player Detail Analysis (Old Wing Restoration) ---
                st.markdown("---")
                st.markdown("### ğŸ” å‡ºå ´é¸æ‰‹ è©³ç´°åˆ†æ (Old Wing)")
                
                # Check if we have valid player names
                p_names = df_race['é¸æ‰‹å'].unique().tolist() if 'é¸æ‰‹å' in df_race.columns else []
                
                if p_names:
                    # Use unique key per race
                    selected_player = st.selectbox("é¸æ‰‹ã‚’é¸æŠã—ã¦è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ:", p_names, key=f"p_select_{selected_label}")
                    
                    if selected_player:
                        # Find the row
                        p_row = df_race[df_race['é¸æ‰‹å'] == selected_player].iloc[0]
                        
                        # Call Logic
                        with st.spinner(f"{selected_player} é¸æ‰‹ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­..."):
                            # Pass 'meta' instead of undefined 'meta_info'
                            detail_res = logic_v2.analyze_player_detailed_stats(p_row, meta)
                        
                        if detail_res and 'basic' in detail_res:
                            # Display Labels
                            labels = detail_res.get('labels', [])
                            if labels:
                                st.success(" ".join(labels))
                            else:
                                st.info("ç‰¹ç­†ã™ã¹ãå±æ€§ï¼ˆé­”äººãƒ»ã‚µãƒã‚¤ãƒãƒ¼ç­‰ï¼‰ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                            
                            # Display Stats Columns
                            c1, c2, c3 = st.columns(3)
                            
                            # 1. Basic (Last 1 year)
                            bs = detail_res['basic']
                            with c1:
                                st.markdown("#### ğŸ“Š ç›´è¿‘1å¹´æˆç¸¾")
                                st.metric("å‹ç‡", f"{bs['win_rate']:.1f}%")
                                st.metric("2é€£å¯¾ç‡", f"{bs['ren2_rate']:.1f}%")
                                st.metric("3é€£å¯¾ç‡", f"{bs['ren3_rate']:.1f}%")
                                st.caption(f"å¯¾è±¡: ç›´è¿‘ {bs['total']} èµ°")

                            # 2. Condition Match
                            cs = detail_res.get('condition', {})
                            with c2:
                                st.markdown("#### ğŸ”§ åŒæ¡ä»¶æˆç¸¾")
                                if cs:
                                    st.metric("å‹ç‡", f"{cs['win_rate']:.1f}%", delta=f"{cs['win_rate']-bs['win_rate']:.1f}%")
                                    st.metric("2é€£å¯¾ç‡", f"{cs['ren2_rate']:.1f}%", delta=f"{cs['ren2_rate']-bs['ren2_rate']:.1f}%")
                                    st.metric("3é€£å¯¾ç‡", f"{cs['ren3_rate']:.1f}%", delta=f"{cs['ren3_rate']-bs['ren3_rate']:.1f}%")
                                    match_names = ",".join(cs.get('match_conditions', []))
                                    st.caption(f"ä»Šå›ã®ãƒ©ã‚¤ãƒ³é•·ãƒ»ä½ç½®ã¨åŒã˜æ™‚ã®æˆç¸¾ ({cs['match_count']}èµ°)")
                                else:
                                    st.warning("è©²å½“ãƒ‡ãƒ¼ã‚¿ãªã—")

                            # 3. Bank Match
                            bks = detail_res.get('bank', {})
                            with c3:
                                st.markdown("#### ğŸ° é¡ä¼¼ãƒãƒ³ã‚¯æˆç¸¾")
                                if bks:
                                    st.metric("å‹ç‡", f"{bks['win_rate']:.1f}%", delta=f"{bks['win_rate']-bs['win_rate']:.1f}%")
                                    st.metric("2é€£å¯¾ç‡", f"{bks['ren2_rate']:.1f}%", delta=f"{bks['ren2_rate']-bs['ren2_rate']:.1f}%")
                                    st.metric("3é€£å¯¾ç‡", f"{bks['ren3_rate']:.1f}%", delta=f"{bks['ren3_rate']-bs['ren3_rate']:.1f}%")
                                    match_names = ",".join(bks.get('match_banks', []))
                                    st.caption(f"é¡ä¼¼: {match_names} ãªã© ({bks['total']}èµ°)")
                                else:
                                    st.warning("è©²å½“ãƒ‡ãƒ¼ã‚¿ãªã—")
                                    
                            # 4. History Table (User Request)
                            if 'history_df' in detail_res:
                                st.markdown("#### ğŸ“œ éå»èµ°ãƒ‡ãƒ¼ã‚¿ (ãƒ©ã‚¤ãƒ³æ§‹æˆãƒ»ç€é †)")
                                h_df = detail_res['history_df']
                                
                                # Select & Rename Columns for Display
                                target_cols = [
                                    ('æ—¥ä»˜', 'æ—¥ä»˜'), 
                                    ('ç«¶è¼ªå ´', 'å ´'), 
                                    ('ãƒ¬ãƒ¼ã‚¹ç•ªå·', 'R'), 
                                    ('ç€é †', 'ç€'), 
                                    ('æ±ºã¾ã‚Šæ‰‹', 'æ±º'), 
                                    ('line_length', 'ãƒ©ã‚¤ãƒ³é•·'), 
                                    ('line_pos', 'ä½ç½®'),
                                    ('ãƒã‚¸ã‚·ãƒ§ãƒ³', 'ä½ç½®'), # Fallback
                                    ('lines_parsed', 'ä¸¦ã³') # Optional
                                ]
                                
                                disp_cols = []
                                rename_dict = {}
                                
                                for col, label in target_cols:
                                    if col in h_df.columns:
                                        if label not in rename_dict.values(): # Avoid duplicate columns
                                            disp_cols.append(col)
                                            rename_dict[col] = label
                                
                                if disp_cols:
                                    st.dataframe(
                                        h_df[disp_cols].rename(columns=rename_dict).head(50), 
                                        use_container_width=True,
                                        height=300
                                    )
                                else:
                                    st.info("è¡¨ç¤ºå¯èƒ½ãªå±¥æ­´ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        else:
                            st.error("è©³ç´°ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ (éå»ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å¯èƒ½æ€§)")
                else:
                    st.warning("é¸æ‰‹åãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                


                # --- æ–°ãƒ­ã‚¸ãƒƒã‚¯è§£èª¬ã‚¨ãƒªã‚¢ (Local) ---
                st.markdown("---")
                st.caption("ğŸ“ **ãƒ‡ãƒ¼ã‚¿ã§ç†±ç‹‚ã‚’ã¤ã‹ã‚! ãƒ­ã‚¸ãƒƒã‚¯è§£èª¬**")
                
                def check_reason(df, keyword):
                    if 'bonus_reasons' not in df.columns: return []
                    return df[df['bonus_reasons'].astype(str).str.contains(keyword, na=False)]['è»Šç•ª'].tolist()

                # 1. ğŸ’£ é­”äºº (åƒåˆ‡ã‚Œ)
                majin_cars = check_reason(df_scored, "é­”äºº")
                if majin_cars:
                    cars_str = ",".join(map(str, majin_cars))
                    st.info(f"**ğŸ’£ ãƒ©ã‚¤ãƒ³ã‚¯ãƒ©ãƒƒã‚·ãƒ£ãƒ¼ (è»Šç•ª: {cars_str})**\n\n"
                            "è‡ªåˆ†ãŒé€ƒã’æ®‹ã£ãŸï¼ˆ2ç€ä»¥å†…ï¼‰ã®ã«ã€å¾Œã‚ã®é¸æ‰‹ã‚’ç½®ãå»ã‚Šï¼ˆ4ç€ä»¥ä¸‹ï¼‰ã«ã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚\n"
                            "â†’ **ã‚¹ã‚¸é•ã„ï¼ˆãƒ©ã‚¤ãƒ³ä¸æˆç«‹ï¼‰**ã‚’ç‹™ã†ãƒãƒ£ãƒ³ã‚¹ã§ã™ã€‚")

                # 2. ğŸ—¡ï¸ å·®ã—é€†è»¢ (ã‚ºãƒ–ã‚ºãƒ–)
                zubu_cars = check_reason(df_scored, "å·®é€†")
                if zubu_cars:
                    cars_str = ",".join(map(str, zubu_cars))
                    st.info(f"**ğŸ—¡ï¸ å·®ã—è„šé‹­ã„ (è»Šç•ª: {cars_str})**\n\n"
                            "ç•ªæ‰‹ã‹ã‚‰1ç€ã‚’å–ã‚Šã¤ã¤ã€å‰ã®é¸æ‰‹ã‚‚2ç€ã«æ®‹ã™ï¼ˆã‚ºãƒ–ã‚ºãƒ–ï¼‰å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚\n"
                            "â†’ **ãƒ©ã‚¤ãƒ³ãƒ¯ãƒ³ãƒ„ãƒ¼ï¼ˆå·®ã—ç›®ï¼‰**ã‚’åšã‚ã«ã€‚")

                # 3. ğŸƒ ã‚µãƒã‚¤ãƒãƒ¼
                survivor_cars = check_reason(df_scored, "ã‚µãƒã‚¤ãƒãƒ¼")
                if survivor_cars:
                    cars_str = ",".join(map(str, survivor_cars))
                    st.info(f"**ğŸƒ ã‚µãƒã‚¤ãƒãƒ¼ (è»Šç•ª: {cars_str})**\n\n"
                            "å‰ã®é¸æ‰‹ãŒãƒœãƒ­è² ã‘ã—ã¦ã‚‚ã€è‡ªåˆ†ã ã‘3ç€ä»¥å†…ã«çªã£è¾¼ã‚“ã§ãã‚‹ã€Œç©´é¸æ‰‹ã€ã§ã™ã€‚\n"
                            "â†’ ãƒ©ã‚¤ãƒ³ãŒå¼±ãã¦ã‚‚ã€æ··æˆ¦ã«ãªã‚Œã°**ãƒ’ãƒ¢ï¼ˆ3ç€ï¼‰**ã‚„é ­ã§æµ®ä¸Šã—ã¾ã™ã€‚")

                # 4. ğŸ‡ªğŸ‡º æ¬§å·ç©´ (äº‹æ•…è¦å“¡)
                euro_cars = check_reason(df_scored, "æ¬§å·")
                if euro_cars:
                    cars_str = ",".join(map(str, euro_cars))
                    st.info(f"**ğŸ‡ªğŸ‡º äº‹æ•…è¦å“¡ (è»Šç•ª: {cars_str})**\n\n"
                            "4ãƒ»6ãƒ»8ç•ªè»Šãªã©ã®äººæ°—è–„ãƒ»å˜é¨æ§‹æˆã§3ç€ä»¥å†…ã«æ¥ã‚‹ã€Œä¸€ç™ºå±‹ã€ã§ã™ã€‚\n"
                            "â†’ é«˜é…å½“ç‹™ã„ãªã‚‰ã€3é€£å˜ã®3ç€ã«å…¥ã‚Œã¦ãŠãä¾¡å€¤ãŒã‚ã‚Šã¾ã™ã€‚")

                # 5. ğŸ’’ ç›¸æ€§è‰¯
                love_cars = check_reason(df_scored, "ç›¸æ€§è‰¯")
                if love_cars:
                    cars_str = ",".join(map(str, love_cars))
                    st.success(f"**ğŸ’’ ãƒãƒ³ã‚¯ç›¸æ€§æŠœç¾¤ (è»Šç•ª: {cars_str})**\n\n"
                               "ã“ã®ç«¶è¼ªå ´ã§ã®é€£å¯¾ç‡ï¼ˆ2ç€ä»¥å†…ç‡ï¼‰ãŒ50%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚\n"
                               "â†’ ç†å±ˆæŠœãã§è²·ã„ç›®ã«å…¥ã‚Œã‚‹ã¹ãã€Œå¾—æ„ãƒãƒ³ã‚¯ã€ã®é¸æ‰‹ã§ã™ã€‚")


                # --- AI Reporter Section (API) ---
                st.markdown("---")
                try:
                    if st.button("ğŸ¤– AIæˆ¦æ³ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ (è¨˜è€…ãƒ¢ãƒ¼ãƒ‰)"):
                        if not api_key_input:
                            st.error("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§Gemini APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
                        else:
                            with st.spinner("æ•è…•è¨˜è€…ãŒè¨˜äº‹ã‚’åŸ·ç­†ä¸­..."):
                                # Create Meta - include parsed lines for accurate reporting
                                meta_info = {
                                    'place': place_name, 
                                    'race_class': race_class, 
                                    'race_num': df_scored['ãƒ¬ãƒ¼ã‚¹ç•ªå·'].iloc[0] if 'ãƒ¬ãƒ¼ã‚¹ç•ªå·' in df_scored.columns else '?',
                                    'lines_parsed': meta.get('lines_parsed', '')  # Add correct line configuration
                                }
                                
                                # Generate Special Bonus Strategy as Main Strategy
                                strategy_data = logic_v2.generate_bonus_strategy(df_scored, score_col='final_score')
                                strategy_data['title'] = "ç‰¹æ³¨äºˆæƒ³ (ãƒœãƒ¼ãƒŠã‚¹é‡è¦–)"
                                
                                report_text = logic_v2.generate_race_report(df_scored, meta_info, strategy_data, api_key_input)
                                
                                # Save Prediction History (As Main)
                                try:
                                    # Normalize Date
                                    d_raw = meta.get('date', datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'))
                                    d_clean = d_raw.replace('-', 'å¹´').replace('/', 'å¹´')
                                    if 'å¹´' not in d_clean:
                                         try: d_clean = datetime.strptime(d_raw, "%Y-%m-%d").strftime("%Yå¹´%mæœˆ%dæ—¥")
                                         except: pass
                                    
                                    # Normalize Race Num (Fix 1RR bug)
                                    cur_r_num = str(meta_info.get('race_num', '1R')).replace('R', '')
                                    cur_r_num = f"{cur_r_num}R"
                                    
                                    pred_data = {
                                        "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                        "place": place_name,
                                        "race_num": cur_r_num,
                                        "date": d_clean,
                                        "prediction_text": report_text,
                                        "tickets": strategy_data.get('tickets', []),
                                        "structured_bets": strategy_data.get('structured_bets', []),  # Added for stats calc
                                        "strategy_title": strategy_data.get('title', 'Special'),
                                        "strategy_type": "special_bonus", # Mark as special
                                        "ai_indices": df_scored[['è»Šç•ª', 'final_score', 'é¸æ‰‹å', 'ai_tag']].to_dict('records') if 'final_score' in df_scored.columns else []
                                    }
                                    if db_utils.save_prediction(pred_data):
                                        st.toast("âœ… ç‰¹æ³¨äºˆæƒ³ã‚’å±¥æ­´ã«ä¿å­˜ã—ã¾ã—ãŸ")
                                except Exception as e_save:
                                    print(f"History Save Error: {e_save}")
                                    

                                        
                                except Exception as e_save:
                                    print(f"History Save Error: {e_save}")
                                
                                st.subheader("ğŸ“° æœ¬æ—¥ã®äºˆæƒ³ã‚³ãƒ©ãƒ ")
                                st.markdown(report_text, unsafe_allow_html=True)
                                st.info("â€» ã“ã®è¨˜äº‹ã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã«åŸºã¥ãAIãŒè‡ªå‹•ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚")

                except Exception as e:
                    st.error(f"è¨˜è€…ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼ (API): {e}")
                
                # --- AI Chat Assistant Section ---
                st.markdown("---")
                with st.expander("ğŸ’¬ AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«è³ªå•ã™ã‚‹ï¼ˆã“ã®ãƒ¬ãƒ¼ã‚¹ã«ã¤ã„ã¦ï¼‰"):
                    # Initialize chat history in session state (per race)
                    chat_key = f"chat_history_{selected_label}"
                    if chat_key not in st.session_state:
                        st.session_state[chat_key] = []
                    
                    # Build context data for the AI
                    # (Only recalculate when needed, not on every rerun)
                    context_key = f"chat_context_{selected_label}"
                    if context_key not in st.session_state:
                        # Build player text
                        p_lines = []
                        for _, row in df_scored.iterrows():
                            c = row['è»Šç•ª']
                            n = row['é¸æ‰‹å']
                            score = row.get('final_score', row.get('ç«¶èµ°å¾—ç‚¹', 0))
                            reasons = str(row.get('bonus_reasons', ''))
                            p_lines.append(f"{c}ç•ª: {n} (AIã‚¹ã‚³ã‚¢:{score:.1f}) {reasons}")
                        
                        players_text = "\n".join(p_lines)
                        
                        # Strategy info
                        strat_title = strategy_data.get('title', 'æ¨™æº–')
                        tickets = ", ".join(strategy_data.get('tickets', []))
                        strategy_info = f"æˆ¦ç•¥: {strat_title}\næ¨å¥¨: {tickets}"
                        
                        # Logic info (detected flags)
                        logic_parts = []
                        if majin_cars: logic_parts.append(f"é­”äººç³»: {majin_cars}")
                        if survivor_cars: logic_parts.append(f"ã‚µãƒã‚¤ãƒãƒ¼: {survivor_cars}")
                        if euro_cars: logic_parts.append(f"æ¬§å·ç©´: {euro_cars}")
                        if love_cars: logic_parts.append(f"ç›¸æ€§è‰¯: {love_cars}")
                        logic_info = "\n".join(logic_parts) if logic_parts else "ç‰¹ç­†ã™ã¹ããƒ•ãƒ©ã‚°ãªã—"
                        
                        st.session_state[context_key] = {
                            'place': place_name,
                            'race_num': df_scored['ãƒ¬ãƒ¼ã‚¹ç•ªå·'].iloc[0] if 'ãƒ¬ãƒ¼ã‚¹ç•ªå·' in df_scored.columns else '?',
                            'players_text': players_text,
                            'strategy_info': strategy_info,
                            'logic_info': logic_info
                        }
                    
                    context_data = st.session_state[context_key]
                    
                    # Display chat history
                    for message in st.session_state[chat_key]:
                        with st.chat_message(message["role"]):
                            st.markdown(message["content"])
                    
                    # Chat input
                    if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ï¼ˆä¾‹: 1ç•ªã¯è²·ãˆã‚‹ï¼Ÿ ã“ã®ãƒ¬ãƒ¼ã‚¹ã¯è’ã‚Œãã†ï¼Ÿï¼‰"):
                        # Add user message
                        st.session_state[chat_key].append({"role": "user", "content": prompt})
                        with st.chat_message("user"):
                            st.markdown(prompt)
                        
                        # Generate AI response
                        if not api_key_input:
                            response = "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰è¨­å®šã—ã¦ãã ã•ã„ã€‚"
                        else:
                            with st.spinner("AIè€ƒãˆä¸­..."):
                                response = logic_v2.generate_chat_response(
                                    st.session_state[chat_key],
                                    context_data,
                                    api_key_input
                                )
                        
                        # Add assistant message
                        st.session_state[chat_key].append({"role": "assistant", "content": response})
                        with st.chat_message("assistant"):
                            st.markdown(response)
                        
                        
                        # Note: Removed st.rerun() to prevent article from disappearing
                        
            except Exception as e:
                st.error(f"è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")



# ------------------------------------------
# Tab 2: é¸æ‰‹ãƒ»ãƒ­ã‚¸ãƒƒã‚¯æ¤œç´¢ (æ–°æ©Ÿèƒ½)
# ------------------------------------------
with tab2:
    st.header("ğŸ•µï¸ é¸æ‰‹è§£æãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹")
    st.caption("ã€ŒFinal Analysisã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ã—ã€æ¡ä»¶ã«åˆè‡´ã™ã‚‹å±é™ºãªé¸æ‰‹ï¼ˆé­”äººãƒ»ã‚µãƒã‚¤ãƒãƒ¼ç­‰ï¼‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚")
    
    # Place Selection (To load specific file)
    l_place = st.selectbox("ç«¶è¼ªå ´ãƒ‡ãƒ¼ã‚¿é¸æŠ", ["æ¾é˜ª", "é˜²åºœ", "å‰æ©‹", "å¹³å¡š", "å°å€‰"], index=0)
    
    if st.button("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"):
        # Load stats
        df_logic = engine.get_final_analysis_stats(l_place)
        if df_logic is not None:
            st.session_state['logic_df'] = df_logic
            st.success(f"{l_place}ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ ({len(df_logic)}å)")
        else:
            st.error(f"{l_place}ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (logic_data/{l_place}/..._final.xlsx)")

    if 'logic_df' in st.session_state:
        df_l = st.session_state['logic_df']
        
        # Filters
        c1, c2, c3, c4 = st.columns(4)
        f_chigire = c1.checkbox("ï¿½ é­”äºº (åƒåˆ‡ã‚Œ)", value=False)
        f_hyena = c2.checkbox("ğŸƒ ã‚µãƒã‚¤ãƒãƒ¼", value=False)
        f_predator = c3.checkbox("ğŸ—¡ï¸ å·®ã—é€†è»¢", value=False)
        f_europe = c4.checkbox("ğŸ‡ªğŸ‡º æ¬§å·ç©´", value=False)
        
        # Filter Logic
        filtered_df = df_l.copy()
        
        if f_chigire and 'A_åƒåˆ‡ã‚Œãƒ•ãƒ©ã‚°' in filtered_df.columns:
            # Need Mean? The final analysis file usually has aggregated stats OR raw race rows.
            # If Raw Rows: Group by Player and calc mean.
            # If the file is "Analysis Final", it might be raw history.
            # Let's check columns. Step 867 showed: 'A_åƒåˆ‡ã‚Œãƒ•ãƒ©ã‚°', 'é¸æ‰‹å' etc.
            # And many rows per player. So we need to aggregate.
            
            # Aggregate Mode
            st.info("é›†è¨ˆä¸­... (åˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)")
            
            # Group by Player Name
            # We want players who have HIGH rate.
            # Calculate means for all flags first?
            g_cols = ['é¸æ‰‹å', 'A_åƒåˆ‡ã‚Œãƒ•ãƒ©ã‚°', 'B_ãƒã‚¤ã‚¨ãƒŠãƒ•ãƒ©ã‚°', 'A_å·®ã—é€†è»¢ãƒ•ãƒ©ã‚°', 'B_ç©´é©æ€§_æ¬§å·', 'ç«¶èµ°å¾—ç‚¹']
            # Ensure cols exist
            g_cols = [c for c in g_cols if c in filtered_df.columns]
            
            grp = filtered_df.groupby('é¸æ‰‹å')[g_cols].mean(numeric_only=True).reset_index()
            
            # Apply Filters
            if f_chigire:
                grp = grp[grp['A_åƒåˆ‡ã‚Œãƒ•ãƒ©ã‚°'] >= 0.30]
            if f_hyena:
                grp = grp[grp['B_ãƒã‚¤ã‚¨ãƒŠãƒ•ãƒ©ã‚°'] >= 0.15]
            if f_predator:
                grp = grp[grp['A_å·®ã—é€†è»¢ãƒ•ãƒ©ã‚°'] >= 0.20]
            if f_europe:
                grp = grp[grp['B_ç©´é©æ€§_æ¬§å·'] >= 0.10]
            
            st.dataframe(grp.sort_values('ç«¶èµ°å¾—ç‚¹', ascending=False))
        else:
            # Show Raw if no filter or aggregate all
            st.dataframe(df_l.head(100))
            st.caption("â€»ãƒ•ã‚£ãƒ«ã‚¿æœªé©ç”¨æ™‚ã¯å…ˆé ­100è¡Œã®ã¿è¡¨ç¤º")

# ------------------------------------------
# Tab 3: ã‚¨ãƒ³ã‚¸ãƒ³çŠ¶æ…‹
# ------------------------------------------
with tab3:
    st.header("ãƒ‡ãƒ¼ã‚¿åˆ†æ (Polars Engine Status)")
    if engine.current_place:
        st.success(f"ç¾åœ¨ãƒ­ãƒ¼ãƒ‰ä¸­ã®ãƒ‡ãƒ¼ã‚¿: {engine.current_place}")
        # db keys are 'ALL' or PlaceName
        st.json({k: str(type(v)) for k, v in engine.db.items()})
    else:
        st.info("å‡ºèµ°è¡¨ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€å¯¾å¿œã™ã‚‹ç«¶è¼ªå ´ã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚")

with tab4:
    st.header("è¨­å®š")
    if st.button("DBæ¥ç¶šãƒ†ã‚¹ãƒˆ"):
        try:
            conn = sqlite3.connect(db_utils.DB_PATH)
            st.success("OK: æ¥ç¶šæˆåŠŸ")
            conn.close()
        except Exception as e:
            st.error(f"æ¥ç¶šå¤±æ•—: {e}")

# ------------------------------------------
# Tab 5: AIçš„ä¸­å±¥æ­´
# ------------------------------------------
with tab5:
    st.header("ğŸ“œ AIçš„ä¸­å±¥æ­´ & å›åç‡åˆ†æ")
    
    if st.button("ğŸ”„ å±¥æ­´ã¨åˆ†æã‚’æ›´æ–°", use_container_width=False, key="refresh_hist"):
        st.rerun()

    # --- History Loading Optimization ---
    st.markdown("---")
    col_filter, _ = st.columns([2, 3])
    with col_filter:
        hist_mode = st.radio(
            "è¡¨ç¤ºå¯¾è±¡æœŸé–“",
            ["é‹ç”¨é–‹å§‹å¾Œ (12/31ä»¥é™)", "æ˜¨æ—¥ãƒ»ä»Šæ—¥ã®ã¿", "å…¨æœŸé–“ (é‡ã„)"],
            horizontal=True,
            index=0,
            help="ã€Œé‹ç”¨é–‹å§‹å¾Œã€ã¯2025å¹´12æœˆ31æ—¥ä»¥é™ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"
        )

    # Filter Logic
    from datetime import datetime, timedelta
    now = datetime.now()
    
    cutoff = None
    
    if "12/31ä»¥é™" in hist_mode:
        # Fixed Start Date: 2025-12-31
        cutoff = datetime(2025, 12, 31)
    elif "æ˜¨æ—¥" in hist_mode:
        # Rolling: Today + Yesterday
        cutoff = now - timedelta(days=2)
        cutoff = cutoff.replace(hour=0, minute=0, second=0, microsecond=0)
    
    # Optimized Load
    history = db_utils.load_prediction_history(min_date=cutoff)
        
    if cutoff:
        d_label = cutoff.strftime('%Y/%m/%d')
        st.caption(f"{d_label} ä»¥é™ã®å±¥æ­´ã‚’è¡¨ç¤ºä¸­: {len(history)}ä»¶")
    else:
        st.caption(f"å…¨æœŸé–“ã®å±¥æ­´ã‚’è¡¨ç¤ºä¸­: {len(history)}ä»¶")


    if not history:
        st.info("äºˆæ¸¬å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã€Œå‡ºèµ°è¡¨ãƒ»AIäºˆæ¸¬ã€ã‚¿ãƒ–ã§AIäºˆæ¸¬ã‚’ä½œæˆã™ã‚‹ã¨ã“ã“ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚")
    else:
        # Analyze data
        with st.spinner("ãƒ¬ãƒ¼ã‚¹çµæœã¨ç…§åˆä¸­..."):
            try:
                # df_res: Race Level, df_tickets: Ticket Level
                df_res, stats, df_tickets = logic_v2.analyze_prediction_history(history)
            except Exception as e:
                st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                df_res = pd.DataFrame()
                stats = {}
                df_tickets = pd.DataFrame()
            
        if df_res.empty:
            st.warning("å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰ã€‚")
        else:
            # --- Check for Missing Results ---
            if 'hit_detail' in df_res.columns:
                missing_res = df_res[df_res['hit_detail'] == "çµæœæœªç€"]
                if not missing_res.empty:
                    st.warning(f"âš ï¸ çµæœæœªå–å¾—ã®ãƒ¬ãƒ¼ã‚¹ãŒ {len(missing_res)} ä»¶ã‚ã‚Šã¾ã™ã€‚å›åç‡ã«åæ˜ ã™ã‚‹ã«ã¯çµæœã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚")
                    
                    if st.button(f"å¯¾è±¡ã®{len(missing_res)}ãƒ¬ãƒ¼ã‚¹ã®çµæœã‚’å–å¾—ãƒ»æ›´æ–°ã™ã‚‹"):
                        # Group by Place and Date to minimize requests
                        # missing_res has 'place', 'date'
                        targets = missing_res[['place', 'date']].drop_duplicates()
                        
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        total_tasks = len(targets)
                        success_cnt = 0
                        
                        for idx, (i, row) in enumerate(targets.iterrows()):
                            p_name = row['place']
                            d_str = row['date']
                            
                            # Standardize Date for Scraper (YYYY-MM-DD)
                            try:
                                if "å¹´" in d_str:
                                    dt_obj = datetime.strptime(d_str, "%Yå¹´%mæœˆ%dæ—¥")
                                    search_date = dt_obj.strftime("%Y-%m-%d")
                                else:
                                    search_date = d_str
                            except:
                                search_date = d_str
                            
                            status_text.text(f"å–å¾—ä¸­ ({idx+1}/{total_tasks}): {p_name} {d_str}")
                            
                            try:
                                # Fetch data (This gets the WHOLE day, which includes results if available)
                                # date format in history is usually YYYY-MM-DD. scraper expects YYYY-MM-DD.
                                scraped = scraper.fetch_race_data(p_name, search_date, search_date)
                                
                                if scraped:
                                    # Save to DB
                                    count, msg = db_utils.save_race_data(scraped, overwrite=True)
                                    if count > 0:
                                        success_cnt += 1
                                else:
                                    st.warning(f"{p_name} {d_str}: ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                            except Exception as e:
                                st.error(f"Error {p_name} {d_str}: {e}")
                                
                            progress_bar.progress((idx + 1) / total_tasks)
                            
                        status_text.text("å®Œäº†ï¼")
                        if success_cnt > 0:
                            st.success(f"{success_cnt} é–‹å‚¬æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼")
                            st.rerun()
                        else:
                            st.error("ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆçµæœãŒã¾ã å…¬é–‹ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰")

            # 1. Filters (Top Level)
            st.markdown("### ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°")
            all_places = ["å…¨å ´"] + sorted(df_res['place'].unique().tolist())
            col_f1, col_f2 = st.columns([1, 3])
            sel_place = col_f1.selectbox("é–‹å‚¬å ´ã‚’é¸æŠ", all_places)
            
            # --- Filter Data ---
            df_disp = df_res.copy()
            
            # Ensure race_id exists for filtering sync
            if 'race_id' not in df_disp.columns and not df_disp.empty:
                df_disp['race_id'] = df_disp.apply(
                    lambda x: f"{x.get('place')}_{x.get('date')}_{str(x.get('race_num','')).replace('R','')+'R'}", 
                    axis=1
                )
            
            df_tick_disp = df_tickets.copy()
            
            # --- Feature Engineering for Filters ---
            if not df_disp.empty:
                # 1. Calculate Bonus Value from AI Indices (Tag Parsing)
                def calc_bonus_from_indices(row):
                    try:
                        indices = row.get('ai_indices', [])
                        if isinstance(indices, str):
                            import json
                            indices = json.loads(indices)
                        
                        max_bonus = 0.0
                        
                        for item in indices:
                            # Parse tags from ai_tag string or bonus_reasons columns if available
                            # Batch saves 'ai_tag'.
                            tags = str(item.get('ai_tag', '')) 
                            # Naive parsing of known tags
                            b = 0.0
                            if '[åœ°å…ƒ]' in tags: b += 3.0
                            if 'No.1' in tags: b += 2.0 * tags.count('No.1') # Each No.1 is +2
                            if 'ç›´ç·š' in tags or 'å‚¾æ–œ' in tags: b += 2.0 # Bank specs
                            if 'ãƒ©ã‚¤ãƒ³' in tags and '3' in tags: b += 1.0 # Line bonus (approx)
                            
                            if b > max_bonus: max_bonus = b
                            
                        return max_bonus
                    except:
                        return 0.0

                if 'bonus_value' not in df_disp.columns:
                    df_disp['bonus_value'] = df_disp.apply(calc_bonus_from_indices, axis=1)
                else:
                    # If it exists but is all 0, recalc
                    if df_disp['bonus_value'].sum() == 0:
                         df_disp['bonus_value'] = df_disp.apply(calc_bonus_from_indices, axis=1)
                
                df_disp['bonus_value'] = pd.to_numeric(df_disp['bonus_value'], errors='coerce').fillna(0.0)
                
                # 2. Calculate Score Gap
                def get_score_gap(row):
                    try:
                        indices = row.get('ai_indices', [])
                        if isinstance(indices, str): # Handle stringified JSON
                            import json
                            indices = json.loads(indices)
                        if not isinstance(indices, list) or len(indices) < 2:
                            return 0.0
                        
                        # Extract scores, handling potential malformed data
                        scores = []
                        for x in indices:
                            try: scores.append(float(x.get('final_score', 0)))
                            except: pass
                        
                        if len(scores) < 2: return 0.0
                        
                        scores.sort(reverse=True)
                        return scores[0] - scores[1]
                    except:
                        return 0.0

                df_disp['score_gap'] = df_disp.apply(get_score_gap, axis=1)
            else:
                 df_disp['score_gap'] = 0.0

            # --- Extended Filters ---
            col_f3, col_f4 = st.columns(2)
            min_bonus = col_f3.number_input("æœ€å°ãƒœãƒ¼ãƒŠã‚¹åŠ ç‚¹ (0~20)", min_value=0, max_value=20, value=0, step=1)
            min_gap = col_f4.number_input("æœ€å°æŒ‡æ•°å·® (å¤§å·®:7~)", min_value=0.0, max_value=30.0, value=0.0, step=1.0)
            
            # Apply Filters
            if min_bonus > 0:
                df_disp = df_disp[df_disp['bonus_value'] >= min_bonus]
            
            if min_gap > 0:
                df_disp = df_disp[df_disp['score_gap'] >= min_gap]
            
            # Sync Tickets Filter
            if 'strategy_type' in df_tick_disp.columns:
                df_tick_disp = df_tick_disp[df_tick_disp['strategy_type'] == 'special_bonus']
            
            # Logic to sync tick_disp with filtered races is tricky because ticket DF lacks race-level metrics like Gap/Bonus easily
            # But we can filter by race_id or index matching?
            # df_disp has indices from df_res. 
            # Easiest: Filter tick_disp to include only races present in filtered df_disp
            if not df_disp.empty:
                valid_rids = df_disp['race_id'].unique() if 'race_id' in df_disp.columns else []
                # Fallback if race_id missing
                if 'race_id' in df_tick_disp.columns:
                     df_tick_disp = df_tick_disp[df_tick_disp['race_id'].isin(valid_rids)]
            else:
                df_tick_disp = df_tick_disp.iloc[0:0] # Empty it
            
            if sel_place != "å…¨å ´":
                df_disp = df_disp[df_disp['place'] == sel_place]
                if not df_tick_disp.empty:
                    df_tick_disp = df_tick_disp[df_tick_disp['place'] == sel_place]
            
            # --- Recalculate Stats for Display ---
            # Include only Settled Races for statistics (User Request)
            df_calc = df_disp[~df_disp['hit_detail'].isin(["çµæœæœªç€", "çµæœå¾…/ç„¡"])]
            
            if not df_calc.empty:
                disp_invest = df_calc['investment'].sum()
                disp_return = df_calc['benefit'].sum()
                disp_bal = disp_return - disp_invest
                disp_rec = (disp_return / disp_invest * 100) if disp_invest > 0 else 0.0
                disp_hit = df_calc['is_hit'].sum()
                
                disp_hit_rate = (disp_hit / len(df_calc) * 100) if len(df_calc) > 0 else 0.0
                disp_cnt = len(df_calc)
            else:
                disp_invest = 0
                disp_return = 0
                disp_bal = 0
                disp_rec = 0.0
                disp_hit = 0
                disp_hit_rate = 0.0
                disp_cnt = 0


            st.divider()

            # 2. Summary Metrics (Filtered)
            st.markdown(f"### ğŸ“Š æˆç¸¾ã‚µãƒãƒªãƒ¼ ({sel_place})")
            
            # Counts
            total_cnt = len(df_disp)
            settled_cnt = len(df_calc)
            
            m1, m2, m3, m4 = st.columns(4)
            m1.metric("ç·äºˆæƒ³ãƒ¬ãƒ¼ã‚¹", f"{total_cnt}R", f"ã†ã¡ç¢ºå®š {settled_cnt}R")
            
            if settled_cnt > 0:
                m2.metric("çš„ä¸­æ•° (ç‡)", f"{disp_hit}R ({disp_hit_rate:.1f}%)")
                rec_delta = disp_rec - 100.0
                m3.metric("å›åç‡", f"{disp_rec:.1f}%", delta=f"{rec_delta:.1f}%")
                m4.metric("ç·åæ”¯", f"{int(disp_bal) if pd.notna(disp_bal) else 0:,}å††", delta=f"{int(disp_bal) if pd.notna(disp_bal) else 0:,}å††")
            else:
                 m2.metric("çš„ä¸­æ•° (ç‡)", "-")
                 m3.metric("å›åç‡", "-")
                 m4.metric("ç·åæ”¯", "-")
                 st.caption("â€» çµæœãŒç¢ºå®šã—ãŸãƒ¬ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆã™ã¹ã¦çµæœæœªç€ã¾ãŸã¯é™¤å¤–ï¼‰")
                 m4.metric("ç·åæ”¯", "-")
                 st.caption("â€» çµæœãŒç¢ºå®šã—ãŸãƒ¬ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆã™ã¹ã¦çµæœæœªç€ã¾ãŸã¯é™¤å¤–ï¼‰")
            
            st.divider()
            
            # 3. Ticket Type Stats (Filtered)
            st.markdown(f"### ğŸ¯ åˆ¸ç¨®åˆ¥æˆç¸¾ ({sel_place})")
            
            if not df_tick_disp.empty:
                # Group by Ticket Type
                grp = df_tick_disp.groupby('type').agg({
                    'invest': 'sum',
                    'return': 'sum',
                    'is_hit': 'sum',
                    'type': 'count'
                }).rename(columns={'type':'ticket_count'})
                
                # Calc Rates
                grp['balance'] = grp['return'] - grp['invest']
                grp['recovery_rate'] = (grp['return'] / grp['invest'] * 100).fillna(0.0)
                grp['hit_rate'] = (grp['is_hit'] / grp['ticket_count'] * 100).fillna(0.0)
                
                # Format for Display
                grp = grp.reset_index()
                # Sort by invest desc
                disp_grp = grp.sort_values('invest', ascending=False)
                
                # Rename Columns
                disp_grp.columns = ['åˆ¸ç¨®', 'è³¼å…¥é¡', 'æ‰•æˆ»é¡', 'çš„ä¸­æ•°', 'ç·æ•°', 'åæ”¯', 'å›åç‡', 'çš„ä¸­ç‡']
                
                # Reorder
                disp_grp = disp_grp[['åˆ¸ç¨®', 'ç·æ•°', 'çš„ä¸­æ•°', 'çš„ä¸­ç‡', 'è³¼å…¥é¡', 'æ‰•æˆ»é¡', 'åæ”¯', 'å›åç‡']]
                
                # Format
                st.dataframe(
                    disp_grp.style.format({
                        'çš„ä¸­ç‡': "{:.1f}%",
                        'è³¼å…¥é¡': "{:,.0f}",
                        'æ‰•æˆ»é¡': "{:,.0f}",
                        'åæ”¯': "{:,.0f}",
                        'å›åç‡': "{:.1f}%"
                    }),
                    use_container_width=True,
                    hide_index=True
                )
            else:
                st.info("åˆ¸ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            
            st.divider()

            # 4. Line Strategy Analysis
            st.markdown(f"### ğŸš´ ãƒ©ã‚¤ãƒ³äºˆæƒ³å‚¾å‘åˆ†æ ({sel_place})")
            st.caption("AIã®äºˆæƒ³ã¨ãƒ¬ãƒ¼ã‚¹çµæœãŒã€Œãƒ©ã‚¤ãƒ³ï¼ˆã‚¹ã‚¸ï¼‰ã€æ±ºç€ã ã£ãŸã‹ã€ã€Œåˆ¥ç·šï¼ˆã‚¹ã‚¸é•ã„ï¼‰ã€æ±ºç€ã ã£ãŸã‹ã‚’åˆ†æã—ã¾ã™ã€‚")
            
            # Simple Filter based on UI selection
            if history:
                # 1. Base Filter by Place
                if sel_place == "å…¨å ´":
                    base_history = history
                else:
                    base_history = [h for h in history if h.get('place') == sel_place]
                
                # 2. Sync with df_disp filters (Bonus/Gap)
                # Valid filtered IDs
                if not df_disp.empty and 'race_id' in df_disp.columns:
                    valid_ids = set(df_disp['race_id'].unique())
                    
                    # Need to ensure history items also have race_id to match
                    target_history = []
                    for h in base_history:
                        # Construct ID if missing
                        rid = h.get('race_id')
                        if not rid:
                            r_num = str(h.get('race_num','')).replace('R','') + 'R'
                            rid = f"{h.get('place')}_{h.get('date')}_{r_num}"
                        
                        if rid in valid_ids:
                            target_history.append(h)
                else:
                    # If df_disp is empty (filtered to 0), target_history is empty
                    target_history = []
            
            if target_history:
                # st.write(f"DEBUG: Analyzed {len(target_history)} races") # Debug
                with st.spinner("ãƒ©ã‚¤ãƒ³å‚¾å‘ã‚’åˆ†æä¸­..."):
                    l_stats = logic_v2.analyze_line_strategy_bias(target_history)
                
                if l_stats and l_stats.get('total_races', 0) > 0:
                    tot = l_stats['total_races']
                    
                    # Columns
                    la1, la2 = st.columns(2)
                    
                    with la1:
                        st.subheader("ğŸ¤– AIã®äºˆæƒ³å‚¾å‘")
                        ai_same = l_stats['ai_same_line']
                        ai_sep = l_stats['ai_separate']
                        ai_same_r = ai_same / tot * 100
                        ai_sep_r = ai_sep / tot * 100
                        st.write(f"**ãƒ©ã‚¤ãƒ³æ±ºç€äºˆæƒ³**: {ai_same}R ({ai_same_r:.1f}%)")
                        st.write(f"**åˆ¥ç·š(ã‚¹ã‚¸é•)äºˆæƒ³**: {ai_sep}R ({ai_sep_r:.1f}%)")
                        st.progress(ai_same_r / 100)
                        
                    with la2:
                        st.subheader("ğŸ å®Ÿéš›ã®ãƒ¬ãƒ¼ã‚¹çµæœ")
                        res_same = l_stats['res_same_line']
                        res_sep = l_stats['res_separate']
                        res_same_r = res_same / tot * 100
                        res_sep_r = res_sep / tot * 100
                        st.write(f"**ãƒ©ã‚¤ãƒ³æ±ºç€**: {res_same}R ({res_same_r:.1f}%)")
                        st.write(f"**åˆ¥ç·š(ã‚¹ã‚¸é•)**: {res_sep}R ({res_sep_r:.1f}%)")
                        st.progress(res_same_r / 100)
                    
                    st.write("---")
                    # Match Analysis
                    # AI Same hit rate / AI Sep hit rate (Accuracy of tendency)
                    # Note: l_stats['ai_same_line_hit'] means AI voted Same AND Result was Same.
                    
                    acc_same = l_stats['ai_same_line_hit'] / ai_same * 100 if ai_same > 0 else 0.0
                    acc_sep = l_stats['ai_separate_hit'] / ai_sep * 100 if ai_sep > 0 else 0.0
                    
                    st.markdown(f"**ğŸ’¡ AIã®ç‹™ã„æ–¹ã®ç²¾åº¦**")
                    st.write(f"- ãƒ©ã‚¤ãƒ³(ã‚¹ã‚¸)ã‚’ç‹™ã£ãŸæ™‚ã®çš„ä¸­(å‚¾å‘ä¸€è‡´)ç‡: **{acc_same:.1f}%** (äºˆæƒ³æ•° {ai_same}Rä¸­ {l_stats['ai_same_line_hit']}Ræ­£è§£)")
                    st.write(f"- åˆ¥ç·š(ã‚¹ã‚¸é•)ã‚’ç‹™ã£ãŸæ™‚ã®çš„ä¸­(å‚¾å‘ä¸€è‡´)ç‡: **{acc_sep:.1f}%** (äºˆæƒ³æ•° {ai_sep}Rä¸­ {l_stats['ai_separate_hit']}Ræ­£è§£)")
                    
                else:
                    st.info("ãƒ©ã‚¤ãƒ³æƒ…å ±ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚åˆ†æã§ãã¾ã›ã‚“ã€‚")
            else:
                st.info("è¡¨ç¤ºå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

                # End of Ticket Stats Block

            st.divider()

            # 5. AI Score Analysis
            st.markdown(f"### ğŸ¤– AIè©•ä¾¡ç‚¹åˆ†æ ({sel_place})")
            st.caption("AIè©•ä¾¡ç‚¹1ä½ãƒ»2ä½ã®é¸æ‰‹ã®æˆç¸¾ã€ãŠã‚ˆã³è©•ä¾¡ç‚¹å·®ï¼ˆè‡ªä¿¡åº¦ï¼‰ã¨å‹ç‡ã®é–¢ä¿‚ãªã©ã‚’åˆ†æã—ã¾ã™ã€‚")

            if target_history:
                with st.spinner("AIã‚¹ã‚³ã‚¢å‚¾å‘ã‚’åˆ†æä¸­..."):
                    s_stats = logic_v2.analyze_ai_score_performance(target_history)
                
                if s_stats and s_stats.get('total_races', 0) > 0:
                    stot = s_stats['total_races']
                    
                    # 1. Basic Stats Comparison (1st vs 2nd)
                    st.markdown("**ğŸ¥‡ AIè©•ä¾¡ç‚¹ 1ä½ vs 2ä½ æˆç¸¾æ¯”è¼ƒ**")
                    sc1, sc2, sc3 = st.columns(3)
                    
                    # 1st Pick Stats
                    w1 = s_stats['ai_top_win']
                    r1 = s_stats['ai_top_rentai']
                    f1 = s_stats['ai_top_fukusho']
                    
                    # 2nd Pick Stats
                    w2 = s_stats.get('ai_2nd_win', 0)
                    r2 = s_stats.get('ai_2nd_rentai', 0)
                    f2 = s_stats.get('ai_2nd_fukusho', 0)

                    sc1.metric("1ç€å›æ•° (å‹ç‡)", 
                               f"1ä½: {w1} ({w1/stot*100:.1f}%)", 
                               f"2ä½: {w2} ({w2/stot*100:.1f}%)", delta_color="off")
                    sc2.metric("2é€£å¯¾å›æ•° (é€£å¯¾ç‡)", 
                               f"1ä½: {r1} ({r1/stot*100:.1f}%)", 
                               f"2ä½: {r2} ({r2/stot*100:.1f}%)", delta_color="off")
                    sc3.metric("3é€£å¯¾å›æ•° (è¤‡å‹ç‡)", 
                               f"1ä½: {f1} ({f1/stot*100:.1f}%)", 
                               f"2ä½: {f2} ({f2/stot*100:.1f}%)", delta_color="off")
                    
                    st.write("---")
                    
                    # 2. Relation with Competition Score Rank
                    st.markdown("**ğŸ“Š AI1ä½ã®é¸æ‰‹ã¯ã€Œç«¶èµ°å¾—ç‚¹ã€ã§ä½•ä½ã‹ï¼Ÿ**")
                    c_dist = s_stats['comp_rank_dist']
                    c_keys = sorted(c_dist.keys())
                    c_data = {"ç«¶èµ°å¾—ç‚¹é †ä½": [f"{k}ä½" for k in c_keys], "å›æ•°": [c_dist[k] for k in c_keys]}
                    st.bar_chart(pd.DataFrame(c_data).set_index("ç«¶èµ°å¾—ç‚¹é †ä½"))
                    
                    st.write("---")
                    
                    # 3. Score Gap Analysis (Detailed)
                    st.markdown("**ğŸ“ 1ä½ã¨2ä½ã®è©•ä¾¡ç‚¹å·®ã«ã‚ˆã‚‹å‹ç‡ãƒ»é€£å¯¾ç‡ã®å¤‰åŒ–**")
                    st.caption("è©•ä¾¡ç‚¹å·®ãŒå¤§ãã„ã»ã©AIãŒã€Œ1ä½ã¨2ä½ã®å®ŸåŠ›å·®ãŒã‚ã‚‹ã€ã¨åˆ¤æ–­ã—ã¦ã„ã¾ã™ã€‚")
                    
                    gap_data = s_stats['gap_data']
                    if gap_data:
                        df_gap = pd.DataFrame(gap_data)
                        # Binning
                        bins = [-100, 0, 2.0, 5.0, 8.0, 1000]
                        labels = ["é€†è»¢(2ä½>1ä½)", "åƒ…å·®(0-2ç‚¹)", "å°å·®(2-5ç‚¹)", "ä¸­å·®(5-8ç‚¹)", "å¤§å·®(8ç‚¹ä»¥ä¸Š)"]
                        
                        df_gap['bin'] = pd.cut(df_gap['gap'], bins=bins, labels=labels)
                        
                        # Aggregation
                        gap_grp = df_gap.groupby('bin', observed=False).agg({
                            'is_win': ['count', 'sum'],
                            'is_rentai': 'sum',
                            'is_fukusho': 'sum'
                        })
                        gap_grp.columns = ['ãƒ¬ãƒ¼ã‚¹æ•°', '1ç€å›æ•°', '2é€£å¯¾å›æ•°', '3é€£å¯¾å›æ•°']
                        
                        # Rate Calc
                        gap_grp['å‹ç‡'] = (gap_grp['1ç€å›æ•°'] / gap_grp['ãƒ¬ãƒ¼ã‚¹æ•°'] * 100).fillna(0)
                        gap_grp['é€£å¯¾ç‡'] = (gap_grp['2é€£å¯¾å›æ•°'] / gap_grp['ãƒ¬ãƒ¼ã‚¹æ•°'] * 100).fillna(0)
                        gap_grp['3é€£å¯¾ç‡'] = (gap_grp['3é€£å¯¾å›æ•°'] / gap_grp['ãƒ¬ãƒ¼ã‚¹æ•°'] * 100).fillna(0)
                        
                        # Display Table with Clean Columns
                        show_cols = ['ãƒ¬ãƒ¼ã‚¹æ•°', '1ç€å›æ•°', 'å‹ç‡', '2é€£å¯¾å›æ•°', 'é€£å¯¾ç‡', '3é€£å¯¾å›æ•°', '3é€£å¯¾ç‡']
                        st.dataframe(
                            gap_grp[show_cols].style.format({
                                'å‹ç‡': '{:.1f}%',
                                'é€£å¯¾ç‡': '{:.1f}%',
                                '3é€£å¯¾ç‡': '{:.1f}%'
                            }),
                            use_container_width=True
                        )
                        st.info("ğŸ’¡ **è¦‹æ–¹**: ã€Œä¸­å·®(5-8ç‚¹)ã€ã‚„ã€Œå¤§å·®(8ç‚¹ä»¥ä¸Š)ã€ã®æ™‚ã«å‹ç‡ãŒé«˜ã‘ã‚Œã°ã€AIã®è‡ªä¿¡åº¦ãŒä¿¡é ¼ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚")
                    
                    st.write("---")
                    
                    # 4. Bonus Player Analysis
                    st.markdown("**ğŸ AIåŠ ç‚¹(ãƒœãƒ¼ãƒŠã‚¹)æœ€å¤§é¸æ‰‹ã®æˆç¸¾**")
                    st.caption("ã€Œåœ°å…ƒã€ã€Œé€ƒã’No.1ã€ã€Œãƒãƒ³ã‚¯ç›¸æ€§ã€ãªã©ã®åŠ ç‚¹ãŒæœ€ã‚‚å¤§ãã„é¸æ‰‹ã®æˆç¸¾ã§ã™ã€‚")
                    
                    bonus_data = s_stats.get('bonus_data', [])
                    if bonus_data:
                        df_bonus = pd.DataFrame(bonus_data)
                        btot = len(df_bonus)
                        
                        # Overall Stats
                        b_win = df_bonus['is_win'].sum()
                        b_rentai = df_bonus['is_rentai'].sum()
                        b_fukusho = df_bonus['is_fukusho'].sum()
                        
                        bc1, bc2, bc3 = st.columns(3)
                        bc1.metric("å‹ç‡", f"{b_win/btot*100:.1f}%", f"{b_win}/{btot}R")
                        bc2.metric("é€£å¯¾ç‡", f"{b_rentai/btot*100:.1f}%", f"{b_rentai}/{btot}R")
                        bc3.metric("3é€£å¯¾ç‡", f"{b_fukusho/btot*100:.1f}%", f"{b_fukusho}/{btot}R")
                        
                        # By Comp Rank
                        st.markdown("**ç«¶èµ°å¾—ç‚¹é †ä½åˆ¥ï¼ˆåŠ ç‚¹1ä½é¸æ‰‹ï¼‰**")
                        rank_grp = df_bonus.groupby('comp_rank').agg({
                            'is_win': ['count', 'mean'],
                            'is_rentai': 'mean',
                            'is_fukusho': 'mean'
                        })
                        rank_grp.columns = ['å›æ•°', 'å‹ç‡', 'é€£å¯¾ç‡', '3é€£å¯¾ç‡']
                        rank_grp['å‹ç‡'] *= 100
                        rank_grp['é€£å¯¾ç‡'] *= 100
                        rank_grp['3é€£å¯¾ç‡'] *= 100
                        rank_grp.index = [f"{i}ä½" for i in rank_grp.index]
                        st.dataframe(
                            rank_grp.style.format({'å‹ç‡': '{:.1f}%', 'é€£å¯¾ç‡': '{:.1f}%', '3é€£å¯¾ç‡': '{:.1f}%'}),
                            use_container_width=True
                        )
                        
                        # By Bonus Amount (Breakpoints)
                        st.markdown("**åŠ ç‚¹é‡ã«ã‚ˆã‚‹æ–­å±¤**")
                        bins_b = [0, 5.0, 7.0, 9.0, 100]
                        labels_b = ["ã€œ5ç‚¹", "5ã€œ7ç‚¹", "7ã€œ9ç‚¹", "9ç‚¹ä»¥ä¸Š"]
                        df_bonus['bonus_bin'] = pd.cut(df_bonus['bonus'], bins=bins_b, labels=labels_b)
                        
                        bonus_grp = df_bonus.groupby('bonus_bin', observed=False).agg({
                            'is_win': ['count', 'mean'],
                            'is_rentai': 'mean',
                            'is_fukusho': 'mean'
                        })
                        bonus_grp.columns = ['å›æ•°', 'å‹ç‡', 'é€£å¯¾ç‡', '3é€£å¯¾ç‡']
                        bonus_grp['å‹ç‡'] *= 100
                        bonus_grp['é€£å¯¾ç‡'] *= 100
                        bonus_grp['3é€£å¯¾ç‡'] *= 100
                        st.dataframe(
                            bonus_grp.style.format({'å‹ç‡': '{:.1f}%', 'é€£å¯¾ç‡': '{:.1f}%', '3é€£å¯¾ç‡': '{:.1f}%'}),
                            use_container_width=True
                        )
                        st.info("ğŸ’¡ **æ–­å±¤**: ã€Œ7ç‚¹ä»¥ä¸Šã€ã§å‹ç‡ãƒ»é€£å¯¾ç‡ãŒè·³ã­ä¸ŠãŒã‚‹å‚¾å‘ãŒã‚ã‚Œã°ã€AIåŠ ç‚¹ã‚’ä¿¡é ¼ã§ãã‚‹ã‚µã‚¤ãƒ³ã§ã™ã€‚")
                else:
                    st.info("è©³ç´°ãªç«¶èµ°å¾—ç‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚åˆ†æã§ãã¾ã›ã‚“ã€‚")
            else:
                st.info("ãƒ‡ãƒ¼ã‚¿ãªã—")
                
            st.divider()
            
            # 4. History Table
            st.markdown(f"### ğŸ“œ è©³ç´°å±¥æ­´ ({sel_place})")
            
            # Reset df_disp to ignore strategy filters (Gap/Bonus) for the main history table
            # But keep the Place filter
            df_disp = df_res.copy()
            if sel_place != "å…¨å ´":
                df_disp = df_disp[df_disp['place'] == sel_place]
                
            # --- Badge Calculation Logic ---
            def get_ai_badges(row):
                badges = []
                
                # Parse AI Indices
                try:
                    ai_indices = row.get('ai_indices', [])
                    if not ai_indices: return ""
                    
                    # Ensure properly parsed score
                    parsed = []
                    for item in ai_indices:
                        try: s = float(item.get('final_score', 0))
                        except: s = 0.0
                        c = int(item.get('è»Šç•ª', 0))
                        parsed.append({'c': c, 's': s})
                    parsed.sort(key=lambda x: x['s'], reverse=True)
                    
                    if not parsed: return ""
                    
                    # Gap Badge
                    ai_top = parsed[0]
                    ai_2nd = parsed[1] if len(parsed)>1 else None
                    if ai_2nd:
                        gap = ai_top['s'] - ai_2nd['s']
                        if gap >= 8.0:
                            badges.append(f"ğŸ”¥å¤§å·®({gap:.1f})")
                        elif gap >= 5.0:
                            badges.append(f"âœ¨ä¸­å·®({gap:.1f})")
                    
                    # Upset Badge (Need Comp Rank info from somewhere)
                    # We might not have comp rank in `df_disp` easily unless we join or kept it.
                    # As a proxy, let's assume if gap is NEGATIVE or very small it's risky? No.
                    # The user specifically asked for "Competitor Score Rank 1 vs AI Top".
                    # `df_res` (which `df_disp` comes from) comes from `analyze_prediction_history`.
                    # logic_v2.analyze_prediction_history loads DB info including `comp_ranks`.
                    # But does it save it to the DataFrame? 
                    # Let's check logic_v2.py or just use what we have.
                    # If we can't get Comp Rank easily here without DB hit, we might skip Upset Badge for now
                    # OR we can try to infer from 'strategy_title' if it says "ç©´"?
                    
                    # However, to be accurate for "AI top is not Comp top", we need Comp Top Car.
                    # For now, let's just show Gap Confidence which is high value.
                    
                except:
                    pass
                return " ".join(badges)
            
            # If `ai_indices` is not in df_disp columns, we can't do it.
            # `analyze_prediction_history` returns `df_res` constructed from history dicts.
            # It usually flattens specific cols. We might need to ensure `ai_indices` is kept.
            # Actually, `df_disp` IS `df_res`. logic_v2 constructs it.
            # If `ai_indices` isn't in columns, we must rely on what we have.
            # Let's assume it's NOT there by default.
            
            # Re-map from `history` list based on index or ID?
            # `df_disp` rows correspond to `history` items processed.
            # `history` is available here (`history` variable).
            # Let's map badges via finding matching history item.
            
            id_map = {}
            
            # Pre-load DB connection for bonus calculation
            import sqlite3
            conn_badge = sqlite3.connect(db_utils.DB_PATH)
            
            for h in history:
                rid = h.get('race_id') # or construct
                if not rid:
                    r_num = str(h.get('race_num','')).replace('R','') + 'R'
                    rid = f"{h.get('place')}_{h.get('date')}_{r_num}"
                
                # Calc Badges
                b = []
                ai_indices = h.get('ai_indices', [])
                if ai_indices:
                    parsed = []
                    for item in ai_indices:
                        try: s = float(item.get('final_score', 0))
                        except: s = 0.0
                        c = int(item.get('è»Šç•ª', 0))
                        parsed.append({'c': c, 's': s})
                    parsed.sort(key=lambda x: x['s'], reverse=True)
                    
                    if len(parsed) >= 2:
                        gap = parsed[0]['s'] - parsed[1]['s']
                        if gap >= 8.0: b.append(f"ğŸ”¥å¤§å·®{gap:.1f}")
                        elif gap >= 5.0: b.append(f"âœ¨ä¸­å·®{gap:.1f}")
                
                # Bonus Badge - Calculate max bonus for this race
                try:
                    query_race = "SELECT * FROM race_result WHERE race_id = ?"
                    df_race = pd.read_sql(query_race, conn_badge, params=[rid])
                    if not df_race.empty:
                        df_scored = logic_v2.calculate_ai_score(df_race)
                        if 'base_score' in df_scored.columns and 'ai_score' in df_scored.columns:
                            df_scored['bonus'] = df_scored['ai_score'] - df_scored['base_score']
                            max_bonus = df_scored['bonus'].max()
                            # Check for NaN
                            if pd.notna(max_bonus):
                                if max_bonus >= 9.0:
                                    b.append(f"ğŸåŠ ç‚¹{int(max_bonus)}")
                                elif max_bonus >= 7.0:
                                    b.append(f"â­åŠ ç‚¹{int(max_bonus)}")
                except:
                    pass
                
                id_map[rid] = " ".join(b)
            
            conn_badge.close()
                
            # Ensure race_id exists
            if 'race_id' not in df_disp.columns:
                # Reconstruct
                # Assuming date format is consistent or available
                # Logic_v2 usually preserves 'race_id' if in input. 
                # If missing, we construct: place + date + race_num
                # Warning: date format might differ (YYYY-MM-DD vs YYYYå¹´...)
                # But 'id_map' keys were constructed using h.get('date').
                # df_disp['date'] comes from h.get('date').
                # So they should match.
                df_disp['race_id'] = df_disp.apply(
                    lambda x: f"{x.get('place')}_{x.get('date')}_{str(x.get('race_num','')).replace('R','')+'R'}", 
                    axis=1
                )

            df_disp['ai_memo'] = df_disp['race_id'].map(id_map).fillna("")

            df_disp['race_str'] = df_disp['place'] + " " + df_disp['race_num'].astype(str)
            
            def fmt_tickets(t):
                if isinstance(t, list): 
                    return "\n".join(t)
                return str(t)
            
            df_disp['tickets_str'] = df_disp['tickets'].apply(fmt_tickets)
            
            disp_cols = [
                'timestamp', # HIDDEN
                'date', 'race_str', 'strategy_title', 'ai_memo', 'tickets_str', 
                'result_top3', 'hit_detail', 'benefit', 'balance'
            ]
            
            final_cols = [c for c in disp_cols if c in df_disp.columns]
            
            column_config = {
                'date': "æ—¥ä»˜",
                'race_str': "ãƒ¬ãƒ¼ã‚¹",
                'strategy_title': "æˆ¦ç•¥å",
                'ai_memo': st.column_config.TextColumn("AIè‡ªä¿¡åº¦", width="small", help="ç‚¹æ•°å·®5ç‚¹ä»¥ä¸Šã§è¡¨ç¤º"),
                'tickets_str': st.column_config.TextColumn("æ¨å¥¨è²·ã„ç›®", width="large"),
                'result_top3': st.column_config.TextColumn("çµæœ (1-2-3)", width="small"),
                'hit_detail': st.column_config.TextColumn("çš„ä¸­åˆ¤å®š", width="medium"),
                'benefit': st.column_config.NumberColumn("æ‰•æˆ»é‡‘", format="%då††"),
                'balance': st.column_config.NumberColumn("åæ”¯", format="%då††"),
            }
            
            try:
                if 'timestamp' in df_disp.columns:
                    disp_df_final = df_disp[final_cols].sort_values('timestamp', ascending=False)
                    # Optional: drop timestamp from view if desired, but keeping it is useful for exact time
                    # disp_df_final = disp_df_final.drop(columns=['timestamp']) 
                else:
                    disp_df_final = df_disp[final_cols]
                
                st.dataframe(
                    disp_df_final,
                    column_config=column_config,
                    use_container_width=True,
                    height=600,
                    hide_index=True
                )
            except Exception as e:
                st.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
                st.dataframe(df_disp) # Fallback raw

    # ==========================================
    # AI Chat Assistant (Tab1 Bottom)
    # ==========================================
    # Only show if in Analysis Mode (Tab1)
    # Actually, tab1 scope ended way up. We need to check indentation or placement.
    # The previous code was Tab 3 (History). 
    # Chat should be available for the ACTIVE race analysis.
    # So it should be part of the race analysis flow, likely after the "Today's Prediction Column".
    # Or as a global floating element? No, Streamlit doesn't float easily.
    # We will place it at the very bottom of the main area (outside tabs probably, or specifically in Tab1).
    
# Moving back to indent level 0 to ensure it's outside Tab 3 loop
# But we need access to 'df_scored', 'strategy_data' etc. which are local to Tab 1.
# So we must insert this INSIDE Tab 1, after the reporter section.
# The previous view was lines 1400+, which is inside Tab 3.
# I need to target the end of Tab 1.
# Tab 1 ends around line 1250-1300? 
# Let's abort this replace and view Tab 1 end first.
