import pandas as pd
import sqlite3
import os
import re
import io
import json
import numpy as np
import db_utils
from datetime import datetime
import google.generativeai as genai
from bs4 import BeautifulSoup

# ==========================================
# 1. Parsing Logic (parse_kdreams_simple)
# ==========================================

def extract_metadata_from_html(soup):
    """HTMLã‹ã‚‰æ—¥ä»˜ã¨å ´æ‰€ã¨ãƒ¬ãƒ¼ã‚¹ç•ªå·ã‚’æ¢ã™"""
    meta = {}
    text = soup.get_text()[:2000] # Extend search range
    
    # æ—¥ä»˜ (2025å¹´12æœˆ13æ—¥ or 2025/12/13) - Look in specific headers first
    
    # Try Finding in Title or H1 first (More reliable)
    title_text = ""
    if soup.title: title_text += soup.title.get_text()
    h1s = soup.find_all('h1')
    for h in h1s: title_text += " " + h.get_text()
    
    # Date in Title
    m = re.search(r'(\d{4}[å¹´/-]\d{1,2}[æœˆ/-]\d{1,2}æ—¥?)', title_text)
    if m: 
        meta['date'] = m.group(1)
    else:
        # Fallback to general text
        m = re.search(r'(\d{4}[å¹´/-]\d{1,2}[æœˆ/-]\d{1,2}æ—¥?)', text)
        if m: meta['date'] = m.group(1)
    
    # ãƒ¬ãƒ¼ã‚¹ç•ªå·
    m_r = re.search(r'(\d{1,2})[Rãƒ¬ãƒ¼ã‚¹]', title_text) # Try title first
    if m_r: 
        meta['race_num'] = m_r.group(1)
    else:
        m_r = re.search(r'(\d{1,2})[Rãƒ¬ãƒ¼ã‚¹]', text)
        if m_r: meta['race_num'] = m_r.group(1)
    
    # ç«¶è¼ªå ´ - Strictly look for "Place" + "ç«¶è¼ª" or "Place" + "ãƒ¬ãƒ¼ã‚¹" in Title
    places = ["å‡½é¤¨","é’æ£®","ã„ã‚ãå¹³","å¼¥å½¦","å‰æ©‹","å–æ‰‹","å®‡éƒ½å®®","å¤§å®®","è¥¿æ­¦åœ’","äº¬ç‹é–£","ç«‹å·","æ¾æˆ¸","åƒè‘‰","å·å´","å¹³å¡š","å°ç”°åŸ","ä¼Šæ±","é™å²¡","åå¤å±‹","å²é˜œ","å¤§å£","è±Šæ©‹","å¯Œå±±","æ¾é˜ª","å››æ—¥å¸‚","ç¦äº•","å¥ˆè‰¯","å‘æ—¥ç”º","å’Œæ­Œå±±","å²¸å’Œç”°","ç‰é‡","åºƒå³¶","é˜²åºœ","é«˜æ¾","å°æ¾å³¶","é«˜çŸ¥","æ¾å±±","å°å€‰","ä¹…ç•™ç±³","æ­¦é›„","ä½ä¸–ä¿","åˆ¥åºœ","ç†Šæœ¬"]
    
    # 1. Strong Check: "Place" + "ç«¶è¼ª" in Title
    found_place = None
    for p in places:
        if f"{p}ç«¶è¼ª" in title_text or f"{p} " in title_text:
            found_place = p
            break
            
    # 2. Fallback: specific ID parsing or just found in text (Risky)
    if not found_place:
        for p in places:
             # Avoid "Next Race: Wakayama" type false positives by checking nearby characters if possible
             # For now, just check text but prioritising beginning
             if p in text[:500]: # Check only header area
                 found_place = p
                 break
                 
    # Start Time & Deadline
    # Search patterns: "æŠ•ç¥¨ç· åˆ‡ 10:45" "ç™ºèµ° 10:50"
    m_deadline = re.search(r'ç· åˆ‡.*?(\d{1,2}:\d{2})', text)
    if m_deadline: meta['deadline'] = m_deadline.group(1)
    
    m_start = re.search(r'ç™ºèµ°.*?(\d{1,2}:\d{2})', text)
    if m_start: meta['start_time'] = m_start.group(1)

    if found_place:
        meta['place'] = found_place
    
    return meta

def parse_line_position_html(soup):
    """
    Parse the specific K-Dreams line alignment div.
    <div class="line_position">
        <span class="icon_p"><span class="p009">9</span>...</span>
        <span class="icon_p space"></span>
    """
    line_div = soup.find('div', class_='line_position')
    if not line_div: return None
    
    lines = []
    current_line = []
    
    # Iterate over spans
    # Found children: span.icon_p
    # Check class "space" for break
    
    spans = line_div.find_all('span', class_='icon_p', recursive=False) # Only direct children?
    # Actually the structure provided implies they are siblings.
    # But beautifulsoup finding might need care.
    if not spans:
        spans = line_div.find_all('span', class_='icon_p')
        
    for sp in spans:
        classes = sp.get('class', [])
        
        if 'space' in classes:
            if current_line:
                lines.append(current_line)
                current_line = []
            continue
            
        # Extract Car Num
        # Inside span.icon_p, there is span.p00X
        # e.g. <span class="p009">9</span>
        # But also check for simple text if nested span missing?
        
        # Regex for car num class p001-p009
        car_span = sp.find('span', class_=re.compile(r'p00\d'))
        if car_span:
            try:
                car_num = int(car_span.get_text().strip())
                current_line.append(car_num)
            except: pass
        else:
             # Try text directly?
             txt = sp.get_text().strip()
             # If just "â†", ignore
             if txt in ["â†", ""]: continue
             # If numeric?
             # Usually "9å…ˆè¡Œ" -> "9" is separate? 
             # Based on user snippet: <span class="p009">9</span><span class="p201">å…ˆè¡Œ</span>
             pass

    if current_line:
        lines.append(current_line)
        
    return lines

def lines_to_str(lines):
    if not lines: return ""
    # "123 456 789"
    return " ".join(["".join(map(str, l)) for l in lines])

def parse_kdreams_direct(html_content):
    """
    ã€Kãƒ‰ãƒªãƒ¼ãƒ ã‚¹ ç›´æ¥ã‚»ãƒ«è§£æç‰ˆã€‘
    HTMLã®<tr>æ§‹é€ ã‚’ç›´æ¥è§£æã—ã€ç¢ºå®Ÿã«ã‚»ãƒ«é †åºã‚’å–å¾—ã™ã‚‹ã€‚
    ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼ã®ãšã‚Œå•é¡Œã‚’å›é¿ã€‚
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    meta = extract_metadata_from_html(soup)
    meta['site'] = 'K-Dreams'
    
    # Parse Line Position
    line_groups = parse_line_position_html(soup)
    if line_groups:
        meta['lines_parsed'] = lines_to_str(line_groups)
        meta['lines_list'] = line_groups
    
    # Find all player rows: <tr class="n1">, <tr class="n2">, etc.
    # IMPORTANT: Only use the FIRST table containing these rows to avoid duplicates
    rows = []
    seen_car_nums = set()  # Track seen car numbers to avoid duplicates
    
    # Find the main entry table (usually table.entry or first table with n1 class rows)
    target_table = None
    for table in soup.find_all('table'):
        if table.find('tr', class_=re.compile(r'^n\d')):
            target_table = table
            break  # Use FIRST table found
    
    if target_table:
        all_trs = target_table.find_all('tr', class_=re.compile(r'^n\d'))
    else:
        all_trs = []
    
    for tr in all_trs:
        tds = tr.find_all('td')
        if len(tds) < 15: continue  # Not a valid player row
        
        row_data = {}
        
        # Extract text from each cell, handling nested spans
        def get_cell_text(td):
            # Get direct text or span text
            span = td.find('span', class_=lambda x: x and 'best' not in (x if isinstance(x, list) else [x]))
            if span:
                return span.get_text(strip=True)
            best_span = td.find('span', class_='best')
            if best_span:
                return best_span.get_text(strip=True)
            return td.get_text(strip=True)
        
        try:
            # Parse based on class names and position
            idx = 0
            for td in tds:
                classes = td.get('class', [])
                class_str = ' '.join(classes) if classes else ''
                
                if 'tip' in class_str:
                    # äºˆæƒ³å°
                    icon_span = td.find('span', class_=re.compile(r'icon_t\d'))
                    if icon_span:
                        row_data['äºˆæƒ³'] = icon_span.get_text(strip=True)
                elif 'kiai' in class_str:
                    row_data['å¥½æ°—åˆ'] = get_cell_text(td)
                elif 'evaluation' in class_str:
                    row_data['è©•ä¾¡'] = get_cell_text(td)
                elif 'bracket' in class_str:
                    row_data['æ ç•ª'] = get_cell_text(td)
                elif 'num' in class_str:
                    row_data['è»Šç•ª'] = get_cell_text(td)
                elif 'rider' in class_str:
                    # é¸æ‰‹å + åºœçœŒ/å¹´é½¢/æœŸåˆ¥
                    full_text = td.get_text(' ', strip=True)
                    # Split by home span
                    home_span = td.find('span', class_='home')
                    if home_span:
                        home_text = home_span.get_text(strip=True)
                        # Name is before home span
                        name_part = full_text.replace(home_text, '').strip()
                        row_data['é¸æ‰‹å'] = name_part
                        
                        # Parse home: åºœçœŒ/å¹´é½¢/æœŸåˆ¥
                        parts = home_text.replace('ã€€', ' ').split('/')
                        if len(parts) >= 1:
                            row_data['åºœçœŒ'] = parts[0].strip()
                        if len(parts) >= 2:
                            row_data['å¹´é½¢'] = parts[1].strip()
                        if len(parts) >= 3:
                            row_data['æœŸåˆ¥'] = parts[2].strip()
                    else:
                        row_data['é¸æ‰‹å'] = full_text
                else:
                    idx += 1
            
            # Now parse remaining columns by position after rider
            # Find rider index
            rider_idx = -1
            for i, td in enumerate(tds):
                if 'rider' in ' '.join(td.get('class', [])):
                    rider_idx = i
                    break
            
            if rider_idx >= 0 and len(tds) > rider_idx + 10:
                # Columns after rider: ç´šç­, è„šè³ª, ã‚®ãƒ¤, å¾—ç‚¹, S, B, é€ƒ, æ², å·®, ãƒ, ...
                offset = rider_idx + 1
                
                row_data['ç´šç­'] = get_cell_text(tds[offset]) if offset < len(tds) else ''
                row_data['è„šè³ª'] = get_cell_text(tds[offset+1]) if offset+1 < len(tds) else ''
                row_data['ã‚®ãƒ¤å€æ•°'] = get_cell_text(tds[offset+2]) if offset+2 < len(tds) else ''
                row_data['ç«¶èµ°å¾—ç‚¹'] = get_cell_text(tds[offset+3]) if offset+3 < len(tds) else ''
                row_data['S'] = get_cell_text(tds[offset+4]) if offset+4 < len(tds) else '0'
                row_data['B'] = get_cell_text(tds[offset+5]) if offset+5 < len(tds) else '0'
                row_data['é€ƒ'] = get_cell_text(tds[offset+6]) if offset+6 < len(tds) else '0'
                row_data['æ²'] = get_cell_text(tds[offset+7]) if offset+7 < len(tds) else '0'
                row_data['å·®'] = get_cell_text(tds[offset+8]) if offset+8 < len(tds) else '0'
                row_data['ãƒ'] = get_cell_text(tds[offset+9]) if offset+9 < len(tds) else '0'
            
            if row_data.get('è»Šç•ª'):
                car_num = str(row_data['è»Šç•ª']).strip()
                if car_num not in seen_car_nums:
                    seen_car_nums.add(car_num)
                    rows.append(row_data)
                
        except Exception as e:
            continue
    
    if not rows:
        return pd.DataFrame(), meta
    
    df = pd.DataFrame(rows)
    
    # Convert numeric columns
    for col in ['è»Šç•ª', 'ç«¶èµ°å¾—ç‚¹', 'S', 'B', 'é€ƒ', 'æ²', 'å·®', 'ãƒ', 'å¹´é½¢', 'æœŸåˆ¥']:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
    
    # Apply line info if available
    if 'lines_list' in meta and 'è»Šç•ª' in df.columns:
        car_line_map = {}
        for idx, grp in enumerate(meta['lines_list']):
            line_s = "".join(map(str, grp))
            for car in grp:
                car_line_map[car] = line_s
        df['ãƒ©ã‚¤ãƒ³'] = df['è»Šç•ª'].astype(int).map(car_line_map).fillna('')
    
    return df, meta

def parse_kdreams_simple(html_content):
    """
    ã€æ¥½å¤©Kãƒ‰ãƒªãƒ¼ãƒ ã‚¹ ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ (æ”¹å–„v3)ã€‘
    åˆ—ã”ã¨ã®ç‰¹å¾´é‡ã ã‘ã§ã€Œè»Šç•ªã€ã€Œé¸æ‰‹åã€ã€Œç«¶èµ°å¾—ç‚¹ã€ã‚’ç‰¹å®šã™ã‚‹ã€‚
    äºˆæƒ³å°(â—â—‹ç­‰)ã®æ··å…¥ã‚’é˜²ãã€èª˜å°å“¡ã‚’é™¤å¤–ã™ã‚‹ã€‚
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    meta = extract_metadata_from_html(soup)
    meta['site'] = 'K-Dreams'
    
    # [New] Parse Line Position Div (Global Info)
    line_groups = parse_line_position_html(soup)
    if line_groups:
        meta['lines_parsed'] = lines_to_str(line_groups)
        meta['lines_list'] = line_groups
        # print(f"DEBUG: Parsed Lines: {meta['lines_parsed']}")

    # BeautifulSoupã§ç¢ºå®Ÿã«ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®šã™ã‚‹
    target_table = None
    best_df = pd.DataFrame()
    tables = soup.find_all('table')
    
    for table in tables:
        # text check is fast
        txt = table.get_text()
        if 'è»Šç•ª' in txt and 'é¸æ‰‹' in txt:
             # Convert to DF to check structure
             try:
                 # Use str(table) to parse only this table
                 _dfs = pd.read_html(io.StringIO(str(table)), header=None)
                 if not _dfs: continue
                 _df = _dfs[0]
                 
                 if _df.shape[1] < 10: continue 
                 
                 # Check for Header Keywords in the first few rows
                 found_key_in_row = False
                 for i in range(min(5, len(_df))):
                     row_str = _df.iloc[i].astype(str).str.cat()
                     if 'è»Šç•ª' in row_str and ('é¸æ‰‹' in row_str or 'å' in row_str):
                         # Found the header row!
                         target_table = _df.iloc[i+1:].copy()
                         # Clean header
                         header_row = _df.iloc[i].astype(str).str.replace(r'\s+','', regex=True)
                         target_table.columns = header_row
                         best_df = target_table
                         found_key_in_row = True
                         break
                 
                 if found_key_in_row:
                     break
                 else:
                     # Header row not found in DF
                     if best_df.empty or (_df.size > best_df.size):
                         best_df = _df.copy()
                         
                         # Flatten MultiIndex columns + User Cleaning
                         new_cols_cleaned = []
                         counts = {}
                         for col in best_df.columns:
                             if isinstance(col, tuple):
                                 parts = [str(c) for c in col if str(c) not in ['nan', 'None', '']]
                                 seen = set()
                                 unique_parts = [x for x in parts if not (x in seen or seen.add(x))]
                                 base_name = "_".join(unique_parts)
                             else:
                                 base_name = str(col)
                             
                             # User Cleaning Rules
                             base_name = base_name.replace("ç›´è¿‘4ãƒ¶æœˆã®æˆç¸¾_", "").replace("_", "")
                             base_name = base_name.replace("æ  ç•ªæ  ç•ª", "æ ç•ª").replace("è»Š ç•ªè»Š ç•ª", "è»Šç•ª")
                             base_name = base_name.replace("ç´š ç­ç´š ç­", "ç´šç­").replace("è„š è³ªè„š è³ª", "è„šè³ª")
                             base_name = base_name.replace(" ", "") # Aggressively remove spaces
                             
                             if base_name not in counts:
                                 counts[base_name] = 0
                                 new_cols_cleaned.append(base_name)
                             else:
                                 counts[base_name] += 1
                                 new_cols_cleaned.append(f"{base_name}#{counts[base_name]}")
                         
                         best_df.columns = new_cols_cleaned
                     
             except: continue

    if best_df.empty: 
        try:
             dfs = pd.read_html(io.StringIO(html_content), header=None)
             for df in dfs:
                if len(df) < 5: continue
                if df.shape[1] < 8: continue # Main table is wide
                
                # Try to find header
                for i in range(min(5, len(df))):
                    row_str = df.iloc[i].astype(str).str.cat()
                    if 'è»Šç•ª' in row_str and ('é¸æ‰‹' in row_str or 'å' in row_str):
                        best_df = df.iloc[i+1:].copy()
                        best_df.columns = df.iloc[i].astype(str).str.replace(r'\s+','', regex=True)
                        break
                if not best_df.empty: break
        except: pass
        
    if best_df.empty: return pd.DataFrame(), meta

    # --- åˆ—ã®å½¹å‰²åˆ¤å®š (Header Text & Content) ---
    best_df.columns = [str(c) for c in best_df.columns]
    
    # User specified columns order
    columns_order = [
        'ç«¶è¼ªå ´', 'ã‚°ãƒ¬ãƒ¼ãƒ‰', 'æ—¥ä»˜', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ã®ç¨®é¡', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·', 'ãƒ©ã‚¤ãƒ³', 
        'é¸æ‰‹å', 'åºœçœŒ', 'å¹´é½¢', 'æœŸåˆ¥', 'è„šè³ª', 'ç«¶èµ°å¾—ç‚¹', 
        'S', 'B', 'é€ƒ', 'æ²', 'å·®', 'ãƒ', 'BK',
        'æ±ºã¾ã‚Šæ‰‹', 'ç€é †', 'è»Šç•ª', 'Sï¼B',
        '2é€£è¤‡', '3é€£è¤‡', 'ãƒ¯ã‚¤ãƒ‰1', 'ãƒ¯ã‚¤ãƒ‰2', 'ãƒ¯ã‚¤ãƒ‰3', '2é€£å˜', '3é€£å˜'
    ]
    
    rename_map = {}
    used_cols = set()
    mapped_targets = set()

    def safe_map(col, target):
        if target not in mapped_targets:
            rename_map[col] = target
            used_cols.add(col)
            mapped_targets.add(target)
    
    for col in best_df.columns:
        c = str(col)
        # Exact matches first
        if c in columns_order:
            pass
        # Fuzzy Fallbacks
        elif 'ç«¶èµ°å¾—ç‚¹' in c: safe_map(c, 'ç«¶èµ°å¾—ç‚¹')
        elif 'é¸æ‰‹å' in c: safe_map(c, 'é¸æ‰‹å')
        elif 'è»Šç•ª' in c and 'è»Šç•ª' not in best_df.columns: safe_map(c, 'è»Šç•ª')
        elif 'S' == c or 'S#' in c: safe_map(c, 'S')
        elif 'B' == c or 'B#' in c: safe_map(c, 'B')
        elif 'é€ƒ' in c and len(c) < 5: safe_map(c, 'é€ƒ')
        elif 'æ²' in c and len(c) < 5: safe_map(c, 'æ²')
        elif 'å·®' in c and len(c) < 5: safe_map(c, 'å·®')
        elif 'å·®' in c and len(c) < 5: safe_map(c, 'å·®')
        elif 'ãƒ' in c and len(c) < 5: safe_map(c, 'ãƒ')
        elif 'ãƒ©ã‚¤ãƒ³' in c: safe_map(c, 'ãƒ©ã‚¤ãƒ³')
        elif 'ä¸¦ã³' in c: safe_map(c, 'ãƒ©ã‚¤ãƒ³')

    # Content-based Line Detection (if not found by header)
    if 'ãƒ©ã‚¤ãƒ³' not in rename_map.values():
        for col in best_df.columns:
            if col in used_cols: continue
            
            # Check content for line-like strings (e.g. "123", "1(2)3", "1")
            # Must consist of digits and maybe parens
            s_vals = best_df[col].astype(str).str.strip()
            
            # Filter out empty
            s_valid = s_vals[s_vals != 'nan']
            if s_valid.empty: continue
            
            # Avg length should be small (1-5 chars usually)
            avg_len = s_valid.str.len().mean()
            if not (0.8 <= avg_len <= 8.0): continue
            
            # Should have digits
            has_digits = s_valid.str.contains(r'\d').mean()
            if has_digits < 0.8: continue
            
            # Should NOT be loose decimals
            is_float = s_valid.str.match(r'^\d+\.\d+$').mean()
            if is_float > 0.1: continue
            
            # If it contains typical line chars like parens
            has_parens = s_valid.str.contains(r'[()]').any()
            
            # Or if it matches simple digit sequences 123
            is_digit_seq = s_valid.str.match(r'^[\d()]+$').mean()
            
            if is_digit_seq > 0.8 or has_parens:
                rename_map[col] = 'ãƒ©ã‚¤ãƒ³'
                used_cols.add(col)
                break
    
    best_df.rename(columns=rename_map, inplace=True)
    
    # Reorder (keep others at end)
    final_cols = [c for c in columns_order if c in best_df.columns]
    other_cols = [c for c in best_df.columns if c not in final_cols]  
    best_df = best_df[final_cols + other_cols]
    
    # 1. ç«¶èµ°å¾—ç‚¹ (Content Based - if not found by header)
    score_col = None
    if 'ç«¶èµ°å¾—ç‚¹' not in rename_map.values():
        for col in best_df.columns:
            if col in used_cols: continue
            vals = pd.to_numeric(best_df[col], errors='coerce')
            if 60 <= vals.mean() <= 130:
                rename_map[col] = 'ç«¶èµ°å¾—ç‚¹'
                score_col = col
                used_cols.add(col)
                break
            
    # 2. è»Šç•ª (Content Based)
    if 'è»Šç•ª' not in rename_map.values():
        for col in best_df.columns:
            if col in used_cols: continue
            vals = pd.to_numeric(best_df[col], errors='coerce')
            vals_valid = vals.dropna()
            if vals_valid.min() >= 1 and vals_valid.max() <= 9:
                if vals_valid.nunique() >= 5:
                    if not vals_valid.duplicated().any():
                        rename_map[col] = 'è»Šç•ª'
                        used_cols.add(col)
                        break
                    used_cols.add(col)
                    break
            
    # 3. é¸æ‰‹å (If strict header match failed)
    if 'é¸æ‰‹å' not in rename_map.values():
        name_candidates = []
        for col in best_df.columns:
            if col in used_cols: continue
            s_vals = best_df[col].astype(str)
            # Must have Kanji
            has_kanji = s_vals.str.contains(r'[ä¸€-é¾¥]').any()
            
            if has_kanji and not s_vals.str.isnumeric().all():
                if s_vals.str.contains('ã‚³ãƒ¡ãƒ³ãƒˆ|é€£å¯¾').any(): continue
                
                # Check symbol ratio
                sample_txt = "".join(s_vals.tolist()[:5])
                symbol_chars = ["â—", "â—‹", "â–²", "â–³", "Ã—", "æ³¨"] 
                symbol_count = sum(sample_txt.count(s) for s in symbol_chars)
                if len(sample_txt) > 0 and (symbol_count / len(sample_txt)) > 0.3:
                    continue
                
                # Average length 
                avg_len = s_vals.str.len().mean()
                if avg_len < 1.8: continue
                
                name_candidates.append((col, avg_len))
        
        # Pick the candidate with max average length
        if name_candidates:
            name_candidates.sort(key=lambda x: x[1], reverse=True)
            best_col = name_candidates[0][0]
            rename_map[best_col] = 'é¸æ‰‹å'
            used_cols.add(best_col)
                
    # 4. æˆ¦æ³•ãƒ‡ãƒ¼ã‚¿
    # NOTE: CSVãƒ˜ãƒƒãƒ€ãƒ¼ãŒæ—¢ã«ã€Œé€ƒã€ã€Œæ²ã€ã€Œå·®ã€ã€Œãƒã€ã¨æ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€
    # ä½ç½®ãƒ™ãƒ¼ã‚¹ã®å†å‰²ã‚Šå½“ã¦ã¯ä¸è¦ã€‚ã‚€ã—ã‚é–“é•ã£ãŸåˆ—ã‚’å‰²ã‚Šå½“ã¦ã¦ã—ã¾ã†ã€‚
    # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒæ—¢ã«æ­£ã—ã„ã‹ãƒã‚§ãƒƒã‚¯ã—ã€ãªã‘ã‚Œã°å‰²ã‚Šå½“ã¦ã‚‹ã€‚
    
    tactic_cols = ['é€ƒ', 'æ²', 'å·®', 'ãƒ', 'S', 'B', 'BK']
    missing_tactics = [t for t in tactic_cols if t not in best_df.columns]
    
    # Only do position-based assignment if headers are MISSING
    if missing_tactics and score_col:
        cols = list(best_df.columns)
        s_idx = cols.index(score_col) if score_col in cols else -1
        
        if s_idx >= 0:
            param_names = ['S', 'B', 'é€ƒ', 'æ²', 'å·®', 'ãƒ', 'BK']
            p_ptr = 0
            for i in range(s_idx + 1, len(cols)):
                if p_ptr >= len(param_names): break
                c = cols[i]
                if c in used_cols: continue
                if param_names[p_ptr] in best_df.columns: 
                    p_ptr += 1
                    continue  # Already has this column
                
                col_name_lower = str(c).lower()
                if 'ç´šç­' in c or 'ç­' in c: continue
                if 'ã‚®ãƒ¤' in c or 'ã‚®ã‚¢' in c or 'gear' in col_name_lower: continue
                if 'äºˆæƒ³' in c or 'å¥½æ°—' in c: continue
                
                vals = pd.to_numeric(best_df[c], errors='coerce')
                if vals.isna().all(): continue
                
                vals_clean = vals.dropna()
                if len(vals_clean) == 0: continue
                
                has_decimal = any(abs(v - round(v)) > 0.001 for v in vals_clean)
                if has_decimal: continue
                
                v_max = vals.max()
                v_min = vals.min()
                if v_max > 100 or v_min < 0: continue
                
                if param_names[p_ptr] not in rename_map.values():
                    rename_map[c] = param_names[p_ptr]
                    used_cols.add(c)
                p_ptr += 1

    best_df.rename(columns=rename_map, inplace=True)
    
    # --- æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° (èª˜å°å“¡ã®æ’é™¤) ---
    if 'è»Šç•ª' in best_df.columns:
        best_df['è»Šç•ª'] = pd.to_numeric(best_df['è»Šç•ª'], errors='coerce')
        best_df = best_df.dropna(subset=['è»Šç•ª'])
        best_df['è»Šç•ª'] = best_df['è»Šç•ª'].astype(int)
    
    if 'é¸æ‰‹å' in best_df.columns:
        # èª˜å°å“¡å‰Šé™¤
        best_df = best_df[~best_df['é¸æ‰‹å'].astype(str).str.contains('èª˜å°|å…ˆé ­')]
        # è¨˜å·å‰Šé™¤
        best_df['é¸æ‰‹å'] = best_df['é¸æ‰‹å'].astype(str).str.replace(r'[â—â—‹â–²â–³Ã—æ³¨]', '', regex=True)
        
        prefs = ["åŒ—æµ·é“","é’æ£®","å²©æ‰‹","å®®åŸ","ç§‹ç”°","å±±å½¢","ç¦å³¶","èŒ¨åŸ","æ ƒæœ¨","ç¾¤é¦¬","åŸ¼ç‰","åƒè‘‰","æ±äº¬","ç¥å¥ˆå·","æ–°æ½Ÿ","å¯Œå±±","çŸ³å·","ç¦äº•","å±±æ¢¨","é•·é‡","å²é˜œ","é™å²¡","æ„›çŸ¥","ä¸‰é‡","æ»‹è³€","äº¬éƒ½","å¤§é˜ª","å…µåº«","å¥ˆè‰¯","å’Œæ­Œå±±","é³¥å–","å³¶æ ¹","å²¡å±±","åºƒå³¶","å±±å£","å¾³å³¶","é¦™å·","æ„›åª›","é«˜çŸ¥","ç¦å²¡","ä½è³€","é•·å´","ç†Šæœ¬","å¤§åˆ†","å®®å´","é¹¿å…å³¶","æ²–ç¸„"]
        prefs.sort(key=len, reverse=True)

        def extract_kdreams_info(val):
            val = str(val).strip()
            name = val
            pref = ""
            age = ""
            period = ""
            
            # Pattern 0: User Specified Regex
            user_regex = r'(?P<é¸æ‰‹å>\S+\s*\S*)\s+(?P<åºœçœŒ>[^/]+)/(?P<å¹´é½¢>\d+)/(?P<æœŸåˆ¥>\d+)'
            match_user = re.search(user_regex, val)
            if match_user:
                name = match_user.group('é¸æ‰‹å').strip()
                pref = match_user.group('åºœçœŒ').strip()
                age = match_user.group('å¹´é½¢').strip()
                period = match_user.group('æœŸåˆ¥').strip()
                return name, pref, age, period

            # Pattern 1: Nameã€Prefecture Periodã€‘
            match_brackets = re.search(r'ã€(.*?)ã€‘', val)
            if match_brackets:
                info = match_brackets.group(1)
                name = val.split('ã€')[0].strip()
                
                info = info.replace('ã€€', '').replace(' ', '')
                m_period = re.search(r'(\d+)æœŸ', info)
                if m_period:
                    period = m_period.group(1)
                    info = info.replace(m_period.group(0), '')
                
                m_age = re.search(r'(\d+)æ­³', info)
                if m_age:
                    age = m_age.group(1)
                    info = info.replace(m_age.group(0), '')
                    
                pref = info
                return name, pref, age, period

            # Pattern 2: Fallback
            m_period = re.search(r'(\d+)æœŸ', val)
            if m_period:
                period = m_period.group(1)
                val = val.replace(m_period.group(0), ' ')
                
            m_age = re.search(r'(\d+)æ­³', val)
            if m_age:
                age = m_age.group(1)
                val = val.replace(m_age.group(0), ' ')
            
            val = val.replace('/', ' ').replace('ã€€', ' ')
            
            val_norm = val.replace(' ', '')
            found_pref = None
            for p in prefs:
                if val_norm.endswith(p):
                    found_pref = p
                    break
            
            if found_pref:
                pref = found_pref
                p_regex = r"\s*".join(list(found_pref)) + r"\s*$"
                if re.search(p_regex, val):
                     val = re.sub(p_regex, '', val)
            
            name = val.strip()
            return name, pref, age, period

        # Apply extraction
        extracted = best_df['é¸æ‰‹å'].apply(extract_kdreams_info)
        
        # Assign back to columns
        best_df['é¸æ‰‹å'] = extracted.apply(lambda x: x[0])
        best_df['åºœçœŒ'] = extracted.apply(lambda x: x[1])
        best_df['å¹´é½¢'] = extracted.apply(lambda x: x[2])
        best_df['æœŸåˆ¥'] = extracted.apply(lambda x: x[3])

    # 5. Overwrite Line Column if Parsed from HTML Div (More Accurate)
    if 'lines_list' in meta and 'è»Šç•ª' in best_df.columns:
        # Create map: CarNum -> LineID (str 1,2,3)
        # lines_list = [[1,2,3], [4,5], [6]]
        car_line_map = {}
        line_str_map = {} # Line String "123"
        
        for idx, grp in enumerate(meta['lines_list']):
            line_id = str(idx + 1)
            line_s = "".join(map(str, grp))
            for car in grp:
                car_line_map[car] = line_id
                line_str_map[car] = line_s
                
        # Update DF
        # best_df['ãƒ©ã‚¤ãƒ³'] = ...
        # Ensure CarNum is int
        try:
            best_df['temp_car'] = best_df['è»Šç•ª'].astype(int)
            best_df['ãƒ©ã‚¤ãƒ³'] = best_df['temp_car'].map(line_str_map).fillna(best_df.get('ãƒ©ã‚¤ãƒ³', ''))
            # Also maybe we want LineID column? Logic usually uses 'ãƒ©ã‚¤ãƒ³' as ID or String?
            # logic_v2.calculate_ai_score uses "line_id = row['ãƒ©ã‚¤ãƒ³']" or "line_str".
            # Actually db_utils.run_global_features parses 'ãƒ©ã‚¤ãƒ³' column content (e.g. "123") to find length/pos.
            # So setting 'ãƒ©ã‚¤ãƒ³' to the full string ie "123" is correct for `run_global_features`.
            
            del best_df['temp_car']
            # print(f"DEBUG: Applied HTML Lines to DF: {line_str_map}")
        except: pass

    return best_df, meta


# ==========================================
# 2. Betting Strategy Logic
# ==========================================

def generate_betting_strategy(pred_df, ai_match_cars=None, score_col='äºˆæ¸¬å‹ç‡'):
    """
    Generates betting strategy and tickets based on prediction dataframe.
    """
    if ai_match_cars is None:
        ai_match_cars = []

    # Use data sorted by Win Rate (or score_col) for logic base
    # If score_col not in columns, fallback to 'äºˆæ¸¬å‹ç‡' or 'ai_score'
    if score_col not in pred_df.columns:
        if 'final_score' in pred_df.columns: score_col = 'final_score'
        elif 'ai_score' in pred_df.columns: score_col = 'ai_score'
        elif 'äºˆæ¸¬å‹ç‡' in pred_df.columns: score_col = 'äºˆæ¸¬å‹ç‡'
        
    df_logic = pred_df.sort_values(score_col, ascending=False).reset_index(drop=True)
    
    if len(df_logic) < 3:
        return {
            "type": "error",
            "title": "ãƒ‡ãƒ¼ã‚¿ä¸è¶³",
            "reason": "é¸æ‰‹ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
            "tickets": []
        }

    # Top players
    p1 = df_logic.iloc[0]
    p2 = df_logic.iloc[1]
    p3 = df_logic.iloc[2]
    
    # Normalize score to 0-100 scale roughly if it's raw score
    # But usually this logic expects Win Rate %.
    # If we are using 'final_score' (e.g. 80-120), thresholds need adjustment?
    # Original logic checks w1 < 25.0 etc.
    # If score is > 100, these checks will fail (always > 45).
    # We should normalize/interpret based on previous context. 
    # But for now, let's just make sure it runs. 
    # To fix "Unexpected Argument", simply adding the arg is enough.
    # However, logic values (w1, w2) are used for thresholds. 
    # If 'final_score' is passed (e.g. 115.5), w1=115.5.
    # w1 >= 45.0 is True. -> Teppan.
    # This might be acceptable for now as Antigravity Score is high.
    
    w1_raw = p1.get(score_col)
    w2_raw = p2.get(score_col)
    w3_raw = p3.get(score_col)
    
    # Safe conversion to float (handle None, NaN, and string values)
    def safe_float(v):
        if v is None:
            return 0.0
        try:
            return float(v)
        except (ValueError, TypeError):
            return 0.0
    
    w1_raw = safe_float(w1_raw)
    w2_raw = safe_float(w2_raw)
    w3_raw = safe_float(w3_raw)
    
    # Normalize to percentage if scores are not already percentages (e.g., raw scores > 50)
    total_score = df_logic[score_col].sum()
    if total_score is None or pd.isna(total_score):
        total_score = 0
    if total_score > 0 and w1_raw > 50:  # Likely raw scores, not percentages
        w1 = (w1_raw / total_score) * 100
        w2 = (w2_raw / total_score) * 100
        w3 = (w3_raw / total_score) * 100
    else:
        w1, w2, w3 = w1_raw, w2_raw, w3_raw
    
    c1 = p1['è»Šç•ª']
    c2 = p2['è»Šç•ª']
    c3 = p3['è»Šç•ª']
    
    # Others
    c4 = df_logic.iloc[3]['è»Šç•ª'] if len(df_logic) > 3 else None
    c5 = df_logic.iloc[4]['è»Šç•ª'] if len(df_logic) > 4 else None
    
    # --- Classification Logic ---
    race_type = "standard"
    reason = ""
    strategy_title = "ãƒãƒ©ãƒ³ã‚¹å‹"
    
    # Check High Return Candidate (AI Rules)
    is_high_return_mode = False
    target_hole_cars = sorted(list(set(ai_match_cars))) if ai_match_cars else []
    
    if target_hole_cars:
        top_car = int(c1)
        if any(int(tc) != top_car for tc in target_hole_cars):
            is_high_return_mode = True
            race_type = "snipe"
            reason = f"é«˜å›åç‡ãƒ‘ã‚¿ãƒ¼ãƒ³è©²å½“è»Šã‚ã‚Š ({','.join(map(str, target_hole_cars))})"
            strategy_title = "ğŸ’° ä¸€æ’ƒå›åç‹™ã„"

    place_name = pred_df['ç«¶è¼ªå ´'].iloc[0] if 'ç«¶è¼ªå ´' in pred_df.columns else ""
    bank_specs = db_utils.VELODROME_SPECS.get(place_name, (400, 30, 400)) # Default 400
    bank_len = bank_specs[2]
    bank_straight = bank_specs[0]

    # --- Logic V3: Star Bet (Focused Strategy) ---
    is_star_bet = False
    
    # Check Flags on P1
    p1_is_dom_makuri = p1.get('is_dom_makuri', False)
    p1_is_dom_nige = p1.get('is_dom_nige', False)
    p1_is_b_top = p1.get('is_b_top', False)
    
    if not is_high_return_mode:
        # 1. Dominant Makuri (SS Grade)
        if p1_is_dom_makuri:
            is_star_bet = True
            race_type = "star_makuri"
            strategy_title = "ğŸŒŸ åœ§å€’çš„æ²ã‚Š (SS)"
            reason = "åœ§å€’çš„æ²ã‚Šé¸æ‰‹ã«ã‚ˆã‚‹å®ŸåŠ›æ±ºç€æ¿ƒåš"
            confidence_level = "SS"
            recommended_points = {"3é€£å˜": 4, "2è»Šå˜": 2}
            
        # 2. Dominant Nige (Short Bank) -> S Grade
        elif p1_is_dom_nige and bank_straight < 50.0:
            is_star_bet = True
            race_type = "star_nige_short"
            strategy_title = "ğŸƒ åœ§å€’çš„é€ƒã’ [çŸ­] (S)"
            reason = "çŸ­èµ°è·¯ã§ã®åœ§å€’çš„é€ƒã’ (æŠ¼ã—åˆ‡ã‚Šæ¿ƒåš)"
            confidence_level = "S"
            recommended_points = {"3é€£å˜": 3, "2è»Šå˜": 1}
            
        # 3. B-Top (Short Bank) -> A Grade
        elif p1_is_b_top and bank_straight < 50.0:
            is_star_bet = True
            race_type = "star_btop_short"
            strategy_title = "ğŸš€ B-Top [çŸ­] (A)"
            reason = "çŸ­èµ°è·¯Ã—Bãƒˆãƒƒãƒ— (ãƒ©ã‚¤ãƒ³æ±ºç€æ¿ƒåš)"
            confidence_level = "A"
            recommended_points = {"3é€£å˜": 6, "2è»Šå˜": 2}

    diff_1_2 = w1 - w2
    
    # --- NEW LOGIC: Suji & Line Analysis ---
    suji_mode = None # A, B, C or None
    
    # 1. Parse Line Config (e.g. "3-3-1")
    line_counts = []
    line_config_str = "ä¸æ˜"
    if 'ãƒ©ã‚¤ãƒ³' in pred_df.columns:
        # Group by Line Content/ID
        # 'ãƒ©ã‚¤ãƒ³' col contains "123" or similar
        # Get unique line strings (careful of empty or default)
        valid_lines = pred_df[pred_df['ãƒ©ã‚¤ãƒ³'].astype(str).str.len() > 0]['ãƒ©ã‚¤ãƒ³'].unique()
        # Filter out lines that seem to be just single "0" or empty
        valid_lines = [l for l in valid_lines if l not in ["0", ""] and len(str(l)) > 0]
        
        # Calculate lengths
        lengths = [len(str(l)) for l in valid_lines]
        lengths.sort(reverse=True)
        line_counts = lengths
        line_config_str = "-".join(map(str, lengths))
    
    # 2. Get Race Class & Specs
    race_class = "A" # Default
    if 'ç´šç­' in pred_df.columns:
        classes = pred_df['ç´šç­'].astype(str).unique()
        if any('S' in c for c in classes): race_class = "S"
        elif any('A3' in c for c in classes): race_class = "A3"
    
    # 3. Calculate Gap for Safety Valve (Favorite vs Line Partner)
    # Find P1's line
    p1_line_val = p1.get('ãƒ©ã‚¤ãƒ³', '')
    p1_base_score = p1.get('base_score', 80.0)
    
    partner_gap = 999.0
    p1_partner = None
    
    if p1_line_val and str(p1_line_val) not in ["0", ""]:
        # Find others in same line
        same_line_df = pred_df[pred_df['ãƒ©ã‚¤ãƒ³'] == p1_line_val]
        others = same_line_df[same_line_df['è»Šç•ª'] != c1]
        
        if not others.empty:
            # Assume strongest partner is the "Suji" target
            # Sort by score
            others_sorted = others.sort_values(by='base_score', ascending=False)
            best_partner = others_sorted.iloc[0]
            p1_partner = best_partner
            p_score = best_partner.get('base_score', 80.0)
            partner_gap = abs(p1_base_score - p_score)
    
    # 4. Evaluate Suji Conditions
    # Only if not High Return Mode
    if not is_high_return_mode:
        
        # [A] Teppan Suji (70%+)
        # Cond: (A3 & 4-car-line) OR (A & 2-bunsen & Short)
        # Safety: Gap <= 10
        cond_a_1 = (race_class == "A3" and max(line_counts) >= 4) if line_counts else False
        cond_a_2 = (race_class == "A" and len(line_counts) == 2 and bank_straight < 50.0)
        
        if (cond_a_1 or cond_a_2) and partner_gap <= 10.0:
            suji_mode = "A"
            
        # [B] High Prob Suji (60%+)
        # Cond: (A3) OR (A & 3-bunsen & Long) OR (S & 3-bunsen & 33Bank)
        # [B] Suji Lead (60%+)
        # SIMPLIFIED: Default to B if line exists and not Hosogire S-class
        if not suji_mode: # Only check B if A wasn't triggered
            cond_b_1 = True # Default
            
            is_valid_b = False
            if cond_b_1:
                 # Safety Valve Logic (S<=10, A<=15) applied here
                 if race_class == "S": is_valid_b = (partner_gap <= 10.0)
                 else: is_valid_b = (partner_gap <= 15.0)
                 
            if is_valid_b:
                suji_mode = "B"
                
        # [C] Dangerous Suji (40%-)
        # Cond: (S & 4-bunsen/Hosogire)
        is_hosogire = (len(line_counts) >= 4)
        cond_c_1 = (race_class == "S" and is_hosogire)
        
        if cond_c_1:
            suji_mode = "C"
        elif partner_gap > 20.0 and race_class == "S": # Safety Valve fail -> Chaos/C (Tightened from 25.0)
             suji_mode = "C" # Gap too wide in S class often breaks line history
             
    # --- Thresholds adjusted for normalized win rate scale ---
    if not is_high_return_mode and not is_star_bet:
        if suji_mode == "A":
            race_type = "suji_fix"
            reason = f"é‰„æ¿ã‚¹ã‚¸ (æ§‹æˆ:{line_config_str}, Gap:{partner_gap:.1f})"
            # Strict Check for Geki-Atsu (ORIGINAL STRICT CONDITIONS)
            # Reverting to the exact logic that produced 80% hit rate.
            is_strict = False
            
            # 1. S-Class: Strict (33 Bank + 3 lines + Small Gap)
            if race_class == "S":
                if len(line_counts) == 3 and bank_len in [333, 335] and partner_gap <= 5.0:
                    is_strict = True
                    
            # 2. A-Class: Strict (Long Straight + 3 lines + Small Gap)
            elif race_class == "A":
                bs = bank_specs[1] if isinstance(bank_specs, (list, tuple)) and len(bank_specs) > 1 else 30.0
                if len(line_counts) == 3 and bs >= 50.0 and partner_gap <= 10.0:
                    is_strict = True
                    
            # 3. Challenge (A3): Just Gap
            elif race_class == "A3":
                if partner_gap <= 10.0:
                    is_strict = True
            
            suffix = " ğŸ”¥(æ¿€ç†±)" if is_strict else ""
            
            strategy_title = f"ğŸ”’ ã‚¹ã‚¸ä¸€ç‚¹å‹è² {suffix}"
            confidence_level = "æ¥µ"
            # Ensure 4 points for 3-Rentan
            recommended_points = {"3é€£å˜": 4, "2è»Šå˜": 1}
            
        elif suji_mode == "B":
            race_type = "suji_lead"
            reason = f"æœ‰åŠ›ã‚¹ã‚¸ (æ§‹æˆ:{line_config_str}, Gap:{partner_gap:.1f})"
            
            # Strict Check for Geki-Atsu (ORIGINAL STRICT CONDITIONS)
            is_strict = False
            
            if race_class == "S":
                if len(line_counts) == 3 and bank_len in [333, 335] and partner_gap <= 5.0:
                    is_strict = True
            elif race_class == "A":
                bs = bank_specs[1] if isinstance(bank_specs, (list, tuple)) and len(bank_specs) > 1 else 30.0
                if len(line_counts) == 3 and bs >= 50.0 and partner_gap <= 10.0:
                    is_strict = True
            elif race_class == "A3":
                if partner_gap <= 10.0:
                    is_strict = True
            
            suffix = " ğŸ”¥(æ¿€ç†±)" if is_strict else ""
            
            strategy_title = f"ğŸ¯ ã‚¹ã‚¸æœ¬ç·šãƒ»å …å®Ÿ{suffix}"
            confidence_level = "é«˜"
            recommended_points = {"3é€£å˜": 8, "2è»Šå˜": 3}
            
        elif suji_mode == "C":
            race_type = "line_breaker"
            reason = f"ã‚¹ã‚¸å´©ã‚Œè­¦æˆ’ (æ§‹æˆ:{line_config_str})"
            strategy_title = "âš¡ ãƒ©ã‚¤ãƒ³ãƒ–ãƒ¬ã‚¤ã‚«ãƒ¼ (åˆ¥ç·šç‹™ã„)"
            confidence_level = "ä½"
            recommended_points = {"3é€£å˜": 16, "2è»Šå˜": 6} # Wide net
            
        elif w1 < 12.0:  # Below average = no clear favorite
            race_type = "skip"
            reason = "çµ¶å¯¾çš„æœ¬å‘½ä¸åœ¨ (è¦‹é€ã‚Šæ¨å¥¨)"
            strategy_title = "ğŸ›‘ è¦‹é€ã‚Š"
            
        # 1. Stricter "Teppan" Definition (Fallback if no Suji Mode caught or Standard)
        elif w1 >= 30.0 and diff_1_2 >= 10.0:
            is_teppan = True
            race_type = "teppan"
            reason = "åœ§å€’çš„æœ¬å‘½ (1å¼·) - ä¿¡é ¼åº¦é«˜"
            strategy_title = "ğŸ° é‰„æ¿éŠ€è¡Œãƒ¬ãƒ¼ã‚¹"
            confidence_level = "é«˜"
            recommended_points = {"3é€£å˜": 6, "2è»Šå˜": 3}  # Tight points
        elif w1 >= 25.0 and w2 >= 20.0:
            race_type = "two_strong"
            reason = "2å¼·å¯¾æ±º (é †å½“ãƒ»æŠ˜ã‚Šè¿”ã—æ¨å¥¨)"
            strategy_title = "âš”ï¸ 2å¼·å¯¾æ±º"
            confidence_level = "é«˜"
            recommended_points = {"3é€£å˜": 8, "2è»Šå˜": 4}
        elif (w1 - w3) < 5.0:
            race_type = "chaos"
            reason = "å¤§æ··æˆ¦ (ã‚ªãƒƒã‚ºå‰²ã‚Œãƒ»ç©´ç‹™ã„æ¨å¥¨)"
            strategy_title = "ğŸ’£ ç©´ç‹™ã„ãƒ»é«˜é…å½“"
            confidence_level = "ä½"
            recommended_points = {"3é€£å˜": 18, "2è»Šå˜": 9}  # Wide points for chaos
        else:
            race_type = "standard"
            reason = "ä¸­æ··æˆ¦ (è»¸é¸å®šãŒéµ)"
            strategy_title = "âš–ï¸ æ¨™æº–"
            confidence_level = "ä¸­"
            recommended_points = {"3é€£å˜": 12, "2è»Šå˜": 6}

    # --- Pseudo-EV Calculation ---
    # Without live odds, use AI win rate as probability proxy
    # EV = (Win Rate / 100) * Assumed_Payout - 1
    # Assumed Payout based on experience: Teppan ~3x, Two Strong ~5x, Standard ~10x, Chaos ~20x
    payout_map = {
        "teppan": 3.0, "two_strong": 5.0, "standard": 10.0, "chaos": 20.0, "snipe": 30.0, "skip": 1.0,
        "suji_fix": 2.5, "suji_lead": 6.0, "line_breaker": 25.0 
    }
    assumed_payout = payout_map.get(race_type, 10.0)
    pseudo_ev = (w1 / 100.0) * assumed_payout - 1.0
    
    # EV-based recommendation
    ev_comment = ""
    if pseudo_ev >= 0.5:
        ev_comment = "æœŸå¾…å€¤â— (ç©æ¥µçš„ã«è²·ãˆã‚‹)"
    elif pseudo_ev >= 0.0:
        ev_comment = "æœŸå¾…å€¤â—‹ (æ¨™æº–)"
    else:
        ev_comment = "æœŸå¾…å€¤â–³ (ç‚¹æ•°ã‚’çµã‚‹ã‹è¦‹é€ã‚Šæ¨å¥¨)"

    # --- Ticket Generation ---
    # --- Ticket Generation ---
    rec_tickets = []
    structured_bets = []
    
    # 2è»Šå˜: Logic Moved to End to allow deduplication

    if race_type == "skip":
        pass 
        
    elif race_type == "star_makuri":
        # Strategy: p1 (Makuri) -> p2, p3 (Formation)
        # Trust p1 completely for 1st.
        rec_tickets.append(f"3é€£å˜: {c1} â†’ {c2},{c3} â†’ {c2},{c3},{c4}")
        rec_tickets.append(f"2è»Šå˜: {c1} â†’ {c2},{c3}")
        # rec_tickets.append(f"3é€£è¤‡: {c1} - {c2} - {c3},{c4}") # Removed for Focus
        
        # Structure
        structured_bets.append({'type': '3é€£å˜', '1': [c1], '2': [c2,c3], '3': [c2,c3,c4]})
        structured_bets.append({'type': '2è»Šå˜', '1': [c1], '2': [c2,c3]})
        
    elif race_type == "star_nige_short":
        # Strategy: p1 (Nige) -> Partner (One-Two)
        pt = int(p1_partner['è»Šç•ª']) if p1_partner is not None else int(c2)
        
        # Clean list candidates
        th_cands = [x for x in [c2,c3,c4] if int(x) != int(c1) and int(x) != pt]
        th_str = ",".join(map(str, th_cands)) if th_cands else "å…¨"
        
        rec_tickets.append(f"3é€£å˜: {c1} â†’ {pt} â†’ {th_str}")
        rec_tickets.append(f"2è»Šå˜: {c1} â†’ {pt} (1ç‚¹)")
        
        # Structure
        structured_bets.append({'type': '3é€£å˜', '1': [c1], '2': [pt], '3': th_cands})
        structured_bets.append({'type': '2è»Šå˜', '1': [c1], '2': [pt]})
        
    elif race_type == "star_btop_short":
        # Strategy: p1 (B-Top) = Partner (Folding/Zubuzubu cover)
        pt = int(p1_partner['è»Šç•ª']) if p1_partner is not None else int(c2)
        th_cands = [x for x in [c2,c3,c4] if int(x) != int(c1) and int(x) != int(pt)]
        th_str = ",".join(map(str, th_cands))
        
        rec_tickets.append(f"3é€£å˜: {c1} â†” {pt} â†’ {th_str}")
        rec_tickets.append(f"2è»Šå˜: {c1} â†” {pt}")
        
        # Structure
        structured_bets.append({'type': '3é€£å˜', '1': [c1, pt], '2': [c1, pt], '3': th_cands})
        structured_bets.append({'type': '2è»Šå˜', '1': [c1, pt], '2': [c1, pt]})

    elif race_type == "snipe":
        # High Return / Specific Hole cars
        tc = target_hole_cars[0] if target_hole_cars else c3
        rec_tickets.append(f"3é€£å˜ (ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³): {tc} - {c1},{c2} - {c1},{c2},{c3}")
        # rec_tickets.append(f"3é€£è¤‡: {tc} - {c1} - {c2},{c3}") # Removed
        
        if len(target_hole_cars) == 1:
            tc = target_hole_cars[0]
            # structured_bets.append({'type': '3rencpu_axis1_flow', 'axis': [tc], 'flow': [c1,c2,c3]})
            # structured_bets.append({'type': 'wide_axis1_flow', 'axis': [tc], 'flow': [c1,c2]})
            pass
        else:
            # structured_bets.append({'type': '3rencpu_box', 'cars': target_hole_cars + [c1, c2]})
            # structured_bets.append({'type': 'wide_box', 'cars': target_hole_cars})
            pass

    elif race_type == "suji_fix":
        # A: Suji Fix
        pt = int(p1_partner['è»Šç•ª']) if p1_partner is not None else int(c2)
        
        # 3rd candidates: c2, c3, c4 (excluding c1, pt)
        others = [x for x in [c2, c3, c4] if int(x) != int(c1) and int(x) != pt]
        s_3rd_real = ",".join(map(str, others))
        
        if s_3rd_real:
            rec_tickets.append(f"3é€£å˜: {c1} â†’ {pt} â†’ {s_3rd_real}")
        else:
            rec_tickets.append(f"3é€£å˜: {c1} â†’ {pt} â†’ å…¨") # Fallback
            
        # rec_tickets.append(f"3é€£è¤‡: {c1} - {pt} - {s_3rd_real}") # Removed
        # Standardized 2T is added at end? No, logic moved

    elif race_type == "suji_lead":
        # B: Suji Lead
        pt = int(p1_partner['è»Šç•ª']) if p1_partner is not None else int(c2)
        other_heads = [x for x in [c2, c3] if int(x) != int(c1) and int(x) != pt] # Simplified
        s_2nd = ",".join(map(str, [pt] + other_heads))
        
        rec_tickets.append(f"3é€£å˜ (ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³): {c1} â†’ {s_2nd} â†’ {s_2nd},{c4}")
        # rec_tickets.append(f"3é€£è¤‡: {c1} - {pt} - {c3},{c4}") # Removed

    elif race_type == "line_breaker":
        # C: Line Breaker
        targets = [x for x in [c2, c3, c4] if int(x) != int(c1)]
        if not targets: targets = [c2, c3]
        s_targets = ",".join(map(str, targets))
        
        # rec_tickets.append(f"3é€£è¤‡ (BOX): {c1},{c2},{c3},{c4}") # Removed
        # rec_tickets.append(f"ãƒ¯ã‚¤ãƒ‰: {c1} = {s_targets}") # Removed
        
        # Alternative: Just recommend Skip or Wide? User dislikes wide.
        rec_tickets.append(f"3é€£å˜ (Box): {c1},{c2},{c3}")

    elif race_type == "teppan":
        # Ironclad
        third_row = [x for x in [c2, c3, c4] if x and x != c2]
        s_3rd = ",".join(map(str, third_row))
        
        rec_tickets.append(f"3é€£å˜ (ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³): {c1} - {c2},{c3} - {s_3rd}")
        # rec_tickets.append(f"3é€£è¤‡: {c1} - {c2} - {c3},{c4}") # Removed
        
        second_row = [x for x in [c2, c3] if x]
        third_row_all = [x for x in [c2, c3, c4, c5] if x]
        structured_bets.append({'type': '3rentan_form', '1st': [c1], '2nd': [c2], '3rd': third_row_all})

    elif race_type == "two_strong":
        # c1 and c2 are strong. Fold (Ura-Omote).
        rec_tickets.append(f"3é€£å˜ (2è»¸): {c1} = {c2} - {c3},{c4}")
        # rec_tickets.append(f"3é€£è¤‡: {c1} - {c2} - {c3},{c4}") # Removed
        
        structured_bets.append({'type': '3rentan_fold', '1st': [c1, c2], '2nd': [c1, c2], '3rd': [c3, c4] if c4 else [c3]})

    elif race_type == "chaos":
        # Chaos
        third_list = [x for x in [c2, c3, c4, c5] if x]
        third_str = ",".join(map(str, third_list))
        # flow_list = [x for x in [c2, c3, c4] if x] # For Wide
        # flow_str = ",".join(map(str, flow_list))
        
        rec_tickets.append(f"3é€£å˜ (ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³): {c1},{c2} - {c1},{c2},{c3} - {third_str}")
        # rec_tickets.append(f"3é€£è¤‡: {c1} - {c2},{c3} - {third_str}") # Removed
        # rec_tickets.append(f"ãƒ¯ã‚¤ãƒ‰: {c1} = {flow_str}") # Removed
        
        structured_bets.append({'type': '3rencpu_form', '1st': [c1], '2nd': [c2, c3], '3rd': [c2, c3, c4, c5]})
        
    else: # Standard
        third_candidates = [c1, c2, c3, c4, c5]
        third_row = [x for x in third_candidates if x and x not in [c1, c2]]
        s_3rd = ",".join(map(str, third_row))
        w_flow = [x for x in [c3, c4] if x]
        w_flow_str = ",".join(map(str, w_flow))
        
        rec_tickets.append(f"3é€£å˜ (ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³): {c1} - {c2},{c3} - {s_3rd}")
        # rec_tickets.append(f"3é€£è¤‡: {c1} - {c2} - {s_3rd}") # Removed
        # if w_flow:
            # rec_tickets.append(f"ãƒ¯ã‚¤ãƒ‰: {c1} = {w_flow_str}") # Removed

        structured_bets.append({'type': '3rentan_fold', '1st': [c1, c2], '2nd': [c1, c2], '3rd': third_row})
        # structured_bets.append({'type': 'wide_axis1_flow', 'axis': [c1], 'flow': [c3, c4]}) # Removed

    # ==========================================
    # Deduplicated Base 2-Car Exacta Logic
    # ==========================================
    # Identify what is already covered
    covered_2t_pairs = set()
    
    # helper to expand simple range strings "1,2,3" -> [1,2,3]
    import re
    def _parse_cars(s):
        parts = s.split(',')
        res = []
        for p in parts:
            # Extract first number sequence (Car Number)
            # e.g. "5 (1ç‚¹)" -> 5
            nums = re.findall(r'(\d+)', p)
            if nums:
                try: res.append(int(nums[0]))
                except: pass
        return res


    for t in rec_tickets:
        if "2è»Šå˜" in t:
            body = t.split(':')[-1].strip()
            if 'â†”' in body: # Fold
                parts = body.split('â†”')
                if len(parts) >= 2:
                    gSet1, gSet2 = _parse_cars(parts[0]), _parse_cars(parts[1])
                    for x in gSet1:
                        for y in gSet2:
                            covered_2t_pairs.add((x, y))
                            covered_2t_pairs.add((y, x))
            elif 'â†’' in body: # Direct
                parts = body.split('â†’')
                if len(parts) >= 2:
                    gHead, gTail = _parse_cars(parts[0]), _parse_cars(parts[1])
                    for x in gHead:
                        for y in gTail:
                            covered_2t_pairs.add((x, y))
            elif '=' in body: # Same as fold
                parts = body.split('=')
                if len(parts) >= 2:
                    gSet1, gSet2 = _parse_cars(parts[0]), _parse_cars(parts[1])
                    for x in gSet1:
                        for y in gSet2:
                            covered_2t_pairs.add((x, y))
                            covered_2t_pairs.add((y, x))
                            
    # Generate Base 2T: Rank 1 -> Rank 2,3,4
    # But only allow (c1, x) if not in covered_2t_pairs
    base_flow_full = [x for x in [c2, c3, c4] if x]
    base_flow_dedup = []
    
    for cand in base_flow_full:
        try:
            cand_int = int(cand)
            c1_int = int(c1)
            if (c1_int, cand_int) not in covered_2t_pairs:
                base_flow_dedup.append(cand)
        except:
             # Fallback if non-int
             base_flow_dedup.append(cand)

    if base_flow_dedup:
        flow_str_2t = ",".join(map(str, base_flow_dedup))
        # Insert at 0 so it appears first (Base)
        rec_tickets.insert(0, f"2è»Šå˜: {c1} â†’ {flow_str_2t}")

    return {
        "type": race_type,
        "title": strategy_title,
        "reason": reason,
        "tickets": rec_tickets,
        "structured_bets": structured_bets,
        "top_win_rate": w1,
        "top_name": p1['é¸æ‰‹å'],
        "confidence_level": confidence_level,
        "recommended_points": recommended_points,
        "pseudo_ev": round(pseudo_ev, 2),
        "ev_comment": ev_comment
    }

# ==========================================
# 2b. Special Bonus Strategy (ç‰¹æ³¨äºˆæƒ³)
# ==========================================

def generate_bonus_strategy(pred_df, score_col='ai_score'):
    """
    Generates a SPECIAL betting strategy based on the player with the HIGHEST BONUS.
    This is different from the main strategy which uses highest final score.
    """
    if pred_df is None or pred_df.empty:
        return {
            "type": "error",
            "title": "ãƒ‡ãƒ¼ã‚¿ä¸è¶³",
            "reason": "é¸æ‰‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“",
            "tickets": [],
            "strategy_type": "special_bonus"
        }
    
    df = pred_df.copy()
    
    # Calculate bonus if not already present
    if 'bonus' not in df.columns:
        if 'ai_score' in df.columns and 'base_score' in df.columns:
            df['bonus'] = df['ai_score'] - df['base_score']
        else:
            return {
                "type": "error",
                "title": "ãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—ä¸å¯",
                "reason": "ai_score/base_scoreãŒã‚ã‚Šã¾ã›ã‚“",
                "tickets": [],
                "strategy_type": "special_bonus"
            }
    
    # Find max bonus player
    max_bonus = df['bonus'].max()
    if pd.isna(max_bonus) or max_bonus <= 0:
        return {
            "type": "skip",
            "title": "ç‰¹æ³¨ãªã—",
            "reason": "æœ‰æ„ãªåŠ ç‚¹é¸æ‰‹ãŒã„ã¾ã›ã‚“",
            "tickets": [],
            "strategy_type": "special_bonus"
        }
    
    # Sort by bonus descending
    df_bonus = df.sort_values('bonus', ascending=False).reset_index(drop=True)
    
    # Top bonus player is the AXIS
    p_axis = df_bonus.iloc[0]
    c_axis = p_axis['è»Šç•ª']
    bonus_val = p_axis['bonus']
    axis_name = p_axis.get('é¸æ‰‹å', 'ä¸æ˜')
    
    # Get secondary players (by ai_score for flow)
    df_score = df.sort_values(score_col, ascending=False).reset_index(drop=True)
    
    # Get top 4 by score (excluding axis if present)
    flow_candidates = []
    for idx, row in df_score.iterrows():
        if row['è»Šç•ª'] != c_axis:
            flow_candidates.append(row['è»Šç•ª'])
        if len(flow_candidates) >= 4:
            break
    
    c2 = flow_candidates[0] if len(flow_candidates) > 0 else None
    c3 = flow_candidates[1] if len(flow_candidates) > 1 else None
    c4 = flow_candidates[2] if len(flow_candidates) > 2 else None
    c5 = flow_candidates[3] if len(flow_candidates) > 3 else None
    
    # Generate tickets with bonus player as axis
    rec_tickets = []
    second_row = [x for x in [c2, c3] if x]
    third_row = [x for x in [c2, c3, c4, c5] if x]
    s_2nd = ",".join(map(str, second_row))
    s_3rd = ",".join(map(str, third_row))
    
    if second_row and third_row:
        rec_tickets.append(f"3é€£å˜ (ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³): {c_axis} - {s_2nd} - {s_3rd}")
        rec_tickets.append(f"2è»Šå˜: {c_axis} â†’ {s_2nd}")
        rec_tickets.append(f"3é€£è¤‡: {c_axis} - {c2} - {c3},{c4}")
        rec_tickets.append(f"ãƒ¯ã‚¤ãƒ‰: {c_axis} = {c2},{c3}")
    
    # Structured bets for hit calculation
    structured_bets = []
    structured_bets.append({'type': '3rentan_form', '1st': [c_axis], '2nd': second_row, '3rd': third_row})
    structured_bets.append({'type': '2shatan', '1st': [c_axis], '2nd': second_row})
    structured_bets.append({'type': 'wide_axis1_flow', 'axis': [c_axis], 'flow': [c2, c3] if c3 else [c2]})
    
    # Confidence based on bonus amount
    if bonus_val >= 9.0:
        confidence = "é«˜"
        title = "ğŸ ç‰¹æ³¨äºˆæƒ³ (é«˜åŠ ç‚¹)"
    elif bonus_val >= 7.0:
        confidence = "ä¸­"
        title = "â­ ç‰¹æ³¨äºˆæƒ³ (åŠ ç‚¹ã‚ã‚Š)"
    else:
        confidence = "ä½"
        title = "ğŸ“Œ ç‰¹æ³¨äºˆæƒ³"
    
    return {
        "type": "special_bonus",
        "title": title,
        "reason": f"AIåŠ ç‚¹æœ€å¤§: {axis_name} (+{bonus_val:.1f}ç‚¹)",
        "tickets": rec_tickets,
        "structured_bets": structured_bets,
        "axis_car": c_axis,
        "axis_name": axis_name,
        "bonus_value": bonus_val,
        "confidence_level": confidence,
        "strategy_type": "special_bonus"
    }

# ==========================================
# 2c. Hybrid Strategy (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰äºˆæƒ³)
# ==========================================

def generate_hybrid_strategy(pred_df, score_col='ai_score', meta=None):
    """
    Generates OPTIMAL betting strategy based on race type analysis.
    Uses "Suji-Rate" and "Score Gap" logic to switch between Suji-Fix, Suji-Lead, and Ana-Nerai.
    """
    if pred_df is None or pred_df.empty:
        return {
            "type": "error",
            "title": "ãƒ‡ãƒ¼ã‚¿ä¸è¶³",
            "reason": "é¸æ‰‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“",
            "tickets_3rentan": [],
            "tickets_2shatan": [],
            "structured_bets_3rentan": [],
            "structured_bets_2shatan": [],
            "strategy_type": "hybrid"
        }

    # Girls Keirin Exclusion
    is_girls = False
    if 'class_code' in pred_df.columns:
        if 'L' in pred_df['class_code'].values: is_girls = True
    if 'ç´šç­' in pred_df.columns:
        if pred_df['ç´šç­'].astype(str).str.contains('L').any(): is_girls = True
    if 'ã‚¯ãƒ©ã‚¹' in pred_df.columns:
        if pred_df['ã‚¯ãƒ©ã‚¹'].astype(str).str.contains('ã‚¬ãƒ¼ãƒ«ã‚º').any(): is_girls = True
        
    if is_girls:
        return {
            "type": "disabled",
            "title": "å¯¾è±¡å¤–",
            "reason": "ã‚¬ãƒ¼ãƒ«ã‚ºã‚±ã‚¤ãƒªãƒ³ã¯äºˆæ¸¬å¯¾è±¡å¤–ã§ã™",
            "tickets_3rentan": [],
            "tickets_2shatan": [],
            "structured_bets_3rentan": [],
            "structured_bets_2shatan": [],
            "strategy_type": "hybrid"
        }
    
    df = pred_df.copy()
    
    # Ensure bonus column exists
    if 'bonus' not in df.columns:
        if 'ai_score' in df.columns and 'base_score' in df.columns:
            df['bonus'] = df['ai_score'] - df['base_score']
        else:
            df['bonus'] = 0.0
    
    # Get rankings
    top_main = df.sort_values(score_col, ascending=False).reset_index(drop=True)
    top_bonus = df.sort_values('bonus', ascending=False).reset_index(drop=True)
    
    # Extract key players
    def get_car(idx): return int(top_main.iloc[idx]['è»Šç•ª']) if len(top_main) > idx else 0
    m1, m2, m3, m4 = get_car(0), get_car(1), get_car(2), get_car(3)
    b1 = int(top_bonus.iloc[0]['è»Šç•ª'])
    
    m1_name = top_main.iloc[0].get('é¸æ‰‹å', f'{m1}ç•ª')
    b1_name = top_bonus.iloc[0].get('é¸æ‰‹å', f'{b1}ç•ª')
    b1_bonus = float(top_bonus.iloc[0]['bonus'])
    
    # --- Score Gap Calculation ---
    s1 = float(top_main.iloc[0][score_col])
    s2 = float(top_main.iloc[1][score_col]) if len(top_main) > 1 else s1
    s3 = float(top_main.iloc[2][score_col]) if len(top_main) > 2 else s2
    diff_1_2 = s1 - s2
    diff_1_3 = s1 - s3

    # --- Line & Class Analysis ---
    race_class = meta.get('race_class', 'Aç´š') if meta else 'Aç´š' 
    place_name = meta.get('place', '') if meta else ''
    
    # Infer Line Config (n_bun_sen)
    # Check if temp_line_id exists (from advanced_logic) or parse 'ãƒ©ã‚¤ãƒ³'
    if 'temp_line_id' in df.columns:
        uniq_lines = df[df['temp_line_id'] != -1]['temp_line_id'].unique()
        n_lines = len(uniq_lines)
    elif 'ãƒ©ã‚¤ãƒ³' in df.columns: # Naive parse if needed, usually temp_line_id is safe
        uniq_lines = df['ãƒ©ã‚¤ãƒ³'].unique()
        n_lines = len(uniq_lines)
    else:
        n_lines = 3 # Default
        
    # Max Line Length
    if 'line_length' in df.columns:
        max_line_len = df['line_length'].max()
    else:
         max_line_len = 0 # Unknown

    # Identify Conditions (A/B/C) based on Plan
    is_suji_fix = False    # A: High Suji (>70%)
    is_suji_lead = False   # B: High Suji (>60%)
    is_ana_nerai = False   # C: Low Suji (<40%) or Safety Valve Triggered
    
    # Condition A (Fix)
    if 'ãƒãƒ£ãƒ¬ãƒ³ã‚¸' in race_class:
        if max_line_len >= 4: is_suji_fix = True
    elif 'Aç´š' in race_class:
        # Short Bank (333/335) Check? Plan says "Short Bank: 2-bun-sen"
        # We check bank via place_name (simplified)
        short_banks = ["æ¾æˆ¸", "å°ç”°åŸ", "ä¼Šæ±", "å¯Œå±±", "å¥ˆè‰¯", "é˜²åºœ", "å‰æ©‹"] 
        if place_name in short_banks and n_lines == 2:
            is_suji_fix = True

    # Condition B (Lead) - if not Fix
    # SIMPLIFIED: Almost always Suji-Lead unless Gap is too large
    if not is_suji_fix:
        is_suji_lead = True 
        # (We will filter this via Safety Valve later)
        
    # Condition C (Ana) - if not Fix/Lead
    # SIMPLIFIED: Only explicit chaotic conditions
    if int(n_lines) >= 4 and 'Sç´š' in race_class: # S-Class Hosogire
         is_ana_nerai = True
         is_suji_lead = False

    # --- Safety Valve (Score Gap) ---
    valved_reason = ""
    # Challenge/A-Class Valve: Gap > 10 -> Ana
    if 'ãƒãƒ£ãƒ¬ãƒ³ã‚¸' in race_class or 'Aç´š' in race_class:
        # Check Gap between Line Leader and Partner
        # Simplified: Check diff_1_2 if m2 is partner. 
        # Ideally we check partner score. For now, use global diff_1_2 as proxy for "Dominance but Risky"
        # Wait, plan says "Line Partner Gap".
        # If m1 is line leader, find m1's partner.
        # If we can't find partner easily, assume m2 is main rival.
        # Actually, if diff_1_2 > 10, it implies m1 is dominant. 
        # But if partner is weak (Gap large), Suji fails. 
        # So "Gap > 10" refers to (m1_score - partner_score).
        # We need partner score.
        m1_row = top_main.iloc[0]
        m1_line = m1_row.get('temp_line_id', -1)
        # Find partner (same line, pos 2)
        partners = df[(df['temp_line_id'] == m1_line) & (df['è»Šç•ª'] != m1)]
        if not partners.empty:
             # Max score of partner
             p_score = partners[score_col].max()
             gap = s1 - p_score
             
             if (is_suji_fix or is_suji_lead) and gap > 15.0:
                 is_suji_fix = False
                 is_suji_lead = False
                 is_ana_nerai = True
                 valved_reason = f" (å¾—ç‚¹å·®{gap:.1f}éå¤§ã«ã¤ãå¥½æ©Ÿåˆ°æ¥)"
        
    # S-Class Valve: Gap > 5 -> Ana
    if 'Sç´š' in race_class:
        m1_row = top_main.iloc[0]
        m1_line = m1_row.get('temp_line_id', -1)
        partners = df[(df.get('temp_line_id') == m1_line) & (df['è»Šç•ª'] != m1)]
        if not partners.empty:
             p_score = partners[score_col].max()
             gap = s1 - p_score
             if (is_suji_fix or is_suji_lead) and gap > 10.0:
                 is_suji_fix = False
                 is_suji_lead = False
                 is_ana_nerai = True
                 valved_reason = f" (Sç´šå¾—ç‚¹å·®{gap:.1f}éå¤§)"

    # --- Strategy Generation ---
    race_type = 'æ¨™æº–'
    race_type_emoji = 'ğŸ“Š'
    race_type_reason = "æ¨™æº–çš„ãªå±•é–‹"
    
    l1_3r, l2_3r, l3_3r = [], [], []
    tickets_3r, tickets_2s = [], []
    struct_3r, struct_2s = [], []
    pattern_3r, pattern_2s = "", ""
    
    # 1. Ana-Nerai Mode
    if is_ana_nerai:
        race_type = 'ç©´ç‹™ã„'
        race_type_emoji = 'ğŸ’£'
        race_type_reason = f"ã‚¹ã‚¸ä¿¡é ¼åº¦ä½{valved_reason} - åˆ¥ç·šãƒ»ãƒœãƒƒã‚¯ã‚¹æ¨å¥¨"
        
        # Strategy: Line Breaker (Top1 -> Separate Line Top)
        # Find Separate Line Top (m2 if diff line, else m3)
        # Or just Top 3 Box
        
        # 3Ren: Box (m1, m2, m3, m4/b1)
        box_cars = list(set([m1, m2, m3, b1]))
        l1_3r, l2_3r, l3_3r = box_cars, box_cars, box_cars
        
        # 2Sha: m1 - m2, m3 (Multi)
        tickets_2s.append(f"2è»Šå˜: {m1} â†” {m2}, {m3} (åˆ¥ç·šè‡ªåŠ›)")
        tickets_3r.append(f"3é€£è¤‡/ãƒ¯ã‚¤ãƒ‰: {m1},{m2},{m3},{b1} BOX")
        
        pattern_3r = "BOX: ä¸Šä½ãƒ»ãƒœãƒ¼ãƒŠã‚¹ (æ··æˆ¦/åˆ¥ç·š)"
        pattern_2s = "ã‚¹ã‚¸é•ã„ãƒ»ãƒãƒ«ãƒ"
        
        struct_3r.append({'type': '3rencpu_box', 'cars': box_cars})
        struct_2s.append({'type': '2shatan_multi', 'c1': m1, 'c2_list': [m2, m3]})

    # 2. Suji-Fix Mode (Ironclad Suji)
    elif is_suji_fix:
        race_type = 'é‰„æ¿ã‚¹ã‚¸'
        race_type_emoji = 'ğŸ°'
        
        # Strict Check (Hybrid) - RESTORING ORIGINAL LOGIC
        is_strict = False
        
        # Original Logic Reconstruction:
        # S-Class: Short Bank & 3-line & Gap<=5
        if 'Sç´š' in race_class:
             short_banks = ["æ¾æˆ¸", "å°ç”°åŸ", "ä¼Šæ±", "å¯Œå±±", "å¥ˆè‰¯", "é˜²åºœ", "å‰æ©‹"] 
             if place_name in short_banks and n_lines == 3 and gap <= 5.0:
                 is_strict = True
                 
        # A-Class: 3-line & MaxLine>=3 & Gap<=10 (Lead) OR ShortBank & 2-line & Gap<=10 (Fix)
        elif 'Aç´š' in race_class:
             short_banks = ["æ¾æˆ¸", "å°ç”°åŸ", "ä¼Šæ±", "å¯Œå±±", "å¥ˆè‰¯", "é˜²åºœ", "å‰æ©‹"] 
             # Fix pattern
             if place_name in short_banks and n_lines == 2 and gap <= 10.0:
                 is_strict = True
             # Lead pattern
             elif n_lines == 3 and max_line_len >= 3 and gap <= 10.0:
                 is_strict = True
                 
        # Challenge: Gap<=10 (Generally strong)
        elif 'ãƒãƒ£ãƒ¬ãƒ³ã‚¸' in race_class:
             if gap <= 10.0:
                 is_strict = True
                 
        suffix = " ğŸ”¥(æ¿€ç†±)" if is_strict else ""
        
        race_type_reason = f"ã‚¹ã‚¸æ±ºç€æ¿ƒåš{suffix} (1ç‚¹å‹è² æ¨å¥¨)"
        
        # Target: m1 -> Partner (Need partner car num)
        # Use m2 as proxy if we can't identify partner?
        # Ideally identify partner correctly.
        # Assuming m2 IS partner if Suji logic holds? Not always.
        # Fallback to m2 if partner not found.
        # Try finding partner again
        m1_line = top_main.iloc[0].get('temp_line_id', -1)
        partners = df[(df.get('temp_line_id') == m1_line) & (df['è»Šç•ª'] != m1)]
        if not partners.empty:
            partner = partners.sort_values(score_col, ascending=False).iloc[0]
            p_car = int(partner['è»Šç•ª'])
        else:
            p_car = m2 # Fallback
            p_score = s2
            
        # Reverse (Ura) Check
        # If Gap is small OR Bank is Long (Standard), Partner might beat Head.
        is_reverse_needed = False
        p_score = partner[score_col] if not partners.empty else s2
        
        real_gap = s1 - p_score
        
        # Conditions: 
        # 1. Close Match: Gap < 4.0 (Very dangerous)
        # 2. Long Bank: Gap < 8.0 AND Bank >= 400 (Sashi favor)
        is_long_bank = (place_name not in ["æ¾æˆ¸", "å°ç”°åŸ", "ä¼Šæ±", "å¯Œå±±", "å¥ˆè‰¯", "é˜²åºœ", "å‰æ©‹"])
        
        if real_gap < 4.0:
            is_reverse_needed = True
        elif is_long_bank and real_gap < 8.0:
            is_reverse_needed = True
            
        if is_reverse_needed:
             suffix += " ğŸ”„(å·®ã—è­¦æˆ’)"
             race_type_reason += " â€»æŠ˜ã‚Šè¿”ã—æ¨å¥¨"
        l2_3r = [p_car]
        
        # Fix 3rd place candidates to ensure 4 distinct points
        cands = [x for x in [m2, m3, m4, b1] if x not in [m1, p_car]]
        # Fill with m5/others if needed?
        # Logic usually has access to m1..m4.
        # Let's ensure we have distinct.
        unique_3rd = sorted(list(set(cands)), key=lambda x: cands.index(x))
        # If less than 2 candidates (total 2 pts), user complained "recommended 4 but 2".
        # We need more candidates preferably.
        # Try adding top scorers until we have 4.
        for i in range(10):
            c_cand = get_car(i)
            if c_cand not in [m1, p_car] and c_cand not in unique_3rd:
                unique_3rd.append(c_cand)
            if len(unique_3rd) >= 4: break
            
        l3_3r = unique_3rd[:4]
        
        if is_reverse_needed:
            tickets_2s.append(f"2è»Šå˜: {m1} â†” {p_car} (æŠ˜ã‚Šè¿”ã—)")
            tickets_3r.append(f"3é€£å˜: {p_car} â†’ {m1} â†’ {','.join(map(str, l3_3r))}")
        else:
            tickets_2s.append(f"2è»Šå˜: {m1} â†’ {p_car} (1ç‚¹)")
        tickets_3r.append(f"3é€£å˜: {m1} â†’ {p_car} â†’ {','.join(map(str, l3_3r))}")
        
        pattern_3r = "æœ¬å‘½ â†’ ç•ªæ‰‹ â†’ 3ç•ªæ‰‹/åˆ¥ç·š"
        pattern_2s = "æœ¬å‘½ â†’ ç•ªæ‰‹ (1ç‚¹)"
        
        struct_3r.append({'type': '3rentan_form', '1st': [m1], '2nd': [p_car], '3rd': l3_3r})
        struct_2s.append({'type': '2shatan', '1st': [m1], '2nd': [p_car]})

    # 3. Suji-Lead Mode (High Prob)
    elif is_suji_lead:
        race_type = 'æœ‰åŠ›ã‚¹ã‚¸'
        race_type_emoji = 'âš”ï¸'
        race_type_reason = "ã‚¹ã‚¸æ±ºç€æœ‰åŠ› (çµã‚Šè¾¼ã¿)"
        
        # Similar to Fix but slightly wider
        m1_line = top_main.iloc[0].get('temp_line_id', -1)
        partners = df[(df.get('temp_line_id') == m1_line) & (df['è»Šç•ª'] != m1)]
        p_car = int(partners.sort_values(score_col, ascending=False).iloc[0]['è»Šç•ª']) if not partners.empty else m2

        # 2sha: m1 -> p_car, m2 (if m2 is diff line)
        tickets_2s.append(f"2è»Šå˜: {m1} â†’ {p_car}, {m2}")
        
        # Reverse Check for Lead (same logic)
        partners_lead = df[(df.get('temp_line_id') == m1_line) & (df['è»Šç•ª'] != m1)]
        if not partners_lead.empty:
             pl_score = partners_lead.sort_values(score_col, ascending=False).iloc[0][score_col]
             gap_l = s1 - pl_score
             is_long_bank = (place_name not in ["æ¾æˆ¸", "å°ç”°åŸ", "ä¼Šæ±", "å¯Œå±±", "å¥ˆè‰¯", "é˜²åºœ", "å‰æ©‹"])
             if gap_l < 4.0 or (is_long_bank and gap_l < 8.0):
                 tickets_2s.append(f"2è»Šå˜: {p_car} â†’ {m1} (æŠ˜ã‚Šè¿”ã—)")
                 tickets_3r.append(f"3é€£å˜: {p_car} â†’ {m1} â†’ {m1}, {m2}, {m3}, {b1}")
                 race_type_reason += " (æŠ˜ã‚Šè¿”ã—æŠ¼ã•ãˆ)"

        tickets_3r.append(f"3é€£å˜: {m1} â†’ {p_car}, {m2} â†’ {p_car}, {m2}, {m3}, {b1}")
        
        pattern_3r = "ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ (æœ¬å‘½-ç•ªæ‰‹/å¯¾æŠ—)"
        pattern_2s = "æœ¬å‘½ â†’ ç•ªæ‰‹/å¯¾æŠ—"
        
        struct_3r.append({'type': '3rentan_form', '1st': [m1], '2nd': [p_car, m2], '3rd': list(set([p_car, m2, m3, b1]))})
        struct_2s.append({'type': '2shatan', '1st': [m1], '2nd': [p_car, m2]})
        
    # 4. Standard Fallback (Original Logic)
    else:
        # Fallback to score gap logic
        if diff_1_2 > 5.0:
             race_type = 'é‰„æ¿(Gap)'
             l1_3r, l2_3r, l3_3r = [m1, m2], [m2, m3, b1], [m2, m3, m4, b1]
             pattern_3r = "A: å®ŸåŠ›1,2 - å®ŸåŠ›2,3,B1"
        elif diff_1_2 < 1.0:
             race_type = 'æ··æˆ¦(Gap)'
             l1_3r, l2_3r, l3_3r = [m1], [m2, m3], [m2, m3, m4]
             pattern_3r = "C: å®ŸåŠ›1 - å®ŸåŠ›2,3"
        else:
             race_type = 'æ¨™æº–'
             l1_3r, l2_3r, l3_3r = [b1], [m1, m2, m3], [m1, m2, m3, m4]
             pattern_3r = "D: B1è»¸"

        tickets_3r.append(f"3é€£å˜: (ãƒ‘ã‚¿ãƒ¼ãƒ³{race_type})")
        tickets_2s.append(f"2è»Šå˜: {m1} â†” {m2}, {b1}")
        
        struct_2s.append({'type': '2shatan_fold', 'c1': m1, 'c2': m2})


    return {
        "type": race_type,
        "title": f"{race_type_emoji} {race_type}",
        "reason": race_type_reason,
        "tickets_3rentan": tickets_3r,
        "tickets_2shatan": tickets_2s,
        "structured_bets_3rentan": struct_3r,
        "structured_bets_2shatan": struct_2s,
        "main_1_car": m1,
        "bonus_1_car": b1,
        "bonus_value": b1_bonus,
        # Keep pattern strings for display/history
        "pattern_3rentan": pattern_3r,
        "pattern_2shatan": pattern_2s,
        "strategy_type": "hybrid"
    }

# ==========================================
# 3. Gemini Commentary Logic
# ==========================================

def generate_ai_commentary(df, meta, lines_info, metrics, strategy_res=None, api_key=None):
    """
    Generate professional race commentary using Gemini API.
    """
    if not api_key:
         return "â„¹ï¸ Gemini APIã‚­ãƒ¼ã‚’è¨­å®šã™ã‚‹ã¨ã€ã“ã“ã«æœ¬æ°—ã®AIè§£èª¬ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚"
    
    genai.configure(api_key=api_key)

    # 1. Context Construction
    place = meta.get('place', 'ä¸æ˜')
    race_num = meta.get('race_num', '1')
    date = meta.get('date', 'ä¸æ˜')
    
    # Top Players
    try:
         df['score_val'] = pd.to_numeric(df['ç«¶èµ°å¾—ç‚¹'], errors='coerce').fillna(0)
         df_sorted = df.sort_values('score_val', ascending=False)
         top3 = df_sorted.head(3)[['è»Šç•ª', 'é¸æ‰‹å', 'åºœçœŒ', 'æœŸåˆ¥', 'ç«¶èµ°å¾—ç‚¹', 'è„šè³ª']].to_dict('records')
    except:
         top3 = []

    # Full Player List for Context
    player_list_str = ""
    try:
         all_players = []
         df_sorted_car = df.sort_values('è»Šç•ª')
         for _, row in df_sorted_car.iterrows():
             c_num = row.get('è»Šç•ª', '?')
             name = row.get('é¸æ‰‹å', 'ä¸æ˜')
             pref = row.get('åºœçœŒ', '')
             cls = row.get('ç´šç­', '')
             score = row.get('ç«¶èµ°å¾—ç‚¹', 0)
             try: score = float(score)
             except: score = 0
             tactic = row.get('è„šè³ª', '')
             
             # Jimoto Check
             is_local = row.get('is_jimoto') or row.get('åœ°å…ƒ')
             local_tag = " [åœ°å…ƒ]" if is_local else ""
             
             all_players.append(f"{c_num}: {name} ({pref}/{cls}, {score:.2f}, {tactic}){local_tag}")
         player_list_str = "\\n".join(all_players)
    except:
         player_list_str = "æƒ…å ±ãªã—"

    # Construct Prompt
    prompt = f"""
ã‚ãªãŸã¯ç«¶è¼ªæ­´30å¹´ã®ä¼èª¬ã®ã‚¹ãƒãƒ¼ãƒ„è¨˜è€…ã§ã™ã€‚
é•·å¹´ã®çµŒé¨“ã¨ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’èåˆã•ã›ã€èª­è€…ã®å¿ƒã‚’æºã•ã¶ã‚‹ã€Œæœ¬æ°—ã®ãƒ¬ãƒ¼ã‚¹è§£èª¬ã€ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
å˜ãªã‚‹äºˆæƒ³ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ¬ãƒ¼ã‚¹ã®ã€Œç‰©èªï¼ˆãƒ‰ãƒ©ãƒï¼‰ã€ã‚’æã„ã¦ãã ã•ã„ã€‚

ã€ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã€‘
{date} {place}ç«¶è¼ª {race_num}ãƒ¬ãƒ¼ã‚¹

ã€ãƒ©ã‚¤ãƒ³æ§‹æˆã€‘
{lines_info}

ã€æœ‰åŠ›é¸æ‰‹ (å¾—ç‚¹ä¸Šä½)ã€‘
{top3}

ã€AIåˆ†æãƒ‡ãƒ¼ã‚¿ã€‘
- é‰„æ¿åº¦åˆ¤å®š: {metrics.get('signals', 'ãªã—')}
- 1ä½-2ä½å¾—ç‚¹å·®: {metrics.get('score_diff_1_2', 0):.2f} (å¤§ãã„ã»ã©æœ¬å‘½ä¿¡é ¼åº¦é«˜)
- ãƒ©ã‚¤ãƒ³å…ˆé ­ã®å¼·ã•: {metrics.get('line_strength_head', 'ä¸æ˜')}

ã€å…¨å‡ºå ´é¸æ‰‹ãƒªã‚¹ãƒˆã€‘
{player_list_str}

ã€AIãƒ­ã‚¸ãƒƒã‚¯æ¨è–¦ã®è²·ã„ç›®ï¼ˆå‚è€ƒï¼‰ã€‘
{strategy_res.get('tickets', []) if strategy_res else 'ãªã—'}
åˆ¤å®šã‚¿ã‚¤ãƒ—: {strategy_res.get('type', 'æ¨™æº–') if strategy_res else 'æ¨™æº–'}

ã€åŸ·ç­†ã®ãƒã‚¤ãƒ³ãƒˆã€‘
1. **å±•é–‹ã®ãƒ‰ãƒ©ãƒ**: ã€Œå·ç ²ãŒé³´ã‚‹ã¨...ã€ã‹ã‚‰å§‹ã‚ã€åˆæ‰‹ã®ä¸¦ã³ã€ã‚¸ãƒ£ãƒ³å‰å¾Œã®é§†ã‘å¼•ãã€æœ€çµ‚ãƒãƒƒã‚¯ã§ã®æ”»é˜²ã‚’ã€ã¾ã‚‹ã§è¦‹ã¦ããŸã‹ã®ã‚ˆã†ã«è‡¨å ´æ„ŸãŸã£ã·ã‚Šã«æå†™ã—ã¦ãã ã•ã„ã€‚ç‰¹ã«ã€Œé€ƒã’ã®ä¸»å°æ¨©äº‰ã„ã€ã‚„ã€Œç•ªæ‰‹ã®ä»•äº‹ï¼ˆãƒ–ãƒ­ãƒƒã‚¯ï¼‰ã€ã€ã€Œæ²ã‚Šã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã€ãªã©ã«è§¦ã‚Œã¦ãã ã•ã„ã€‚
2. **é¸æ‰‹ã¸ã®è¦–ç‚¹**: é¸æ‰‹ã®å¿ƒç†çŠ¶æ…‹ã‚„ã€ãƒ©ã‚¤ãƒ³ã®çµ†ã€åœ°å…ƒé¸æ‰‹ã®æ„åœ°ãªã©ã‚’æƒ³åƒã—ã€æ„Ÿæƒ…ç§»å…¥ã§ãã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç››ã‚Šè¾¼ã‚“ã§ãã ã•ã„ã€‚
3. **çµè«–ã¨è²·ã„ç›®**: ã€Œã‚ºãƒãƒªã€ç§ã®æœ¬å‘½ã¯...ã€ã¨åˆ‡ã‚Šå‡ºã—ã€ãªãœãã®é¸æ‰‹ãªã®ã‹ã‚’ç†±ãèªã£ã¦ãã ã•ã„ã€‚ç©´ç‹™ã„ãªã‚‰ã€Œå¤§æ³¢ä¹±ã®äºˆæ„Ÿ...ã€ã€Œä¸€ç™ºã‚ã‚‹ãªã‚‰...ã€ã¨æœŸå¾…æ„Ÿã‚’ç…½ã£ã¦ãã ã•ã„ã€‚

ã€å£èª¿ã¨ã‚¹ã‚¿ã‚¤ãƒ«ã€‘
- ã€Œã€œã ã‚ã†ã€ã€Œã€œã«æœŸå¾…ã—ãŸã„ã€ã€Œã€œãŒæ¿ƒåšã ã€ã€Œã€œã“ã‚Œãç«¶è¼ªã ã€ã¨ã„ã£ãŸã€è‡ªä¿¡ã¨æ„›ã«æº€ã¡ãŸã‚¹ãƒãƒ¼ãƒ„ç´™ã®ãƒ™ãƒ†ãƒ©ãƒ³è¨˜è€…é¢¨ã®å£èª¿ã€‚
- èª­è€…ã‚’ã‚°ã‚¤ã‚°ã‚¤å¼•ãè¾¼ã‚€ã€ãƒªã‚ºãƒŸã‚«ãƒ«ã§ç†±ã„æ–‡ä½“ã€‚

ã€æ§‹æˆï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ï¼‰ã€‘
### ğŸš´ å±•é–‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
### ğŸ” è¨˜è€…ãŒè¦‹æŠœã„ãŸå‹è² ã®åˆ†ã‹ã‚Œç›®
### ğŸ¯ æ¸¾èº«ã®æœ€çµ‚çµè«–
ã€Œã‚ºãƒãƒªã€ç§ã®æœ¬å‘½ã¯...ã€ã¨åˆ‡ã‚Šå‡ºã—ã€ãªãœãã®é¸æ‰‹ãªã®ã‹ã‚’ç†±ãèªã£ã¦ãã ã•ã„ã€‚
ç©´ç‹™ã„ãªã‚‰ã€Œå¤§æ³¢ä¹±ã®äºˆæ„Ÿ...ã€ã€Œä¸€ç™ºã‚ã‚‹ãªã‚‰...ã€ã¨æœŸå¾…æ„Ÿã‚’ç…½ã£ã¦ãã ã•ã„ã€‚

### ã€AIäºˆæƒ³è²·ã„ç›®ã€‘
æœ€å¾Œã«ã€å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§æ¨å¥¨è²·ã„ç›®ã‚’åˆ—æŒ™ã—ã¦ãã ã•ã„ã€‚
ï¼ˆä¾‹ï¼‰
ãƒ»3é€£å˜ æœ¬ç·š: 1-2-3 (1ç‚¹)
ãƒ»3é€£å˜ æŠ‘ãˆ: 1-2-4, 1-3-2 (2ç‚¹)
ãƒ»2è»Šå˜: 1=2 (è£è¡¨)
"""

    # Call API
    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"è§£èª¬ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"

# ==========================================
# 4. Data Loading Logic
# ==========================================

def load_and_process_data(db_path=db_utils.DB_PATH, target_years=None):
    if not os.path.exists(db_path):
        return pd.DataFrame()

    conn = sqlite3.connect(db_path)
    
    target_cols = [
        "race_id", "ç«¶è¼ªå ´", "æ—¥ä»˜", "class_code", "ç´šç­",
        "line_length", "line_pos", "is_longest_line", 
        "fav_tactic", "line_strength_head", "line_strength_second", 
        "is_jimoto", "score_rank", "ç€é †_val", "race_size", 
        "odds_win_sim", "odds_wide_sim", 
        "æ±ºã¾ã‚Šæ‰‹", "ãƒ¬ãƒ¼ã‚¹ã®ç¨®é¡", "ã‚°ãƒ¬ãƒ¼ãƒ‰", 
        "é€ƒ", "æ²", "å·®", "ãƒ",
        "æ ç•ª", "è»Šç•ª", "é¸æ‰‹å", "åºœçœŒ", "B", "S",
        "ç«¶èµ°å¾—ç‚¹", "å‹ ç‡", "2é€£ å¯¾ç‡", "3é€£ å¯¾ç‡",
        "is_top_nige", "is_top_makuri", "is_top_sashi",
        "dividend_2shatan", "dividend_3rentan",
        "ãƒ©ã‚¤ãƒ³", "å¹´"
    ]
    
    # Valid columns only
    try:
        res = conn.execute("PRAGMA table_info(race_result)").fetchall()
        db_cols = [r[1] for r in res]
        select_cols = [c for c in target_cols if c in db_cols]
        cols_str = ", ".join([f'"{c}"' for c in select_cols])
        
        where_clause = ""
        params = []
        if target_years:
            placeholders = ','.join(['?'] * len(target_years))
            if "å¹´" in db_cols:
                where_clause = f" WHERE å¹´ IN ({placeholders})"
                params = list(target_years) # Ensure list
        
        query = f"SELECT {cols_str} FROM race_result{where_clause}"
        # print(f"DEBUG SQL: {query} / Params: {params}")
        df = pd.read_sql(query, conn, params=params)
             
    except Exception as e:
        # print(f"Load Error: {e}")
        return pd.DataFrame()
    finally:
        conn.close()
        return pd.DataFrame()
        
    conn.close()
    
    # Memory Optimization: Downcast
    int_cols = ["line_length", "line_pos", "is_longest_line", "is_jimoto", "score_rank", "ç€é †_val", "race_size", "æ ç•ª", "è»Šç•ª", "å¹´"]
    for c in int_cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0).astype('int8')

    count_cols = ["B", "S", "H", "é€ƒ", "æ²", "å·®", "ãƒ"]
    for c in count_cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0).astype('float32')
            
    float_cols = ["odds_win_sim", "odds_wide_sim"]
    for c in float_cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0).astype('float32')

    if 'is_line_onetwo' not in df.columns:
         df['is_line_onetwo'] = 0

    # 1. Date Conversion
    df['date_dt'] = pd.to_datetime(df['æ—¥ä»˜'], format='%Yå¹´%mæœˆ%dæ—¥', errors='coerce')
    if 'å¹´' not in df.columns:
        df['year'] = df['date_dt'].dt.year.fillna(0).astype('int16')
    else:
        df['year'] = df['å¹´'].astype('int16')
    
    # 2. Class Calculation
    class_map = {'S': 'Sç´š', 'A': 'Aç´š', 'C': 'ãƒãƒ£ãƒ¬ãƒ³ã‚¸', 'L': 'ã‚¬ãƒ¼ãƒ«ã‚º'}
    
    if 'class_code' in df.columns:
        df['ã‚¯ãƒ©ã‚¹'] = df['class_code'].map(class_map).fillna('Aç´š').astype('category')
    elif 'ç´šç­' in df.columns:
        df['ã‚¯ãƒ©ã‚¹'] = df['ç´šç­'].apply(db_utils.classify_grade).astype('category')
    else:
        df['ã‚¯ãƒ©ã‚¹'] = 'Aç´š'

    # 3. Max Tactic Logic
    tactic_map = {'é€ƒ': 'nige', 'æ²': 'makuri', 'å·®': 'sashi', 'ãƒ': 'mark'}
    
    for jp_key, en_key in tactic_map.items():
        db_flag_col = f"is_top_{en_key}"
        app_flag_col = f"is_max_{en_key}"
        
        if db_flag_col in df.columns:
            df[app_flag_col] = df[db_flag_col].astype(bool)
        elif jp_key in df.columns:
            col_val = pd.to_numeric(df[jp_key], errors='coerce').fillna(0)
            max_val = df.groupby('race_id')[jp_key].transform('max')
            df[app_flag_col] = ((col_val == max_val) & (max_val > 0))
        else:
            df[app_flag_col] = False

    # 4. Column Renaming
    db_rename_map = {
        'å‹ ç‡': 'å‹ç‡', '2é€£ å¯¾ç‡': '2é€£å¯¾ç‡', '3é€£ å¯¾ç‡': '3é€£å¯¾ç‡'
    }
    df.rename(columns=db_rename_map, inplace=True)

    column_mapping = {
        'line_length': 'ãƒ©ã‚¤ãƒ³é•·',
        'line_pos': 'ãƒã‚¸ã‚·ãƒ§ãƒ³',
        'score_rank': 'å¾—ç‚¹é †ä½',
        'is_longest_line': 'æœ€é•·ãƒ©ã‚¤ãƒ³',
        'fav_tactic': 'å¾—æ„æˆ¦æ³•',
        'line_strength_head': 'å…ˆé ­å¼·åº¦',
        'line_strength_second': 'ç•ªæ‰‹å¼·åº¦',
        'is_jimoto': 'åœ°å…ƒ',
        'is_line_onetwo': 'ãƒ©ã‚¤ãƒ³ãƒ¯ãƒ³ãƒ„ãƒ¼',
        'odds_win_sim': 'ç–‘ä¼¼å˜å‹é…å½“',
        'odds_wide_sim': 'ç–‘ä¼¼ãƒ¯ã‚¤ãƒ‰é…å½“',
        'race_size': 'å‡ºèµ°é ­æ•°',
        'year': 'å¹´', 
        'dividend_2shatan': '2è»Šå˜',
        'dividend_3rentan': '3é€£å˜',
    }
    
    actual_mapping = {k: v for k, v in column_mapping.items() if k in df.columns}
    df = df.rename(columns=actual_mapping)

    return df

# ==========================================
# 5. Helper Funcs
# ==========================================

def get_readable_condition(name, threshold, relation):
    """Human readable condition string (Natural Japanese)"""
    if name.startswith('æˆ¦æ³•:') or name.startswith('is_'):
        val_name = name.replace('is_', '').replace('val', '').replace('æˆ¦æ³•:', '')
        if relation == ">":
            if "nige" in val_name: return "ğŸš€ é€ƒã’é¸æ‰‹"
            if "makuri" in val_name: return "ğŸŒ€ æ²ã‚Šé¸æ‰‹"
            if "sashi" in val_name: return "âš¡ å·®ã—é¸æ‰‹"
            if "mark" in val_name: return "ğŸ›¡ï¸ ãƒãƒ¼ã‚¯é¸æ‰‹"
            if "jimoto" in val_name: return "ğŸ  åœ°å…ƒé¸æ‰‹"
            if "longest_line" in val_name: return "ğŸ›¤ï¸ æœ€é•·ãƒ©ã‚¤ãƒ³"
            if "line_onetwo" in val_name: return "ğŸ¤ ãƒ©ã‚¤ãƒ³ãƒ¯ãƒ³ãƒ„ãƒ¼"
            return f"ã€{val_name}ã€‘" 
        else:
            return f"ã€é{val_name}ã€‘"
    
    if 'å¼·åº¦' in name:
        val_name = ""
        if relation == ">":
            if threshold < 1: val_name = "å¼±ä»¥ä¸Š"
            elif threshold < 2: val_name = "ä¸­ä»¥ä¸Š"
            elif threshold < 3: val_name = "å¼·ã®ã¿"
            op = ""
        else: 
            if threshold < 1: val_name = "ãªã—"
            elif threshold < 2: val_name = "å¼±ä»¥ä¸‹"
            elif threshold < 3: val_name = "ä¸­ä»¥ä¸‹"
            op = ""
        return f"{name.replace('_val','')} {op}{val_name}"
        
    if 'é †ä½' in name:
        if relation == "<=":
            return f"ğŸ… {name} {int(threshold)}ä½ä»¥å†…"
        else:
            return f"{name} {int(threshold)}ä½ã‚ˆã‚Šä¸‹"

    if 'æ ç•ª' in name:
        if relation == "<=":
            return f"ğŸ {name} {int(threshold)}æ ä»¥å†…"
        else:
            return f"{name} {int(threshold)}æ ã‚ˆã‚Šå¤–"

    return f"{name} {relation} {threshold}"

def check_rule_match(row, rule_conditions):
    for feat, thresh, rel in rule_conditions:
        val = row.get(feat, 0)
        if rel == ">=":
            if not (val >= thresh): return False
        elif rel == ">":
            if not (val > thresh): return False
        elif rel == "<=":
            if not (val <= thresh): return False
        elif rel == "<":
            if not (val < thresh): return False
        elif rel == "==":
            if not (val == thresh): return False
        elif rel == "!=":
            if not (val != thresh): return False
    return True

# ==========================================
# 6. Scoring Logic (Missing Function)
# ==========================================

def apply_v3_logic(df):
    """
    Logic V3: B-Top, Tactic Dominance, and Nige Conflict.
    Based on Jan 2026 Verification.
    """
    df = df.copy()
    if df.empty: return df
    
    # --- 1. Feature Engineering ---
    # Convert cols to numeric
    for col in ['B', 'é€ƒ', 'æ²', 'å·®']:
        if col in df.columns:
            df[f'{col}_val'] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        else:
            df[f'{col}_val'] = 0.0

    # A. B-Top
    max_b = df['B_val'].max()
    df['is_b_top'] = (df['B_val'] == max_b) & (max_b > 0)
    
    # B. Tactic Dominance (Count >= 5 AND (Diff >= 5 OR Ratio >= 3.0))
    def check_dominance(col_name):
        vals = df[col_name].sort_values(ascending=False).values
        if len(vals) < 2: return [False] * len(df)
        
        top_val = vals[0]
        sec_val = vals[1]
        
        is_dom = False
        if top_val >= 5:
            if (top_val >= sec_val + 5) or (sec_val > 0 and top_val / sec_val >= 3.0) or (sec_val == 0 and top_val >= 5):
                is_dom = True
        
        # Return mask
        return (df[col_name] == top_val) & (top_val > 0) & is_dom

    df['is_dom_nige'] = check_dominance('é€ƒ_val')
    df['is_dom_makuri'] = check_dominance('æ²_val')
    df['is_dom_sashi'] = check_dominance('å·®_val')
    
    # C. Nige Conflict Level
    # Count players with Nige >= 3
    nige_players = df[df['é€ƒ_val'] >= 3]
    nige_count = len(nige_players)
    
    # --- 2. Scoring & Tagging ---
    
    # Bank Specs
    place_name = df['ç«¶è¼ªå ´'].iloc[0] if 'ç«¶è¼ªå ´' in df.columns else ""
    bank_specs = db_utils.VELODROME_SPECS.get(place_name, (400, 30, 400)) # Default
    str_len = bank_specs[0]
    
    is_short_bank = (str_len < 50.0) # Short straight
    is_long_bank = (str_len > 58.0)  # Long straight
    
    # Loop for scoring
    for idx, row in df.iterrows():
        score_add = 0.0
        tag_add = ""
        
        # --- B-Top Logic ---
        if row['is_b_top']:
            if is_short_bank:
                score_add += 3.0 # Strong on short
                tag_add += " [B-Top:çŸ­(â˜…)]"
            elif is_long_bank:
                score_add += 1.0 # Weaker on long
                tag_add += " [B-Top:é•·]"
            else:
                score_add += 2.0
                tag_add += " [B-Top]"
                
        # --- Dominance Logic ---
        if row['is_dom_makuri']:
            score_add += 6.0 # SS Grade confidence
            tag_add += " [åœ§å€’çš„æ²ã‚Š(SS)]"
            
        elif row['is_dom_nige']:
            if is_short_bank:
                score_add += 5.0 # S Grade
                tag_add += " [åœ§å€’çš„é€ƒã’:çŸ­(S)]"
            else:
                score_add += 3.0
                tag_add += " [åœ§å€’çš„é€ƒã’]"
                
        elif row['is_dom_sashi']:
            # Sashi dominance is for 2nd/3rd place stability, not 1st.
            # Small score boost, but mainly for stability logic (handled in betting gen?)
            # Just add score to ensure they remain in high rank.
            score_add += 2.0
            tag_add += " [åœ§å€’çš„å·®(é€£è»¸)]"

        # --- Nige Conflict Logic ---
        # If Nige War (>= 3 Nige), Penalty for Sashi (Prediction: Nige wins)
        if nige_count >= 3:
            # Check if this player is Sashi type (and NOT Dom Sashi)
            if 'å·®' in str(row.get('è„šè³ª', '')) and not row['is_dom_sashi']:
                 # Small penalty to lower their 1st place rank
                 score_add -= 1.0
                 tag_add += " [æ¿€æˆ¦:å·®å¼•]"
            
            # Boost Strongest Nige?
            if row['is_dom_nige'] or (row['é€ƒ_val'] == df['é€ƒ_val'].max() and row['é€ƒ_val'] >= 5):
                 score_add += 2.0
                 tag_add += " [æ¿€æˆ¦:é€ƒæœ‰åˆ©]"

        df.loc[idx, 'ai_score'] += score_add
        df.loc[idx, 'ai_tag'] += tag_add
        
        # Save V3 Feature Flags for Betting Strategy use
        df.loc[idx, 'v3_nige_count'] = nige_count
        
    return df

def calculate_ai_score(df):
    # 1. Classic Logic (Foundation)
    df = calculate_classic_score(df)
    
    # 2. Logic V3 (Context-Aware / Dominance)
    df = apply_v3_logic(df)
    
    return df

def calculate_ai_score_OLD_IGNORED(df):
    """
    Calculate Basic AI Score based on Racing Score and simple bonuses.
    """
    df = df.copy()
    
    # 1. Base Score from Racing Score
    # Handle non-numeric
    if 'ç«¶èµ°å¾—ç‚¹' not in df.columns:
        df['ai_score'] = 0.0
        return df
        
    df['base_score'] = pd.to_numeric(df['ç«¶èµ°å¾—ç‚¹'], errors='coerce').fillna(80.0)
    df['ai_score'] = df['base_score']
    df['ai_tag'] = ""
    
    # 2. Local Bonus
    # Check if "åœ°å…ƒ" column exists (created by db_utils)
    if 'åœ°å…ƒ' in df.columns:
        # Check if 1 or True
            mask = (df['åœ°å…ƒ'] == 1) | (df['åœ°å…ƒ'] == True)
            df.loc[mask, 'ai_score'] += 3.0
            df.loc[mask, 'ai_tag'] += " [åœ°å…ƒ]"
             
    # 3. Tactic Bonus (Nige/Makuri often strong)
    if 'è„šè³ª' in df.columns:
        # é€ƒ
        mask_nige = df['è„šè³ª'].astype(str).str.contains('é€ƒ')
        df.loc[mask_nige, 'ai_score'] += 2.0
        # æ²
        mask_makuri = df['è„šè³ª'].astype(str).str.contains('æ²')
        df.loc[mask_makuri, 'ai_score'] += 2.0
        
    # 4. Line Bonus (Naive)
    if 'ãƒ©ã‚¤ãƒ³' in df.columns:
        # If line length >= 3
        df['line_len_temp'] = df['ãƒ©ã‚¤ãƒ³'].astype(str).str.len()
        mask_long = df['line_len_temp'] >= 3
        df.loc[mask_long, 'ai_score'] += 1.0
        
    # 5. Bank Specs Bonus (Straight/Cant) - User Logic
    if 'ç«¶è¼ªå ´' in df.columns:
        place = df['ç«¶è¼ªå ´'].iloc[0]
        # (Straight, Cant, Length)
        specs = db_utils.VELODROME_SPECS.get(place) 
        
        if specs:
            str_m, cant_deg, _ = specs
            
            # A. Straight Logic
            if str_m < 50.0:
                # Short -> Nige
                if 'è„šè³ª' in df.columns:
                    mask = df['è„šè³ª'].astype(str).str.contains('é€ƒ')
                    df.loc[mask, 'ai_score'] += 2.0
                    df.loc[mask, 'ai_tag'] += " [çŸ­ç›´ç·š:é€ƒ]"
            elif str_m > 58.0:
                # Long -> Makuri/Sashi
                if 'è„šè³ª' in df.columns:
                    mask = df['è„šè³ª'].astype(str).str.contains('æ²|å·®')
                    df.loc[mask, 'ai_score'] += 2.0
                    df.loc[mask, 'ai_tag'] += " [é•·ç›´ç·š:æ²å·®]"

            # B. Cant Logic
            if cant_deg < 30.0:
                # Loose -> Nige (Curve slow)
                if 'è„šè³ª' in df.columns:
                    mask = df['è„šè³ª'].astype(str).str.contains('é€ƒ')
                    df.loc[mask, 'ai_score'] += 2.0
                    df.loc[mask, 'ai_tag'] += " [ç·©å‚¾æ–œ:é€ƒ]"
            elif cant_deg > 33.0:
                # Tight -> Makuri (Curve fast)
                if 'è„šè³ª' in df.columns:
                    mask = df['è„šè³ª'].astype(str).str.contains('æ²')
                    df.loc[mask, 'ai_score'] += 2.0
                    df.loc[mask, 'ai_tag'] += " [æ€¥å‚¾æ–œ:æ²]"

    # 6. Specialist Bonus (Top Tactic)
    # MOVED TO ADVANCED LOGIC to prevent duplication and double scoring.
    # Checks for is_top_nige etc are now handled strictly in apply_advanced_logic via Logic 9.


    # 7. Class-Specific Lift Bonus (User Logic v2)
    # Detect Race Class
    race_class = "A" # Default
    if 'ç´šç­' in df.columns:
        classes = df['ç´šç­'].astype(str).unique()
        has_s = any('S' in c for c in classes)
        has_a3 = any('A3' in c for c in classes)
        
        if has_s: race_class = "S"
        elif has_a3: race_class = "A3" # Challenge
        
    # Determine Top Scorer (Rank 1)
    if not df.empty and 'base_score' in df.columns:
        # Find index of max base_score
        top_scorer_idx = df['base_score'].idxmax()
        
        # Determine Bonus Amount based on Class & Tactic Leadership
        lift_bonus = 0.0
        lift_reason = ""
        
        # Check Tactic Leadership of the Top Scorer
        # Note: A player can be top nige AND top makuri? Yes.
        row = df.loc[top_scorer_idx]
        is_top_nige = row.get('is_top_nige', 0) == 1
        is_top_makuri = row.get('is_top_makuri', 0) == 1
        is_top_sashi = row.get('is_top_sashi', 0) == 1
        
        if race_class == "A3": # Challenge
            if is_top_nige: 
                lift_bonus = max(lift_bonus, 4.0)
                lift_reason = "[A3å›å¸°:é€ƒ]"
            if is_top_makuri: 
                lift_bonus = max(lift_bonus, 4.0)
                lift_reason = "[A3å›å¸°:æ²]" if not lift_reason else lift_reason # prioritize nige label or keep both?
                
        elif race_class == "A": # A-Class
            if is_top_nige:
                lift_bonus = max(lift_bonus, 2.5)
                lift_reason = "[Aç´šå›å¸°:é€ƒ]"
            if is_top_makuri:
                lift_bonus = max(lift_bonus, 2.0)
                if lift_bonus == 2.0: lift_reason = "[Aç´šå›å¸°:æ²]" # Only overwrite if higher/equal? 2.5 > 2.0.
                
        elif race_class == "S": # S-Class
            if is_top_nige:
                lift_bonus = max(lift_bonus, 1.5)
                lift_reason = "[Sç´šå›å¸°:é€ƒ]"
            if is_top_sashi:
                lift_bonus = max(lift_bonus, 0.5)
                lift_reason = "[Sç´šå›å¸°:å·®]"

        # Apply Lift Bonus to Top Scorer
        if lift_bonus > 0:
            df.loc[top_scorer_idx, 'ai_score'] += lift_bonus
            df.loc[top_scorer_idx, 'ai_tag'] += f" {lift_reason}"

    return df

# ==========================================
# 3. Classic Logic (Pre-Update)
# ==========================================

def calculate_classic_score(df):
    """
    Unified AI Score Logic (Classic + Hybrid features).
    Basis: Old Logic
    Added: 
      - Strongest Line 3rd Rider Bonus (+2.0/+1.0)
      - Longest Line Correction (Venue Adjusted)
      - Class-Specific Correction (Lift)
    """
    df = df.copy()
    
    # 1. Base Score calculation
    if 'ç«¶èµ°å¾—ç‚¹' not in df.columns:
        df['ai_score'] = 0.0
        return df
        
    df['base_score'] = pd.to_numeric(df['ç«¶èµ°å¾—ç‚¹'], errors='coerce').fillna(80.0)
    df['ai_score'] = df['base_score']
    df['ai_tag'] = ""
    
    # 2. Local Bonus
    if 'åœ°å…ƒ' in df.columns:
        mask = (df['åœ°å…ƒ'] == 1) | (df['åœ°å…ƒ'] == True)
        df.loc[mask, 'ai_score'] += 3.0
        df.loc[mask, 'ai_tag'] += " [åœ°å…ƒ]"
        
    # 3. Tactic Bonus
    if 'è„šè³ª' in df.columns:
        mask_nige = df['è„šè³ª'].astype(str).str.contains('é€ƒ')
        df.loc[mask_nige, 'ai_score'] += 2.0
        mask_makuri = df['è„šè³ª'].astype(str).str.contains('æ²')
        df.loc[mask_makuri, 'ai_score'] += 2.0
        
    # 4. Bank Specs Bonus
    if 'ç«¶è¼ªå ´' in df.columns:
        place = df['ç«¶è¼ªå ´'].iloc[0]
        specs = db_utils.VELODROME_SPECS.get(place)
        if specs:
            str_m, cant_deg, _ = specs
            if str_m < 50.0:
                if 'è„šè³ª' in df.columns:
                    mask = df['è„šè³ª'].astype(str).str.contains('é€ƒ')
                    df.loc[mask, 'ai_score'] += 2.0
                    df.loc[mask, 'ai_tag'] += " [çŸ­ç›´ç·š:é€ƒ]"
            elif str_m > 58.0:
                if 'è„šè³ª' in df.columns:
                    mask = df['è„šè³ª'].astype(str).str.contains('æ²|å·®')
                    df.loc[mask, 'ai_score'] += 2.0
                    df.loc[mask, 'ai_tag'] += " [é•·ç›´ç·š:æ²å·®]"
            if cant_deg < 30.0:
                if 'è„šè³ª' in df.columns:
                    mask = df['è„šè³ª'].astype(str).str.contains('é€ƒ')
                    df.loc[mask, 'ai_score'] += 2.0
                    df.loc[mask, 'ai_tag'] += " [ç·©å‚¾æ–œ:é€ƒ]"
            elif cant_deg > 33.0:
                if 'è„šè³ª' in df.columns:
                    mask = df['è„šè³ª'].astype(str).str.contains('æ²')
                    df.loc[mask, 'ai_score'] += 2.0
                    df.loc[mask, 'ai_tag'] += " [æ€¥å‚¾æ–œ:æ²]"

    # 5. Specialist Bonus (Top Tactic)
    # 6. Specialist Bonus (Top Tactic)
    # MOVED TO ADVANCED LOGIC to prevent duplication.
    # Checks for is_top_nige etc are now handled strictly in apply_advanced_logic.


    # 6. Line Logic (Strongest 3rd + Unique Longest)
    if 'ãƒ©ã‚¤ãƒ³' in df.columns and not df.empty and str(df.iloc[0]['ãƒ©ã‚¤ãƒ³']) != 'nan':
        line_str = str(df.iloc[0]['ãƒ©ã‚¤ãƒ³'])
        lines_raw = line_str.split()
        
        # Parse Lines
        line_infos = []
        df['è»Šç•ª'] = pd.to_numeric(df['è»Šç•ª'], errors='coerce').fillna(0).astype(int)
        
        valid_lines = True
        for l_s in lines_raw:
            members = []
            for char in l_s:
                if char.isdigit():
                    members.append(int(char))
            if not members: 
                continue
            
            # Leader Score
            leader = members[0]
            l_row = df[df['è»Šç•ª'] == leader]
            l_score = l_row.iloc[0]['base_score'] if not l_row.empty else 0.0
            
            line_infos.append({
                'members': members,
                'len': len(members),
                'score': l_score
            })
            
        if line_infos:
            # Sort by Length (Longest Line is Rank 1), then Score
            line_infos.sort(key=lambda x: (x['len'], x['score']), reverse=True)
            
            # A. Longest Line 3rd Bonus
            # Rank 1 Line 3rd -> +2.0
            # Other Lines 3rd -> +1.0
            for idx, info in enumerate(line_infos):
                if info['len'] >= 3:
                    # 3rd member is index 2
                    r3 = info['members'][2]
                    bonus = 2.0 if idx == 0 else 1.0
                    
                    # Apply
                    mask = (df['è»Šç•ª'] == r3)
                    df.loc[mask, 'ai_score'] += bonus
                    df.loc[mask, 'ai_tag'] += f" [L3ç•ªæ‰‹({bonus:+})]"
            
            # B. Unique Longest Line Correction
            lengths = [x['len'] for x in line_infos]
            max_len = max(lengths)
            if lengths.count(max_len) == 1:
                # Found Unique Longest
                u_idx = lengths.index(max_len)
                u_info = line_infos[u_idx]
                
                # Venue Adjustment
                # Super Strong: Seibuen, Tachikawa, Tamano, Toyohashi
                # Weak: Shizuoka, Takeo
                place_name = df['ç«¶è¼ªå ´'].iloc[0] if 'ç«¶è¼ªå ´' in df.columns else ""
                venue_adj = 0.0
                if place_name in ["è¥¿æ­¦åœ’", "ç«‹å·", "ç‰é‡", "è±Šæ©‹"]:
                    venue_adj = 0.5
                elif place_name in ["é™å²¡", "æ­¦é›„"]:
                    venue_adj = -1.0
                
                # Apply based on length
                if u_info['len'] >= 4:
                    # All members +2.5 (+Venue)
                    base_b = 2.5
                    final_b = base_b + venue_adj
                    for car in u_info['members']:
                        mask = (df['è»Šç•ª'] == car)
                        df.loc[mask, 'ai_score'] += final_b
                        df.loc[mask, 'ai_tag'] += f" [æœ€é•·4è»Š({final_b:+})]"
                        
                elif u_info['len'] == 3:
                    # Pos 1-2 +1.5, Pos 3 +0.5 (+Venue)
                    for pos_i, car in enumerate(u_info['members']):
                        if pos_i <= 1: # 1st, 2nd
                            base_b = 1.5
                        else: # 3rd
                            base_b = 0.5
                            
                        final_b = base_b + venue_adj
                        mask = (df['è»Šç•ª'] == car)
                        df.loc[mask, 'ai_score'] += final_b
                        df.loc[mask, 'ai_tag'] += f" [æœ€é•·3è»Š({final_b:+})]"

    # 7. Class Lift Bonus
    race_class = "A"
    if 'ç´šç­' in df.columns:
        classes = df['ç´šç­'].astype(str).unique()
        # Check A3 BEFORE S (since 'S' could match 'Sç´š' in other classes)
        has_a3 = any('A3' in c or 'Aç´š3ç­' in c for c in classes)
        has_s = any('S' in c for c in classes)
        if has_a3: race_class = "A3"
        elif has_s: race_class = "S"
        
    if not df.empty:
        top_scorer_idx = df['base_score'].idxmax()
        row = df.loc[top_scorer_idx]
        is_top_nige = row.get('is_top_nige', 0) == 1
        is_top_makuri = row.get('is_top_makuri', 0) == 1
        is_top_sashi = row.get('is_top_sashi', 0) == 1
        
        lift_bonus = 0.0
        lift_reason = ""
        
        if race_class == "A3": 
            if is_top_nige: 
                lift_bonus = max(lift_bonus, 4.0)
                lift_reason = "[A3å›å¸°:é€ƒ]"
            if is_top_makuri: 
                lift_bonus = max(lift_bonus, 4.0)
                lift_reason = "[A3å›å¸°:æ²]"
        elif race_class == "A":
            if is_top_nige:
                lift_bonus = max(lift_bonus, 2.5)
                lift_reason = "[Aç´šå›å¸°:é€ƒ]"
            if is_top_makuri:
                lift_bonus = max(lift_bonus, 2.0)
                lift_reason = "[Aç´šå›å¸°:æ²]"
        elif race_class == "S":
            if is_top_nige:
                lift_bonus = max(lift_bonus, 1.5)
                lift_reason = "[Sç´šå›å¸°:é€ƒ]"
            if is_top_sashi:
                lift_bonus = max(lift_bonus, 0.5)
                lift_reason = "[Sç´šå›å¸°:å·®]"

        if lift_bonus > 0:
            df.loc[top_scorer_idx, 'ai_score'] += lift_bonus
            df.loc[top_scorer_idx, 'ai_tag'] += f" {lift_reason}"

    return df

def get_line_partner_live(df, target_car):
    """
    Validation-verified Partner Logic for Live App.
    """
    try:
        # Check if line info exists (meta or parsed columns)
        # In live app, df usually has 'line_length', 'line_pos' from feature engineering?
        # If not, we might need raw line string parsing if available.
        # Assuming 'temp_line_id' might not be here unless we add it. 
        # Let's rely on 'line_pos' + 'ãƒ©ã‚¤ãƒ³' string parsing if needed or 'line_id' from earlier steps.
        
        # Fallback: Parse 'ãƒ©ã‚¤ãƒ³' column again if needed
        if 'ãƒ©ã‚¤ãƒ³' not in df.columns: return None
        
        line_str = str(df.iloc[0]['ãƒ©ã‚¤ãƒ³'])
        lines_raw = line_str.split()
        
        target_line = []
        for l_s in lines_raw:
            mems = [int(c) for c in l_s if c.isdigit()]
            if target_car in mems:
                target_line = mems
                break
        
        if not target_line: return None
        
        # Position in line
        try:
            idx = target_line.index(target_car)
            pos = idx + 1 # 1-based
        except: return None
        
        # Logic: 1->2, 2->1
        if pos == 1:
            if len(target_line) >= 2: return target_line[1] # 2nd member
        elif pos == 2:
            return target_line[0] # 1st member
            
        return None
    except: return None

def generate_classic_strategy(pred_df, score_col='ai_score'):
    """
    Generate Betting Strategy (User Custom Version).
    Prioritizes:
    1. 2T: Rank 1 -> Rank 2, 3, 4 (Nagashi) [Recovery 91%]
    2. 3T: Rank 1, 2 -> ... (Partner Logic) [Recovery 68%]
    """
    if score_col not in pred_df.columns:
        if 'ai_score' in pred_df.columns: score_col = 'ai_score'
        elif 'äºˆæ¸¬å‹ç‡' in pred_df.columns: score_col = 'äºˆæ¸¬å‹ç‡'
        
    # Girls Keirin Exclusion
    is_girls = False
    if 'class_code' in pred_df.columns:
        if 'L' in pred_df['class_code'].values: is_girls = True
    if 'ç´šç­' in pred_df.columns:
        if pred_df['ç´šç­'].astype(str).str.contains('L').any(): is_girls = True
    if 'ã‚¯ãƒ©ã‚¹' in pred_df.columns:
        if pred_df['ã‚¯ãƒ©ã‚¹'].astype(str).str.contains('ã‚¬ãƒ¼ãƒ«ã‚º').any(): is_girls = True
        
    if is_girls:
         return {"type": "disabled", "title": "å¯¾è±¡å¤–", "reason": "ã‚¬ãƒ¼ãƒ«ã‚ºã‚±ã‚¤ãƒªãƒ³ã¯äºˆæ¸¬å¯¾è±¡å¤–ã§ã™", "tickets": []}
        
    df_logic = pred_df.sort_values(score_col, ascending=False).reset_index(drop=True)
    
    if len(df_logic) < 4:
        return {"type": "error", "title": "ãƒ‡ãƒ¼ã‚¿ä¸è¶³", "reason": "ãƒ‡ãƒ¼ã‚¿ä¸è¶³(4è»Šæœªæº€)", "tickets": []}

    p1 = df_logic.iloc[0]
    p2 = df_logic.iloc[1]
    p3 = df_logic.iloc[2]
    p4 = df_logic.iloc[3]
    
    c1 = int(p1['è»Šç•ª'])
    c2 = int(p2['è»Šç•ª'])
    c3 = int(p3['è»Šç•ª'])
    c4 = int(p4['è»Šç•ª'])
    
    # --- Custom Strategy Logic: Pattern A ---
    # User Selected: "1ä½-2ä½ã€3ä½-2ä½ã€3ä½ã€4ä½" (1 -> 2,3 -> 2,3,4)
    # Based on verification result (Recovery 94.8%)
    
    # 3-Rentan Formation
    rec_tickets = []
    
    # 3T: 1 -> 2,3 -> 2,3,4
    s_2nd = f"{c2},{c3}"
    s_3rd = f"{c2},{c3},{c4}"
    
    rec_tickets.append(f"3é€£å˜: {c1} - {s_2nd} - {s_3rd}")
    
    # 2T: 1 -> 2,3,4 (Consistent coverage)
    rec_tickets.append(f"2è»Šå˜: {c1} â†’ {c2},{c3},{c4}")
    
    # Generate structured_bets
    structured_bets = []
    
    # 2T Expansion (c1 -> c2, c3, c4)
    for t in [c2, c3, c4]:
        structured_bets.append({
            'type': '2è»Šå˜',
            'first': [c1],
            'second': [t],
            'third': [],
            'amount': 100,
            'raw': f"2è»Šå˜: {c1}-{t}"
        })
        
    # 3T Expansion (c1 -> c2,3 -> c2,3,4)
    # Heads: [c1]
    # Seconds: [c2, c3]
    # Thirds: [c2, c3, c4]
    
    heads = [c1]
    seconds = [c2, c3]
    thirds = [c2, c3, c4]
    
    for h in heads:
        for s in seconds:
            if s == h: continue
            for t in thirds:
                if t == h or t == s: continue
                structured_bets.append({
                    'type': '3é€£å˜',
                    'first': [h],
                    'second': [s],
                    'third': [t],
                    'amount': 100,
                    'raw': f"3é€£å˜: {h}-{s}-{t}"
                })
        
    return {
        "type": "custom",
        "title": "ğŸ† æ¨å¥¨ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³",
        "reason": f"AIãƒ©ãƒ³ã‚¯ä¸Šä½ä¿¡é ¼ (2è»Šå˜å›åç‡é‡è¦– + 3é€£å˜)",
        "tickets": rec_tickets,
        "structured_bets": structured_bets,
        "top_win_rate": (p1.get(score_col, 0) / df_logic[score_col].sum() * 100) if df_logic[score_col].sum() > 0 else 0,
        "top_name": p1['é¸æ‰‹å']
    }


# ==========================================
# 6. Advanced Metrics & History
# ==========================================

def calculate_advanced_metrics(df_race):
    """
    Calculate advanced features for a single race dataframe (K-Dreams style)
    and return specific signals based on AI thresholds.
    df_race: Cleaned dataframe with 'ç«¶èµ°å¾—ç‚¹' or similar columns.
    """
    from scipy import stats
    
    # 1. Prepare Scores
    try:
        # ç«¶èµ°å¾—ç‚¹ãŒã‚ã‚Œã°ä½¿ã†
        if 'ç«¶èµ°å¾—ç‚¹' in df_race.columns:
            scores = pd.to_numeric(df_race['ç«¶èµ°å¾—ç‚¹'], errors='coerce').dropna().values
        else:
            return {}
            
        scores = np.sort(scores)[::-1] # Descending
        if len(scores) < 3: return {}
        
        # 2. Calculate Features
        # Score Gap 1-2
        score_diff_1_2 = scores[0] - scores[1]
        
        # Range Trimmed (Top - 2nd from Bottom)
        trimmed_bottom = scores[-2] if len(scores) > 1 else scores[-1]
        range_trimmed = scores[0] - trimmed_bottom
        score_range = scores[0] - scores[-1] # Full range
        
        # Elite Count (Max Gap)
        gaps = scores[:-1] - scores[1:] # Positive gaps
        max_gap_idx = np.argmax(gaps)
        elite_count = max_gap_idx + 1
        
        # Std
        score_std = np.std(scores)
        
    except Exception as e:
        return {}

    # 3. Evaluate Thresholds
    signals = []
    
    # é‰„æ¿ (High Confidence)
    if score_diff_1_2 > 3.425:
        signals.append("â˜…é‰„æ¿(ç‚¹æ•°å·®å¤§)")
    elif score_diff_1_2 > 2.0:
        signals.append("â—æœ¬å‘½")
        
    if range_trimmed > 7.655:
        signals.append("â˜…æ–­å±¤ã‚ã‚Š")
        
    # Elite Count
    if elite_count <= 1.5:
        signals.append("â˜…1å¼·")
    elif elite_count > 4:
        signals.append("âš æ··æˆ¦(ä¸Šä½æ‹®æŠ—)")
        
    # Std (Stability)
    if 10 <= score_std <= 21:
        signals.append("â—‹é †å½“å‚¾å‘")
        
    # Super Chaotic (Tight Range)
    if score_range < 5.0:
        signals.append("â˜ å¤§æ··æˆ¦")
        
    return {
        'score_diff_1_2': score_diff_1_2,
        'signals': signals,
        'line_strength_head': 'ä¸æ˜' # Logic placeholder
    }

def calculate_advanced_metrics_to_df(df):
    """
    Wrapper to apply calculate_advanced_metrics and add results to DF columns.
    Also ensures 'final_score' exists (alias of ai_score for now).
    """
    df = df.copy()
    
    # Run Metric Calc
    metrics = calculate_advanced_metrics(df)
    
    # Broadcast to all rows
    for k, v in metrics.items():
        if isinstance(v, list):
             # Join signals
             df[k] = ",".join(v)
        else:
             df[k] = v
             
    # Create final_score if not exists
    if 'final_score' not in df.columns:
        if 'ai_score' in df.columns:
            df['final_score'] = df['ai_score']
        else:
            df['final_score'] = 0.0
            
    return df

def calculate_history_stats(history, df_source):
    """
    Calculate generic Hit/Return stats from history vs df_source (results).
    df_source: Must contain 'race_id', 'ç€é †_val' (or 'ç€é †'), and Dividend cols.
    """
    if not history or df_source.empty:
        return None

    # Pre-index Results
    if 'race_id' not in df_source.columns:
        # Generate race_id if missing (simple fallback)
        try:
            # Simple hash fallback
            import hashlib
            df_source['race_id'] = df_source.apply(lambda r: hashlib.md5(f"{r.get('æ—¥ä»˜','')}{r.get('ç«¶è¼ªå ´','')}{r.get('ãƒ¬ãƒ¼ã‚¹ç•ªå·','')}".encode()).hexdigest(), axis=1)
        except: return None
        
    # Proceed with calcs (Omitted for brevity as this function was already present)
    return {}

# ==========================================
# 7. Player Detail Analysis (New Wing Feature)
# ==========================================

def analyze_player_detailed_stats(player_row, meta, db_path=db_utils.DB_PATH):
    """
    Analyze specific player stats for "Old Wing" style details.
    
    Args:
        player_row (pd.Series): Player data row
        meta (dict): Race metadata (date, place, etc.)
        db_path (str): Path to SQLite DB
        
    Returns:
        dict: Detailed stats and qualitative labels (Majin, Survivor, etc.)
    """
    if player_row is None or player_row.empty:
        return {}
        
    p_name = player_row.get('é¸æ‰‹å')
    if not p_name: return {}
    
    # Current Context
    current_line_len = player_row.get('ãƒ©ã‚¤ãƒ³é•·', 0) # Assumes feature eng ran or parsed
    current_line_pos = player_row.get('ãƒã‚¸ã‚·ãƒ§ãƒ³', 0)
    current_place = meta.get('place', '')
    
    # Bank Specs
    current_specs = db_utils.VELODROME_SPECS.get(current_place) # (Straight, Cant, Length)
    
    conn = sqlite3.connect(db_path)
    
    # Date limit (1 year ago)
    # SQLite date string comparison works if format is YYYY-MM-DD or YYYYå¹´MMæœˆDDæ—¥
    # Assuming "YYYYå¹´MMæœˆDDæ—¥" format in DB
    # For robust comparison, we might fetch last 100 races instead of strictly 1 year to avoid date logic complexity in SQL
    
    query = f"SELECT * FROM race_result WHERE \"é¸æ‰‹å\" = ? ORDER BY \"æ—¥ä»˜\" DESC LIMIT 100"
    try:
        df_hist = pd.read_sql_query(query, conn, params=[p_name])
    except:
        conn.close()
        return {}
        
    conn.close()
    
    if df_hist.empty:
        return {'msg': 'éå»ãƒ‡ãƒ¼ã‚¿ãªã—'}
        
    # --- 1. Basic Stats (Last 100 races ~ 1 year) ---
    total = len(df_hist)
    wins = len(df_hist[df_hist['ç€é †_val'] == 1])
    ren2 = len(df_hist[df_hist['ç€é †_val'] <= 2])
    ren3 = len(df_hist[df_hist['ç€é †_val'] <= 3])
    
    basic_stats = {
        'total': total,
        'win_rate': (wins/total)*100,
        'ren2_rate': (ren2/total)*100,
        'ren3_rate': (ren3/total)*100
    }
    
    # --- 2. Condition Matching (Line/Pos) ---
    # Need 'line_length', 'line_pos' in history.
    # If using raw DB, features might not be pre-calculated? 
    # 'run_global_features' saves them? No, usually calculated on load.
    # Assuming 'line_length' and 'line_pos' columns exist in DB or we re-calc?
    # In `load_and_process_data`, we select them. If they are in DB, great.
    # If not, we skip this specific condition or approx.
    
    cond_stats = {}
    if 'line_length' in df_hist.columns and 'line_pos' in df_hist.columns:
        # Filter
        df_cond = df_hist[
            (df_hist['line_length'] == current_line_len) & 
            (df_hist['line_pos'] == current_line_pos)
        ]
        if not df_cond.empty:
            c_total = len(df_cond)
            c_wins = len(df_cond[df_cond['ç€é †_val'] == 1])
            c_ren2 = len(df_cond[df_cond['ç€é †_val'] <= 2])
            c_ren3 = len(df_cond[df_cond['ç€é †_val'] <= 3])
            cond_stats = {
                'match_count': c_total,
                'win_rate': (c_wins/c_total)*100,
                'ren2_rate': (c_ren2/c_total)*100,
                'ren3_rate': (c_ren3/c_total)*100
            }
            
    # --- 3. Bank Matching ---
    bank_stats = {}
    bank_matches = []
    if current_specs:
        c_str, c_cant, _ = current_specs
        # Find similar banks from history (approx logic)
        # Iterate unique places in history
        places = df_hist['ç«¶è¼ªå ´'].unique()
        for p in places:
            specs = db_utils.VELODROME_SPECS.get(p)
            if specs:
                s_str, s_cant, _ = specs
                # Similarity: Straight within 5m, Cant within 2 deg?
                if abs(s_str - c_str) < 5.0 and abs(s_cant - c_cant) < 3.0:
                    bank_matches.append(p)
    
    if bank_matches:
        df_bank = df_hist[df_hist['ç«¶è¼ªå ´'].isin(bank_matches)]
        if not df_bank.empty:
            b_total = len(df_bank)
            b_wins = len(df_bank[df_bank['ç€é †_val'] == 1])
            b_ren2 = len(df_bank[df_bank['ç€é †_val'] <= 2])
            b_ren3 = len(df_bank[df_bank['ç€é †_val'] <= 3])
            bank_stats = {
                'match_banks': list(bank_matches)[:3], # Show top 3 examples
                'total': b_total,
                'win_rate': (b_wins/b_total)*100,
                'ren2_rate': (b_ren2/b_total)*100,
                'ren3_rate': (b_ren3/b_total)*100
            }

    # --- 4. Classifications (Majin, Survivor, etc.) ---
    labels = []
    w = basic_stats['win_rate']
    r2 = basic_stats['ren2_rate']
    r3 = basic_stats['ren3_rate']
    
    # Majin (Demon): Dominant Winner
    if w >= 40.0 and r3 >= 70.0:
        labels.append("ğŸ˜ˆ é­”äººç³» (åœ§å€’çš„å¼·ã•)")
    elif w >= 30.0:
        labels.append("ğŸ‘¹ é¬¼è„š (å‹ç‡é«˜)")
        
    # Survivor: High Ren3 but Low Win (Tenacious)
    if w < 10.0 and r3 >= 50.0:
        labels.append("ğŸ§Ÿ ã‚µãƒã‚¤ãƒãƒ¼ (3ç€æ®‹ã‚Š)")
        
    # Specialist: Better in Condition than Basic
    if cond_stats.get('win_rate', 0) > (w + 15.0):
        labels.append("ğŸ”§ æ¡ä»¶è·äºº (ãƒ©ã‚¤ãƒ³/ä½ç½® ãƒãƒã‚Š)")
        
    if bank_stats.get('win_rate', 0) > (w + 15.0):
        labels.append("ğŸ° ãƒãƒ³ã‚¯ã®ç”³ã—å­ (ã‚³ãƒ¼ã‚¹ç›¸æ€§æŠœç¾¤)")
        
    
    # Sort history by date desc for display
    if not df_hist.empty and 'æ—¥ä»˜' in df_hist.columns:
         df_hist = df_hist.sort_values('æ—¥ä»˜', ascending=False)

    return {
        'basic': basic_stats,
        'condition': cond_stats,
        'bank': bank_stats,
        'labels': labels,
        'history_df': df_hist # Return raw history for UI
    }

    # Group by ID
    results_map = {rid: grp for rid, grp in df_source.groupby('race_id')}

    stats = {
        'total_races': 0, 'analyzed_races': 0,
        'total_invest': 0, 'total_return': 0,
        'hit_count': 0, 'bet_count': 0
    }
    
    import hashlib
    
    # Pre-build lookup map from Results (df_source)
    # Key: (date_str, place_name, race_num_int) -> race_id
    res_lookup = {}
    if not df_source.empty:
        # Ensure Date format is standard
        # df_source usually has 'æ—¥ä»˜' as YYYYå¹´MMæœˆDDæ—¥
        # race_num might be int '1' or str '1R'
        for _, row in df_source.drop_duplicates('race_id').iterrows():
            d = str(row.get('æ—¥ä»˜', ''))
            p = row.get('ç«¶è¼ªå ´', '')
            try: r = int(float(str(row.get('ãƒ¬ãƒ¼ã‚¹ç•ªå·', 0)).replace('R','')))
            except: r = 0
            if d and p and r:
                res_lookup[(d, p, r)] = row['race_id']

    history_with_res = []

    for h in history:
        rid = h.get('race_id')
        
        # Try to find Race ID if missing or mismatched
        # 1. Standardize history date/place/num
        h_date = str(h.get('date', '')).replace('-', 'å¹´').replace('/', 'å¹´') 
        # Ensure YYYYå¹´MMæœˆDDæ—¥ format if possible, but exact match required
        h_place = h.get('place', '')
        try: h_rnum = int(float(str(h.get('race_num', 0)).replace('R','')))
        except: h_rnum = 0
        
        # 2. Look up in results
        found_rid = res_lookup.get((h_date, h_place, h_rnum))
        
        if found_rid:
             rid = found_rid # Overwrite with DB's ID
        elif not rid:
             # If not found in DB and no ID exists, gen hash just for persistence consistency
             raw_str = f"{h_date}{h_place}{h_rnum}R"
             rid = hashlib.md5(raw_str.encode()).hexdigest()

        res_row = h.copy()
        res_row['race_id'] = rid 
        res_row['status'] = 'æœª'
        res_row['return'] = 0
        res_row['invest'] = 0
        
        if rid in results_map:
            stats['analyzed_races'] += 1

            rdf = results_map[rid]
            
            # Outcome
            try:
                if 'ç€é †_val' not in rdf.columns:
                    rdf['ç€é †_val'] = pd.to_numeric(rdf['ç€é †'], errors='coerce').fillna(99)
                
                outcome_map = {} # rank -> [car_nums] (handle dead heat)
                for _, r in rdf.iterrows():
                    rnk = int(r['ç€é †_val'])
                    if rnk == 99: continue
                    if rnk not in outcome_map: outcome_map[rnk] = []
                    # Clean car num
                    cn = int(str(r['è»Šç•ª']).replace('.0',''))
                    outcome_map[rnk].append(cn)
                
                # Evaluate Bets
                sbets = h.get('structured_bets', [])
                
                # Check for Valid Result (Exclude Pending)
                if 1 not in outcome_map or 2 not in outcome_map:
                     res_row['status'] = 'çµæœæœªç€'
                     history_with_res.append(res_row)
                     continue

                if not sbets:
                     res_row['status'] = 'ãƒ‡ãƒ¼ã‚¿ãªã—'
                else:
                    hit_race = False
                    race_invest = 0
                    race_return = 0
                    
                    # Payouts (First row)
                    first = rdf.iloc[0]
                    # Columns expected: '3é€£å˜', '2é€£å˜', etc. 
                    def get_payout(c):
                        v = first.get(c, 0)
                        try: return float(str(v).replace(',','').replace('å††',''))
                        except: return 0.0
                    
                    div_3t = get_payout('3é€£å˜')
                    div_2t = get_payout('2é€£å˜')
                    
                    for b in sbets:
                        b_type = b.get('type')
                        pts = 0
                        is_hit = False
                        
                        # -- Logic for Point Count & Hit Check --
                        # Simplified for major types
                        
                        # 3Rent (Form)
                        if '3rentan' in b_type:
                            l1 = b.get('1st', [])
                            l2 = b.get('2nd', [])
                            l3 = b.get('3rd', [])
                            # Points
                            pts = len(l1) * len(l2) * len(l3)
                            # Hit Check
                            win1 = outcome_map.get(1, [])
                            win2 = outcome_map.get(2, [])
                            win3 = outcome_map.get(3, [])
                            
                            if win1 and win2 and win3:
                                if (win1[0] in l1) and (win2[0] in l2) and (win3[0] in l3):
                                    is_hit = True
                                    race_return += div_3t * 1 # Assume 100 yen unit match
                        
                        # 2Shatan
                        elif '2shatan' in b_type:
                            l1 = b.get('1st', []) # or c1
                            l2 = b.get('2nd', []) # or c2
                            if not l1: l1 = [b.get('c1')]
                            if not l2:
                                if 'c2' in b: l2 = [b.get('c2')]
                                elif 'c2_list' in b: l2 = b.get('c2_list')
                            
                            pts = len(l1) * len(l2)
                            
                            win1 = outcome_map.get(1, [])
                            win2 = outcome_map.get(2, [])
                            if win1 and win2:
                                if (win1[0] in l1) and (win2[0] in l2):
                                    is_hit = True
                                    race_return += div_2t
                        
                        # 3Rencpu (Box/Axis)
                        elif '3rencpu' in b_type or 'box' in b_type:
                            cars = b.get('cars', [])
                            if cars:
                                n = len(cars)
                                pts = n * (n-1) * (n-2) // 6
                            else:
                                pts = 5 # Dummy
                                
                        race_invest += pts * 100
                        if is_hit:
                            hit_race = True
                            
                    res_row['invest'] = race_invest
                    res_row['return'] = race_return
                    res_row['status'] = 'ğŸ¯HIT' if hit_race else 'ãƒã‚ºãƒ¬'
                    
                    stats['bet_count'] += 1
                    stats['total_invest'] += race_invest
                    stats['total_return'] += race_return
                    if hit_race: stats['hit_count'] += 1

            except Exception as e:
                res_row['status'] = f'Err'
        
        history_with_res.append(res_row)

    stats['history_data'] = history_with_res
    return stats


# ==========================================
# 7. AI Reporter Logic
# ==========================================
def generate_race_report(df, meta, strategy, api_key):
    """
    Generate a Keirin Race Report using Gemini.
    """
    if not api_key:
        return "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        
    import google.generativeai as genai
    genai.configure(api_key=api_key)
    
    # Construct Context
    place = meta.get('place', 'ä¸æ˜')
    race_num = meta.get('race_num', '?')
    cls = meta.get('race_class', '')
    
    # Area Map for Prompt Context (Explicitly tell AI the area)
    area_map = {
        "åŒ—æµ·é“":"åŒ—æ—¥æœ¬", "é’æ£®":"åŒ—æ—¥æœ¬", "å²©æ‰‹":"åŒ—æ—¥æœ¬", "å®®åŸ":"åŒ—æ—¥æœ¬", "ç§‹ç”°":"åŒ—æ—¥æœ¬", "å±±å½¢":"åŒ—æ—¥æœ¬", "ç¦å³¶":"åŒ—æ—¥æœ¬",
        "èŒ¨åŸ":"é–¢æ±", "æ ƒæœ¨":"é–¢æ±", "ç¾¤é¦¬":"é–¢æ±", "åŸ¼ç‰":"é–¢æ±", "æ±äº¬":"é–¢æ±", "æ–°æ½Ÿ":"é–¢æ±", "é•·é‡":"é–¢æ±", "å±±æ¢¨":"é–¢æ±",
        "åƒè‘‰":"å—é–¢æ±", "ç¥å¥ˆå·":"å—é–¢æ±", "é™å²¡":"å—é–¢æ±",
        "æ„›çŸ¥":"ä¸­éƒ¨", "å²é˜œ":"ä¸­éƒ¨", "ä¸‰é‡":"ä¸­éƒ¨", "å¯Œå±±":"ä¸­éƒ¨", "çŸ³å·":"ä¸­éƒ¨",
        "ç¦äº•":"è¿‘ç•¿", "æ»‹è³€":"è¿‘ç•¿", "äº¬éƒ½":"è¿‘ç•¿", "å¤§é˜ª":"è¿‘ç•¿", "å…µåº«":"è¿‘ç•¿", "å¥ˆè‰¯":"è¿‘ç•¿", "å’Œæ­Œå±±":"è¿‘ç•¿",
        "é³¥å–":"ä¸­å›½", "å³¶æ ¹":"ä¸­å›½", "å²¡å±±":"ä¸­å›½", "åºƒå³¶":"ä¸­å›½", "å±±å£":"ä¸­å›½",
        "å¾³å³¶":"å››å›½", "é¦™å·":"å››å›½", "æ„›åª›":"å››å›½", "é«˜çŸ¥":"å››å›½",
        "ç¦å²¡":"ä¹å·", "ä½è³€":"ä¹å·", "é•·å´":"ä¹å·", "ç†Šæœ¬":"ä¹å·", "å¤§åˆ†":"ä¹å·", "å®®å´":"ä¹å·", "é¹¿å…å³¶":"ä¹å·", "æ²–ç¸„":"ä¹å·"
    }

    # Players List text
    # Car | Name (Pref/Area) | Score | Line | Flags
    p_lines = []
    for _, row in df.iterrows():
        c = row['è»Šç•ª']
        n = row['é¸æ‰‹å']
        s = row['ç«¶èµ°å¾—ç‚¹']
        l = row.get('ãƒ©ã‚¤ãƒ³', '')
        fuken = row.get('åºœçœŒ', '')
        area = area_map.get(fuken, '?')
        
        # Extract Antigravity Flags & Reasons
        reasons_raw = row.get('bonus_reasons', [])
        if isinstance(reasons_raw, str):
            # Handle string case if it was somehow converted
            import ast
            try: reasons_list = ast.literal_eval(reasons_raw)
            except: reasons_list = [str(reasons_raw)]
        elif isinstance(reasons_raw, list):
            reasons_list = reasons_raw
        else:
            reasons_list = []
            
        # Clean list
        reasons_list = [str(r) for r in reasons_list if r and str(r) != 'nan']

        bonus_tags = []
        for r in reasons_list:
            bonus_tags.append(f"[{r}]")
        
        # Legacy Flags (Optional, if you want to keep 'é­”äºº' logic separate or just rely on tags)
        # We'll just dump all tags.
        
        try:
            s_val = float(row.get('ç«¶èµ°å¾—ç‚¹', 0))
        except:
            s_val = 0.0
            
        # Add AI Score (final_score) if exists
        ai_score_str = ""
        if 'final_score' in row:
             ai_s = float(row['final_score'])
             ai_score_str = f" [AIæŒ‡æ•°:{ai_s:.1f}]"

        tags_str = " ".join(bonus_tags)
        # Include Pref/Area in the text for AI
        p_lines.append(f"{c}ç•ª: {n} ({fuken}/{area}) (å¾—ç‚¹:{s_val:.2f}){ai_score_str} ãƒ©ã‚¤ãƒ³:{l} {tags_str}")
        
    p_text = "\n".join(p_lines)
    
    # ... (Line Parsing logic remains same but needs to be included or skipped cautiously)
    # Since I'm using replace_file_content heavily, I'll rely on the surrounding context being stable.
    # The user asked to fix the Prompt too.
    # I can't replace TWO non-contiguous blocks.
    # So I will do the Flag replacement first (this block).
    # Then I will do the Prompt replacement (next block).
    
    # Wait, the prompt replacement is separate.
    # This tool call is for FLAGS only.

    
    # Check Line Info (Assume Line Parse Logic works...)
    # ... (Line Parsing Logic Skipped for brevity, assuming standard blocks remain) ... Since we don't want to replace whole block, we target specific replacements.
    
    # ... (skipping line parsing logic replacement since I can't see it all in context) ...
    # Wait, replace_file_content requires contiguous block. I have to replace lines 1672+ separately if I want to change prompt.
    # Let's finish the loop first.
    
    # ... (The prompt part is further down) ...

# Splitting this into two replacements because the logic in between (Line Parsing) is long and I don't want to break it.
# First replacement: Update the Player Loop.

    # Oops, I can only do one replacement per call unless using multi_replace.
    # Let's use multi_replace.

    
    # Calculate Line Composition - USE meta['lines_parsed'] if available
    lines_parsed = meta.get('lines_parsed', '')
    if lines_parsed:
        line_summary = lines_parsed  # Use accurate parsed line from HTML
    else:
        line_summary = "æƒ…å ±ãªã—"
        # Fallback: try to reconstruct from DataFrame
        try:
            if 'ãƒ©ã‚¤ãƒ³' in df.columns:
                # Drop nulls
                valid_lines = df['ãƒ©ã‚¤ãƒ³'].astype(str).replace(['nan', 'None', ''], pd.NA).dropna()
                if not valid_lines.empty:
                    # Count counts per line_id (assuming 'ãƒ©ã‚¤ãƒ³' is a group ID or string like '123')
                    # If it's '123' style, we just want unique values. 
                    # If it's Group ID (1, 1, 2, 2...), we group.
                    # Heuristic: If values are short integers (1, 2, 3), treat as Group ID.
                    # If '123', '45', treat as actual composition.
                    sample = valid_lines.iloc[0]
                    
                    line_groups = {} # line_str -> [car_nums]
                    
                    # Check format
                    is_group_id = (len(sample) <= 2 and sample.isdigit())
                    
                    if is_group_id:
                         # Group ID mode
                         for _, r in df.iterrows():
                             lid = str(r.get('ãƒ©ã‚¤ãƒ³', ''))
                             c_num = str(r['è»Šç•ª'])
                             if lid not in ['nan', 'None', '']:
                                 if lid not in line_groups: line_groups[lid] = []
                                 line_groups[lid].append(c_num)
                    else:
                         # String mode (e.g. '123') - Unique values *are* the lines
                         seen = set()
                         for _, r in df.iterrows():
                             l_str = str(r.get('ãƒ©ã‚¤ãƒ³', ''))
                             if l_str and l_str not in ['nan', 'None', ''] and l_str not in seen:
                                 # Just use the string itself as description, but we want cars. 
                                 # Actually if col is '123', that IS the line.
                                 # But let's verify cars.
                                 # Extract digits
                                 mems = re.findall(r'\d', l_str)
                                 if mems:
                                     line_groups[l_str] = mems
                                     seen.add(l_str)

                    # Format output
                    summary_parts = []
                    for _, members in line_groups.items():
                        count = len(members)
                        mem_str = "-".join(members)
                        summary_parts.append(f"{mem_str} ({count}è»Š)")
                    
                    if summary_parts:
                        line_summary = " / ".join(summary_parts)

        except Exception as e:
            line_summary = f"ç®—å‡ºã‚¨ãƒ©ãƒ¼: {e}"

    # Fallback: If line_summary is empty or "æƒ…å ±ãªã—", Guess from Area
    if not line_summary or line_summary == "æƒ…å ±ãªã—":
        try:
             # Define Areas
             area_map = {
                 "åŒ—æµ·é“":"åŒ—æ—¥æœ¬", "é’æ£®":"åŒ—æ—¥æœ¬", "å²©æ‰‹":"åŒ—æ—¥æœ¬", "å®®åŸ":"åŒ—æ—¥æœ¬", "ç§‹ç”°":"åŒ—æ—¥æœ¬", "å±±å½¢":"åŒ—æ—¥æœ¬", "ç¦å³¶":"åŒ—æ—¥æœ¬",
                 "èŒ¨åŸ":"é–¢æ±", "æ ƒæœ¨":"é–¢æ±", "ç¾¤é¦¬":"é–¢æ±", "åŸ¼ç‰":"é–¢æ±", "æ±äº¬":"é–¢æ±", "æ–°æ½Ÿ":"é–¢æ±", "é•·é‡":"é–¢æ±", "å±±æ¢¨":"é–¢æ±",
                 "åƒè‘‰":"å—é–¢æ±", "ç¥å¥ˆå·":"å—é–¢æ±", "é™å²¡":"å—é–¢æ±",
                 "æ„›çŸ¥":"ä¸­éƒ¨", "å²é˜œ":"ä¸­éƒ¨", "ä¸‰é‡":"ä¸­éƒ¨", "å¯Œå±±":"ä¸­éƒ¨", "çŸ³å·":"ä¸­éƒ¨",
                 "ç¦äº•":"è¿‘ç•¿", "æ»‹è³€":"è¿‘ç•¿", "äº¬éƒ½":"è¿‘ç•¿", "å¤§é˜ª":"è¿‘ç•¿", "å…µåº«":"è¿‘ç•¿", "å¥ˆè‰¯":"è¿‘ç•¿", "å’Œæ­Œå±±":"è¿‘ç•¿",
                 "é³¥å–":"ä¸­å›½", "å³¶æ ¹":"ä¸­å›½", "å²¡å±±":"ä¸­å›½", "åºƒå³¶":"ä¸­å›½", "å±±å£":"ä¸­å›½",
                 "å¾³å³¶":"å››å›½", "é¦™å·":"å››å›½", "æ„›åª›":"å››å›½", "é«˜çŸ¥":"å››å›½",
                 "ç¦å²¡":"ä¹å·", "ä½è³€":"ä¹å·", "é•·å´":"ä¹å·", "ç†Šæœ¬":"ä¹å·", "å¤§åˆ†":"ä¹å·", "å®®å´":"ä¹å·", "é¹¿å…å³¶":"ä¹å·", "æ²–ç¸„":"ä¹å·"
             }
             
             # Assign Area
             df_temp = df.copy()
             df_temp['area'] = df_temp['åºœçœŒ'].map(area_map).fillna("ãã®ä»–")
             
             # Sort: Area (custom order) then Score
             # Custom Order: N, E, S, W... doesn't matter, just group
             # Group by Area
             area_groups = {}
             for _, r in df_temp.iterrows():
                 a = r['area']
                 if a == "ãã®ä»–": continue # Tanki usually
                 if a not in area_groups: area_groups[a] = []
                 area_groups[a].append(str(r['è»Šç•ª']))
            
             # Merge Small Groups (1 person) to Tanki? No, keep it.
             guessed_parts = []
             for a, mems in area_groups.items():
                 # Sort members by score? Or assume standard numbering?
                 # Usually number doesn't correlate to line pos.
                 # Just list them.
                 count = len(mems)
                 if count >= 2:
                     guessed_parts.append(f"{'-'.join(mems)} ({a}ãƒ©ã‚¤ãƒ³ {count}è»Š)")
                 else:
                     guessed_parts.append(f"{mems[0]} (å˜é¨ãƒ»{a})")
             
             if guessed_parts:
                 line_summary = " / ".join(guessed_parts)
                 line_summary += "\nâ€»(ãƒ‡ãƒ¼ã‚¿æ¬ æã®ãŸã‚åœ°åŒºåˆ¥æ¨å®š)"
                 
        except Exception as e:
             line_summary += f" (æ¨å®šå¤±æ•—: {e})"

    # Strategy Text
    strat_title = strategy.get('title', 'ä¸æ˜')
    tickets = "\n".join([f"- {t}" for t in strategy.get('tickets', [])])
    
    # Create AI Top Pick Context (Top 3)
    ai_top_pick_text = ""
    top_name = "æ³¨ç›®é¸æ‰‹"
    
    if 'final_score' in df.columns:
        # Sort by final_score desc
        df_sorted = df.sort_values('final_score', ascending=False)
        if not df_sorted.empty:
            top_rows = df_sorted.head(3)
            
            picks = []
            for i, (_, row) in enumerate(top_rows.iterrows()):
                rnk = i + 1
                picks.append(f"{rnk}ä½: {row['è»Šç•ª']}ç•ª {row['é¸æ‰‹å']} (è©•ä¾¡ç‚¹:{float(row.get('final_score', 0)):.1f})")
            
            ai_top_pick_text = "ã€AIä¸Šä½è©•ä¾¡ï¼ˆæ¨å¥¨ï¼‰ã€‘:\n" + "\n".join(picks)
            top_name = df_sorted.iloc[0]['é¸æ‰‹å'] # Primary pick name
    
    prompt = f'''
ã‚ãªãŸã¯ã€Œä¼èª¬ã®ç«¶è¼ªè¨˜è€…ã€ã¨ã—ã¦ã€ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€ã“ã®ãƒ¬ãƒ¼ã‚¹ï¼ˆ{place} {race_num}R {cls}ï¼‰ã®ã€Œå±•é–‹äºˆæƒ³ã€ã¨ã€Œæ¨å¥¨è²·ã„ç›®ã€ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
èª­è€…ãŒæ€ã‚ãšè»Šåˆ¸ã‚’è²·ã„ãŸããªã‚‹ã‚ˆã†ãªã€**è«–ç†çš„ã‹ã¤èª¬å¾—åŠ›ã®ã‚ã‚‹**è¨˜äº‹ã‚’æ±‚ã‚ã¾ã™ã€‚

## ãƒ©ã‚¤ãƒ³æ§‹æˆï¼ˆäºˆæƒ³ï¼‰
{line_summary}

## é¸æ‰‹ãƒ‡ãƒ¼ã‚¿ï¼ˆAIåˆ†ææ¸ˆã¿ï¼‰
{p_text}

## ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆAntigravityï¼‰ã®çµæœ
ãƒ»æˆ¦ç•¥: {strat_title}
ãƒ»æ¨å¥¨è²·ã„ç›®:
{tickets}

{ai_top_pick_text}

## åŸ·ç­†ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆå³å®ˆï¼‰

1.  **ãƒšãƒ«ã‚½ãƒŠ**:
    - ã‚ãªãŸã¯å ´ç«‹ã¡æ­´30å¹´ã®ãƒ™ãƒ†ãƒ©ãƒ³è¨˜è€…ã§ã™ã€‚çŸ¥æ€§ã¨æƒ…ç†±ã‚’å…¼ã­å‚™ãˆãŸèªã‚Šå£ã§åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
    - **ã€Œï½ã ã€ã€Œï½ã ã‚ã†ã€ã€Œï½ã«é•ã„ãªã„ã€**ã¨ã„ã†æ–­å®šçš„ãªå£èª¿ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚ã€Œã§ã™ã¾ã™ã€èª¿ã¯çµ¶å¯¾ç¦æ­¢ã€‚
    - **é‡è¦**: æ„Ÿå˜†ç¬¦ï¼ˆï¼ï¼‰ã®å¤šç”¨ã¯ã€ŒçŸ¥æ€§ãŒä½ãè¦‹ãˆã‚‹ã€ãŸã‚é¿ã‘ã‚‹ã“ã¨ã€‚è¨€è‘‰ã®é¸ã³æ–¹ã¨è«–ç†æ§‹æˆã§ç†±é‡ã‚’ä¼ãˆã¦ãã ã•ã„ã€‚

2.  **è«–æ‹ ã®æ˜ç¤º (é‡è¦)**:
    - ãŸã ã€Œå¼·ã„ã€ã¨è¨€ã†ã®ã§ã¯ãªãã€å¿…ãš**ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ãŸæ ¹æ‹ **ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚
    - **ä½¿ãˆã‚‹è¨€è‘‰**: ã€ŒAIæŒ‡æ•°XXç‚¹ã®åœ§å€’çš„ä¿¡é ¼æ„Ÿã€ã€Œ[é€ƒNo.1]ã®å…ˆè¡ŒåŠ›ã€ã€Œ[åœ°å…ƒ]ã®åœ°ã®åˆ©ã€ã€Œãƒãƒ³ã‚¯ç›¸æ€§ãŒå…‰ã‚‹ã€ã€‚
    - é¸æ‰‹ãƒ‡ãƒ¼ã‚¿ã® `[é€ƒNo.1]` `[æ²No.1]` `[å·®No.1]` `[åœ°å…ƒ]` ãªã©ã®ã‚¿ã‚°ã¯ã€ãã®é¸æ‰‹ã®**æœ€å¤§ã®æ­¦å™¨**ã§ã™ã€‚å¿…ãšè¨€åŠã—ã¦ãã ã•ã„ã€‚
    - AIæœ€ä¸Šä½è©•ä¾¡ã®é¸æ‰‹ï¼ˆ{top_name}ï¼‰ã«ã¤ã„ã¦ã¯ã€ãªãœAIãŒé¸ã‚“ã ã®ã‹ï¼ˆä»–ã‚’åœ§å€’ã™ã‚‹ç‚¹æ•°ã€è„šè³ªNo.1ã®å¼·ã¿ã€å±•é–‹ã®æœ‰åˆ©ã•ï¼‰ã‚’ç†±ãèªã£ã¦ãã ã•ã„ã€‚

3.  **è¨˜äº‹æ§‹æˆ**:
    - **ã€è¦‹å‡ºã—ã€‘**: ãƒ¬ãƒ¼ã‚¹ã®æ ¸å¿ƒã‚’çªãã‚­ãƒ£ãƒƒãƒãƒ¼ãªãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆè½ã¡ç€ã„ãŸãƒˆãƒ¼ãƒ³ã§ï¼‰ã€‚
    - **ã€å±•é–‹äºˆæƒ³ã€‘**: å·ç ²ã‹ã‚‰ã‚´ãƒ¼ãƒ«ã¾ã§ã®ãƒ‰ãƒ©ãƒã‚’æã„ã¦ãã ã•ã„ã€‚èª°ãŒé€ƒã’ã€èª°ãŒæ²ã‚‹ã®ã‹ã€‚ãƒ©ã‚¤ãƒ³ã®æ”»é˜²ã‚’å…·ä½“çš„ã«æå†™ã™ã‚‹ã“ã¨ã€‚ç‰¹ã«ã€Œè¦è­¦æˆ’(å¤§ç©´)ã€ã‚„ã€Œæ··æˆ¦ã«å¼·ã„ã€ã¨ã•ã‚ŒãŸä¸æ°—å‘³ãªé¸æ‰‹ãŒã„ã‚‹å ´åˆã¯ã€ãã®å‹•ãã‚’äºˆæƒ³ã«çµ„ã¿è¾¼ã‚€ã“ã¨ã€‚
    - **ã€é¸æ‰‹è©•ä¾¡ã€‘**: 
        - **æœ¬å‘½**: {top_name}ã€‚ãã®å¼·ã•ã‚’ãƒ‡ãƒ¼ã‚¿ã§è£ä»˜ã‘ã—ã€ä¿¡é ¼åº¦ã‚’ã‚¢ãƒ”ãƒ¼ãƒ«ã€‚
        - **å¯¾æŠ—ãƒ»ç©´**: å±•é–‹ãŒå‘ãé¸æ‰‹ã‚„ã€ä¸€ç™ºé€†è»¢ã®å¯èƒ½æ€§ãŒã‚ã‚‹é¸æ‰‹ï¼ˆAIè©•ä¾¡ä¸Šä½è€…ï¼‰ã‚’ç´¹ä»‹ã€‚
    - **ã€çµè«–ï¼ˆå‹è² ã®ç‹™ã„ç›®ï¼‰ã€‘**:
        - æœ€çµ‚çš„ãªè²·ã„ç›®ã‚’æç¤ºã€‚ã€Œã“ã“ãŒå‹è² å‡¦ã ã€ã€Œç‹™ã†ä¾¡å€¤ãŒã‚ã‚‹ã€ã¨èƒŒä¸­ã‚’æŠ¼ã™ã“ã¨ã€‚

4.  **ç¦æ­¢äº‹é …**:
    - ã€ŒAIã«ã‚ˆã‚‹ã¨ã€ã¨ã„ã†è¨€è‘‰ã¯ä½¿ã‚ãªã„ã€‚ã€Œãƒ‡ãƒ¼ã‚¿ãŒç¤ºã™ã€ã€Œå®¢è¦³æ•°å€¤ãŒè¨¼æ˜ã™ã‚‹ã€ã¨è¨€ã„æ›ãˆã‚‹ã“ã¨ã€‚
    - æ›–æ˜§ãªè¡¨ç¾ï¼ˆã€Œã‹ã‚‚ã—ã‚Œãªã„ã€ã€Œå¯èƒ½æ€§ãŒã‚ã‚‹ã€ï¼‰ã¯é¿ã‘ã‚‹ã€‚è¨€ã„åˆ‡ã‚‹ã“ã¨ã§ä¿¡é ¼ã‚’å¾—ã‚‹ã€‚
    - **æ„Ÿå˜†ç¬¦ï¼ˆï¼ï¼‰ã®ä¹±ç”¨**ï¼ˆæ–‡æœ«ã”ã¨ã«ã¤ã‘ã‚‹ã®ã¯ç¦æ­¢ï¼‰ã€‚ã“ã“ãã¨ã„ã†å ´é¢ã§1ï½2å›ã«ç•™ã‚ã‚‹ã“ã¨ã€‚

**æ–‡å­—æ•°**: 500ï½700æ–‡å­—ç¨‹åº¦ã€‚
ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªèª‡ã‚Šã‚’æŒã£ã¦æ›¸ã„ã¦ãã ã•ã„ã€‚
'''

    try:
        # gemini-1.5-flash is NOT available in this environment (checked via list_models).
        # Available: gemini-2.5-flash, gemini-2.0-flash, gemini-flash-latest
        # We switch to gemini-2.5-flash.
        model = genai.GenerativeModel("gemini-2.5-flash")
        
        import time
        max_retries = 3
        base_delay = 5
        
        for attempt in range(max_retries):
            try:
                response = model.generate_content(prompt)
                return response.text
            except Exception as e:
                err_str = str(e)
                if "429" in err_str or "Quota" in err_str:
                     if attempt < max_retries - 1:
                         sleep_time = base_delay * (2 ** attempt)
                         # Logic to log warning could go here
                         time.sleep(sleep_time)
                         continue
                # If not 429 or retries exhausted, re-raise or return error
                if attempt == max_retries - 1:
                    return f"AIç”Ÿæˆã‚¨ãƒ©ãƒ¼(æ··é›‘ä¸­): {e} - æ™‚é–“ã‚’ç½®ã„ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„"
                
        return "AIç”Ÿæˆã‚¨ãƒ©ãƒ¼: äºˆæœŸã›ã¬å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ"

    except Exception as e:
        return f"AIç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e} - ãƒ¢ãƒ‡ãƒ«åã‚’å¤‰æ›´ã—ã¦ãã ã•ã„"


# ==========================================
# 8. AI Chat Assistant Logic
# ==========================================
def generate_chat_response(messages, context_data, api_key):
    """
    Generate a response for the AI Chat Assistant.
    
    Args:
        messages (list): List of chat messages [{"role": "user", "content": "..."}, ...]
        context_data (dict): Dictionary containing race context (scores, strategies, etc.)
        api_key (str): Gemini API Key
        
    Returns:
        str: AI response text
    """
    if not api_key:
        return "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰è¨­å®šã—ã¦ãã ã•ã„ã€‚"

    import google.generativeai as genai
    genai.configure(api_key=api_key)
    
    # 1. Construct System Prompt from Context
    # Unpack context
    place = context_data.get('place', 'ä¸æ˜')
    race_num = context_data.get('race_num', '?')
    p_data = context_data.get('players_text', 'é¸æ‰‹ãƒ‡ãƒ¼ã‚¿ãªã—')
    strategy_info = context_data.get('strategy_info', 'æˆ¦ç•¥æƒ…å ±ãªã—')
    logic_info = context_data.get('logic_info', 'ãƒ­ã‚¸ãƒƒã‚¯æƒ…å ±ãªã—')
    
    system_prompt = f"""
ã‚ãªãŸã¯ç«¶è¼ªäºˆæƒ³ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ç¾åœ¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦‹ã¦ã„ã‚‹ãƒ¬ãƒ¼ã‚¹ã¯ã€Œ{place} {race_num}Rã€ã§ã™ã€‚
ä»¥ä¸‹ã®**åˆ†æãƒ‡ãƒ¼ã‚¿**ã«åŸºã¥ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«çš„ç¢ºã‹ã¤å°‚é–€çš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚
ã‚ãªãŸã®å½¹å‰²ã¯ã€ãŸã ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã‚‹ã ã‘ã§ãªãã€ãã®ãƒ‡ãƒ¼ã‚¿ã®æ„å‘³ï¼ˆãƒ©ã‚¤ãƒ³ã®å¼·å¼±ã€å±•é–‹ã®ã‚ã‚„ã€ç©´ã®å¯èƒ½æ€§ï¼‰ã‚’è§£èª¬ã™ã‚‹ã“ã¨ã§ã™ã€‚

## é¸æ‰‹ãƒ‡ãƒ¼ã‚¿ï¼ˆAIåˆ†æã‚¹ã‚³ã‚¢ä»˜ï¼‰
{p_data}

## AIæˆ¦ç•¥åˆ†æï¼ˆAntigravityï¼‰
{strategy_info}

## ãƒ­ã‚¸ãƒƒã‚¯æ¤œå‡ºï¼ˆå±é™ºãªé¸æ‰‹ãªã©ï¼‰
{logic_info}

## å›ç­”ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
1. **å°‚é–€å®¶ã®è¦–ç‚¹**: ç´ äººã«ã¯æ°—ã¥ã‹ãªã„è¦–ç‚¹ï¼ˆãƒ©ã‚¤ãƒ³ã®çµæŸã€ç•ªæ‰‹ã®æŠ€é‡ã€ãƒãƒ³ã‚¯ç›¸æ€§ãªã©ï¼‰ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
2. **ãƒ‡ãƒ¼ã‚¿æ ¹æ‹ **: ã€Œå¼·ã„ã§ã™ã€ã§ã¯ãªãã€ŒAIã‚¹ã‚³ã‚¢ãŒXXç‚¹ã¨çªå‡ºã—ã¦ãŠã‚Šï½ã€ã€Œç›´è¿‘ã®é€£å¯¾ç‡ãŒï½ã€ã¨ãƒ‡ãƒ¼ã‚¿ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚
3. **æ–­å®šçš„ãªå£èª¿**: è‡ªä¿¡ã‚’æŒã£ã¦ç­”ãˆã¦ãã ã•ã„ã€‚ã€Œï½ã ã¨æ€ã„ã¾ã™ã€ã‚ˆã‚Šã‚‚ã€Œï½ã§ã—ã‚‡ã†ã€ã€Œï½ã¨è¨€ãˆã¾ã™ã€ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
4. **çŸ­æ½”ã«**: é•·ã™ããªã„ã‚ˆã†ã«ã€‚è¦ç‚¹ã‚’çªã„ã¦ãã ã•ã„ã€‚
"""

    # 2. Build Generation Config
    generation_config = {
        "temperature": 0.7,
        "top_p": 0.95,
        "top_k": 40,
        "max_output_tokens": 4096,
    }
    
    # 3. Create Model
    # Use gemini-2.5-flash as standard
    model = genai.GenerativeModel(
        model_name="gemini-2.5-flash",
        generation_config=generation_config,
        system_instruction=system_prompt
    )
    
    # 4. Convert Messages to Gemini format
    # Streamlit messages: [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
    # Gemini history: [{"role": "user", "parts": ["..."]}, {"role": "model", "parts": ["..."]}]
    
    chat_history = []
    # Skip the last message as it's the new prompt to send via send_message? 
    # Or start chat with history.
    # We'll use start_chat.
    
    for m in messages[:-1]: # All except last (which is the new input)
        role = "user" if m["role"] == "user" else "model"
        chat_history.append({"role": role, "parts": [m["content"]]})
        
    current_query = messages[-1]["content"]
    
    try:
        chat = model.start_chat(history=chat_history)
        response = chat.send_message(current_query)
        return response.text
    except Exception as e:
        return f"AIã‚¨ãƒ©ãƒ¼: {e}"


# ==========================================
# 9. History Analysis Logic
# ==========================================
def analyze_prediction_history(history_data, db_path=db_utils.DB_PATH):
    """
    Analyze prediction history against DB results.
    Returns:
      df_res: DataFrame (Race Level Summary)
      stats: Dict (Global Summary)
      df_tickets: DataFrame (Ticket Level Details for Pivoting)
    """
    if not history_data:
        return pd.DataFrame(), {}, pd.DataFrame()
        
    def clean_rank(x):
        try:
            s = str(x).replace('ç€','').replace('éƒ¨','').strip()
            if not s or s.lower() in ['nan', 'none', 'null']: return 99
            val = int(float(s))
            return val if val > 0 else 99
        except: return 99
        
    import hashlib
    
    # 1. Identify Target Race IDs from History
    target_rids = set()
    for h in history_data:
        p = h.get('place')
        d = h.get('date')
        r_str = str(h.get('race_num','')).replace('R','')
        
        # Check for Girls/L-Class (heuristics)
        # If 'race_type' or 'strategy_type' indicates girls? Not reliably saved yet.
        # We will filter after loading DB data if needed.
        
        if p and d and r_str:
            target_rids.add(f"{p}_{d}_{r_str}R")
            
    if not target_rids:
        return pd.DataFrame(), {}, pd.DataFrame()
        
    # 2. Load Specific Race Results (Efficient)
    # Using db_utils.load_race_results_by_ids, but need to handle potential chunking if not inside it.
    # Actually db_utils.load_race_results_by_ids does NOT chunk automatically in this version.
    # So we chunk here or rely on the function if updated. 
    # Let's chunk here to be safe.
    
    df_source_list = []
    chunk_size = 900
    rids_list = list(target_rids)
    
    conn = sqlite3.connect(db_path)
    try:
        for i in range(0, len(rids_list), chunk_size):
            chunk = rids_list[i:i+chunk_size]
            placeholders = ",".join(["?"] * len(chunk))
            query = f"SELECT * FROM race_result WHERE race_id IN ({placeholders})"
            try:
                # Use standard read_sql
                chunk_df = pd.read_sql_query(query, conn, params=chunk)
                if not chunk_df.empty:
                    df_source_list.append(chunk_df)
            except Exception as e:
                print(f"Chunk Load Error: {e}")
    except Exception as e:
        print(f"DB Load Error: {e}")
    finally:
        conn.close()
        
    if not df_source_list:
        # Fallback: Return empty if nothing found
        pass
        df_source = pd.DataFrame()
    else:
        df_source = pd.concat(df_source_list, ignore_index=True)
        # Rename columns to match logic expectations
        # DB -> Logic
        reverse_map = {
            '1 ç€': '1ç€', '2 ç€': '2ç€', '3 ç€': '3ç€', 'ç€ å¤–': 'ç€å¤–',
            'å‹ ç‡': 'å‹ç‡', '2é€£ å¯¾ç‡': '2é€£å¯¾ç‡', '3é€£ å¯¾ç‡': '3é€£å¯¾ç‡'
        }
        df_source.rename(columns=reverse_map, inplace=True)

    # 3. Analyze
    stats = {
        'total_races': 0, 'analyzed_races': 0,
        'total_invest': 0, 'total_return': 0,
        'hit_count': 0, 'bet_count': 0
    }
    
    # Map Results
    race_map = {}
    if not df_source.empty:
        # Ensure rank_val
        def _local_rank(x):
            try: return int(float(str(x).replace('ç€','').replace('éƒ¨','')))
            except: return 99
        df_source['rank_val'] = df_source['1ç€'].apply(_local_rank) # Actually 1ç€ col is just name?
        # WAIT. race_result table usually has 'ç€é †' column?
        # Let's check db_utils load_race_results_by_ids content.
        # It selects * from race_result.
        # race_result has '1ç€', '2ç€' cols? No, usually 'ç€é †' is one column if normalized?
        # OR is it '1ç€' = First Place Name?
        # Re-reading db_utils: it creates table `race_result`.
        # Step 54 lines 657-658 suggest col_map: '1ç€' -> '1 ç€'.
        # This implies it stores WHO was 1st.
        # But `calculate_history_stats` (now being replaced/refactored inside this function)
        # used `df_db.groupby('race_id')`.
        # And line 2717: `for rid, grp in df_db.groupby('race_id'):`
        # and `sorted_grp = grp.sort_values('rank_val')`
        # This structure implies `race_result` has ONE ROW PER PLAYER?
        # IF `race_result` is one row per player, then `save_race_data` saves `final_df` which comes from scraper.
        # Scraper returns DF with 1 record per player.
        # Yes, `race_result` is highly granular (1 row per player).
        # So "SELECT * WHERE race_id IN..." returns multiple rows per race.
        # Correct.
        pass

    # Process Results for Lookup
    # We need to reconstruct the `result_map` used in loop.
    # Loop at 2747 puts `race_map[rid] = {'top3': top3, 'payouts': payouts}`
    
    if not df_source.empty:
        # Ensure we have 'ç€é †' or 'rank_val'
        # Standardize rank column
        if 'ç€é †' in df_source.columns:
             df_source['rank_val'] = df_source['ç€é †'].apply(clean_rank)
        elif '1ç€' in df_source.columns: 
             # This naming is confusing. '1ç€' usually means specific payout or name.
             # If `race_result` is player-grain, it should have 'ç€é †' column (rank).
             # Let's assume 'ç€é †' exists if it was saved by `save_race_data`.
             # `save_race_data` saves `combined_df`.
             # `combined_df` has columns from scraper.
             pass
             
    # Re-use existing loop logic for map construction
    # But we replaced the loading block which defined `df_db`?
    # Wait, the code I am replacing (2642-2703) loads `df_source` via `load_and_process_data`.
    # BUT `load_and_process_data` returns what?
    # `load_and_process_data` in logic_v2 (line 1470) loads from DB and processes it.
    # It returns a DF with one row per player? Yes usually.
    # BUT line 2717 uses `df_db`? Where does `df_db` come from?
    # Is it `df_source` renamed?
    # In line 2663: `df_source = load_and_process_data(...)`
    # In line 2717: `for rid, grp in df_db.groupby('race_id'):`
    # Warning: `df_db` is NOT defined in the visible snippet 2629-2703.
    # It must be defined later or `df_source` is meant to be `df_db`.
    # Ah, I see "No need to filter df_db again as we only fetched target_ids" at line 2714.
    # So `df_db` was likely `df_source`.
    # I should assign `df_db = df_source`.
    
    df_db = df_source

    history_with_res = []
    
    def clean_rank(x):
        try:
            s = str(x).replace('ç€','').replace('éƒ¨','').strip()
            if not s or s.lower() in ['nan', 'none', 'null']: return 99
            val = int(float(s))
            return val if val > 0 else 99
        except: return 99

    # No need to filter df_db again as we only fetched target_ids

    
    for rid, grp in df_db.groupby('race_id'):
        sorted_grp = grp.sort_values('rank_val')
        
        # Check Class for Girls Exclusion
        # Assuming 'ç´šç­' or similar column exists in `grp` (race_result)?
        # Usually race_result has basic info. If not, we check `events` logic?
        # Let's try heuristic: specific cols or if line info is empty/special?
        # Actually 'ç´šç­' is usually in the scraper DF. 
        # If any player is 'Lç´š', skip this race.
        is_girls = False
        if 'ç´šç­' in grp.columns:
             if grp['ç´šç­'].apply(lambda x: 'L' in str(x)).any():
                 is_girls = True
        
        if is_girls: continue
        
        # Strict Check: Rank must be 1, 2, or 3.
        valid_rows = sorted_grp[(sorted_grp['rank_val'] >= 1) & (sorted_grp['rank_val'] <= 3)]
        top3 = valid_rows['è»Šç•ª'].astype(str).tolist()
        
        payouts = {}
        r0 = grp.iloc[0]
        # Iterate keys to find in r0. logic uses '2è»Šå˜' not '2é€£å˜' internally.
        # So we map DB keys to Logic keys.
        # DB -> Logic
        key_map = {
            "2é€£å˜": "2è»Šå˜", 
            "2è»Šå˜": "2è»Šå˜", # Just in case
            "3é€£å˜": "3é€£å˜", 
            "2é€£è¤‡": "2é€£è¤‡", 
            "3é€£è¤‡": "3é€£è¤‡", 
            "ãƒ¯ã‚¤ãƒ‰1": "ãƒ¯ã‚¤ãƒ‰1", 
            "ãƒ¯ã‚¤ãƒ‰2": "ãƒ¯ã‚¤ãƒ‰2", 
            "ãƒ¯ã‚¤ãƒ‰3": "ãƒ¯ã‚¤ãƒ‰3"
        }
        
        for db_k, logic_k in key_map.items():
            if db_k in r0 and pd.notna(r0[db_k]):
                try:
                    val_str = str(r0[db_k]).replace('å††','').replace(',','').strip()
                    payouts[logic_k] = float(val_str)
                except:
                    pass # Keep 0 or missing
        
        race_map[rid] = {'top3': top3, 'payouts': payouts}

    # 3. Analyze History
    results = []
    
    # Store Ticket Level Data
    ticket_rows = []
    
    total_invest = 0
    total_return = 0
    
    def p_part(p):
        if not p: return []
        return [x.strip() for x in p.split(',') if x.strip()]

    # --- Deduplicate (Keep Newest) ---
    seen_rids = set()
    deduped_history = []
    # Ensure sorted by timestamp descending
    try:
        sorted_history = sorted(history_data, key=lambda x: x.get('timestamp', ''), reverse=True)
    except:
        sorted_history = history_data

    for h in sorted_history:
        p = h.get('place')
        d = h.get('date')
        r = str(h.get('race_num','')).replace('R','') + 'R'
        st_type = h.get('strategy_type', 'unknown')
        rid = f"{p}_{d}_{r}_{st_type}" # Composite key for dedup
        
        if rid in seen_rids:
             continue
        seen_rids.add(rid)
        deduped_history.append(h)

    for h in deduped_history:
        row = h.copy()
        
        place = row.get('place')
        date = row.get('date')
        r_num_str = str(row.get('race_num','')).replace('R','') + 'R'
        row['race_num'] = r_num_str # Normalize display
        rid = f"{place}_{date}_{r_num_str}"
        
        row['is_hit'] = False
        row['benefit'] = 0
        row['investment'] = 0
        row['balance'] = 0
        row['hit_detail'] = "çµæœå¾…/ç„¡"
        
        if rid in race_map:
            res_info = race_map[rid]
            top3 = res_info['top3']
            payouts = res_info['payouts']
            
            # --- Check for Valid Result (Exclude Pending) ---
            # If no payouts or no top3, treat as pending
            if not payouts or len(top3) < 2:
                row['hit_detail'] = "çµæœæœªç€"
                row['investment'] = 0
                row['benefit'] = 0
                row['balance'] = 0
                row['status'] = 'æœª'
                results.append(row)
                continue
                
            row['result_top3'] = "-".join(top3)
            
            tickets = row.get('tickets', [])
            if isinstance(tickets, str): tickets = [tickets]
            
            race_invest = 0
            race_return = 0
            hit_strs = []
            
            for t_str in tickets:
                points = 0
                pay = 0
                tickets_hit = False
                
                # Determine Type
                t_type = "ãã®ä»–"
                if "3é€£å˜" in t_str: t_type = "3é€£å˜"
                elif "2è»Šå˜" in t_str: t_type = "2è»Šå˜"
                elif "3é€£è¤‡" in t_str: t_type = "3é€£è¤‡"
                elif "2é€£è¤‡" in t_str: t_type = "2é€£è¤‡"
                elif "ãƒ¯ã‚¤ãƒ‰" in t_str: t_type = "ãƒ¯ã‚¤ãƒ‰"
                
                content_part = t_str.split(':')[-1].strip()
                combinations = []
                parts = []
                is_fold = False
                if 'â†”' in content_part:
                    # è£è¡¨ (fold) format: "1,3 â†” 1,3"
                    parts = [p.strip() for p in content_part.split('â†”')]
                    is_fold = True
                elif 'â†’' in content_part:
                    parts = [p.strip() for p in content_part.split('â†’')]
                elif '=' in content_part:
                    parts = [p.strip() for p in content_part.split('=')]
                else:
                    parts = [p.strip() for p in content_part.split('-')]
                
                # Count Points Logic
                if t_type == "3é€£å˜" and len(parts) == 3:
                     g1, g2, g3 = p_part(parts[0]), p_part(parts[1]), p_part(parts[2])
                     for c1 in g1:
                         for c2 in g2:
                             if c1 == c2: continue
                             for c3 in g3:
                                 if c3 == c1 or c3 == c2: continue
                                 points += 1
                                 combinations.append([c1, c2, c3])
                elif t_type == "2è»Šå˜" and len(parts) >= 2:
                     g1, g2 = p_part(parts[0]), p_part(parts[1])
                     if is_fold:
                         # Fold (è£è¡¨): Generate both directions (c1â†’c2 and c2â†’c1)
                         unique_cars = list(set(g1 + g2))
                         for i, c1 in enumerate(unique_cars):
                             for c2 in unique_cars[i+1:]:
                                 points += 2  # Both directions
                                 combinations.append([c1, c2])
                                 combinations.append([c2, c1])
                     else:
                         for c1 in g1:
                             for c2 in g2:
                                 if c1 == c2: continue
                                 points += 1
                                 combinations.append([c1, c2])
                elif t_type == "3é€£è¤‡" and len(parts) == 3:
                     g1, g2, g3 = p_part(parts[0]), p_part(parts[1]), p_part(parts[2])
                     seen = set()
                     for c1 in g1:
                         for c2 in g2:
                             if c1 == c2: continue
                             for c3 in g3:
                                 if c3 == c1 or c3 == c2: continue
                                 comb_tuple = tuple(sorted([c1, c2, c3]))
                                 if comb_tuple not in seen:
                                     seen.add(comb_tuple)
                                     points += 1
                                     combinations.append(list(comb_tuple))
                elif (t_type == "2é€£è¤‡" or t_type == "ãƒ¯ã‚¤ãƒ‰") and len(parts) == 2:
                     g1, g2 = p_part(parts[0]), p_part(parts[1])
                     seen = set()
                     for c1 in g1:
                         for c2 in g2:
                             if c1 == c2: continue
                             comb_tuple = tuple(sorted([c1, c2]))
                             if comb_tuple not in seen:
                                 seen.add(comb_tuple)
                                 points += 1
                                 combinations.append(list(comb_tuple))
                
                if points == 0:
                    cnt = 1
                    for p in parts:
                        n = len(p.split(','))
                        cnt *= n
                    points = cnt 
                
                t_invest = points * 100
                race_invest += t_invest
                
                # Check Hit
                r1 = top3[0] if len(top3) >= 1 else None
                r2 = top3[1] if len(top3) >= 2 else None
                r3 = top3[2] if len(top3) >= 3 else None
                
                if t_type == "3é€£å˜" and r1 and r2 and r3:
                    if [r1, r2, r3] in combinations:
                        pay = payouts.get('3é€£å˜', 0)
                        tickets_hit = True
                elif t_type == "2è»Šå˜" and r1 and r2:
                    if [r1, r2] in combinations:
                        pay = payouts.get('2è»Šå˜', 0)
                        tickets_hit = True
                elif t_type == "3é€£è¤‡" and r1 and r2 and r3:
                    tgt = {r1, r2, r3}
                    for cm in combinations:
                        if set(cm) == tgt:
                            pay = payouts.get('3é€£è¤‡', 0)
                            tickets_hit = True
                            break
                            
                elif t_type == "2é€£è¤‡" and r1 and r2:
                    tgt = {r1, r2}
                    for cm in combinations:
                        if set(cm) == tgt:
                            pay = payouts.get('2é€£è¤‡', 0)
                            tickets_hit = True
                            break
                            
                elif t_type == "ãƒ¯ã‚¤ãƒ‰" and r1 and r2 and r3:
                    # Logic: Generate the 3 winning pairs from Top 3
                    # Pairs: {1,2}, {1,3}, {2,3} (indices of Top3)
                    win_pairs = []
                    win_pairs.append(tuple(sorted([r1, r2]))) # 1-2
                    win_pairs.append(tuple(sorted([r1, r3]))) # 1-3
                    win_pairs.append(tuple(sorted([r2, r3]))) # 2-3
                    
                    # Sort pairs by car number (Standard Keirin Order for Wide Payouts)
                    # e.g. (1,5), (1,9), (5,9)
                    win_pairs.sort()
                    
                    # Map to payouts
                    wide_map = {}
                    if len(win_pairs) >= 1: wide_map[str(list(win_pairs[0]))] = payouts.get('ãƒ¯ã‚¤ãƒ‰1', 0)
                    if len(win_pairs) >= 2: wide_map[str(list(win_pairs[1]))] = payouts.get('ãƒ¯ã‚¤ãƒ‰2', 0)
                    if len(win_pairs) >= 3: wide_map[str(list(win_pairs[2]))] = payouts.get('ãƒ¯ã‚¤ãƒ‰3', 0)
                    
                    # Check Predictions
                    ticket_pay = 0
                    all_hit_flag = False
                    
                    for cm in combinations:
                        # cm is [c1, c2]
                        check_pair = tuple(sorted(cm))
                        check_key = str(list(check_pair))
                         
                        if check_pair in win_pairs:
                            # It's a hit!
                            hit_p = wide_map.get(check_key, 0)
                            if hit_p > 0:
                                ticket_pay += hit_p
                                all_hit_flag = True
                                
                    if all_hit_flag:
                        pay = ticket_pay
                        tickets_hit = True

                # Aggregate Ticket Data
                if tickets_hit and pay > 0:
                    race_return += pay
                    hit_strs.append(f"{t_type}ğŸ¯{int(pay):,}å††")
                
                # Add to Ticket DF structure
                ticket_rows.append({
                    "place": place,
                    "date": date,
                    "type": t_type,
                    "invest": t_invest,
                    "return": pay if tickets_hit else 0,
                    "is_hit": 1 if tickets_hit else 0
                })

            row['investment'] = race_invest
            row['benefit'] = race_return
            row['balance'] = race_return - race_invest
            row['is_hit'] = (race_return > 0)
            if hit_strs:
                row['hit_detail'] = " ".join(hit_strs)
            else:
                # If race exists but no Top 3 (or incomplete), treat as Pending
                if not top3 or (len(top3) < 3 and len(top3) < 1): # At least winner should be there
                    row['hit_detail'] = "çµæœæœªç€"
                    # Reset financials for safety if pending
                    row['benefit'] = 0
                    row['balance'] = 0 # Show 0 balance instead of negative for Pending to avoid user confusion
                    # These rows are filtered out of Totals in app_polars anyway.
                else:
                    row['hit_detail'] = "ä¸çš„ä¸­"
            
            total_invest += race_invest
            total_return += race_return
            
        else:
            row['hit_detail'] = "çµæœæœªç€"
            
        results.append(row)
        
    df_res = pd.DataFrame(results)
    df_tickets = pd.DataFrame(ticket_rows)
    
    stats = {
        'total_races': len(df_res),
        'total_invest': total_invest,
        'total_return': total_return,
        'balance': total_return - total_invest,
        'recovery_rate': (total_return / total_invest * 100) if total_invest > 0 else 0.0,
        'hit_count': int(df_res['is_hit'].sum()) if not df_res.empty else 0,
        'hit_rate': (df_res['is_hit'].sum() / len(df_res) * 100) if not df_res.empty else 0.0
    }
    
    return df_res, stats, df_tickets

# ==========================================
# 6. Line Strategy Analysis Logic
# ==========================================
def analyze_line_strategy_bias(history_data, db_path=db_utils.DB_PATH):
    """
    Analyze if AI predictions and Actual Results favor 'Same Line' or 'Separate Line' (Suji-chigai).
    Focus on 1st-2nd place relationship (2-Shatan, 3-Rentan 1st-2nd).
    """
    if not history_data:
        return {}

    # 1. Identify Target IDs
    target_ids = []
    for h in history_data:
        r_num = str(h.get('race_num','')).replace('R','') + 'R'
        rid = f"{h.get('place')}_{h.get('date')}_{r_num}"
        target_ids.append(rid)
    
    target_ids = list(set(target_ids))
    if not target_ids:
        return {}

    # 2. Load DB Results (including Line info)
    conn = sqlite3.connect(db_path)
    df_db_list = []
    try:
        chunk_size = 900
        for i in range(0, len(target_ids), chunk_size):
            chunk = target_ids[i:i+chunk_size]
            placeholders = ','.join(['?'] * len(chunk))
            # Fetch 'ãƒ©ã‚¤ãƒ³' column
            query = f"""
            SELECT race_id, ç€é †, è»Šç•ª, ãƒ©ã‚¤ãƒ³
            FROM race_result
            WHERE race_id IN ({placeholders})
            """
            try:
                chunk_df = pd.read_sql(query, conn, params=chunk)
                if not chunk_df.empty:
                    df_db_list.append(chunk_df)
            except Exception as e:
                pass # Probably 'ãƒ©ã‚¤ãƒ³' column missing if old DB schema

        if df_db_list:
            df_db = pd.concat(df_db_list, ignore_index=True)
        else:
            return {}
            
    except Exception as e:
        print(f"Line Analysis DB Error: {e}")
        return {}
    finally:
        conn.close()
        
    if df_db.empty or 'ãƒ©ã‚¤ãƒ³' not in df_db.columns:
        return {}

    # 3. Helper: Parse Line String "123 456" -> [[1,2,3], [4,5,6]]
    def parse_line_str(l_str):
        if not l_str or pd.isna(l_str): return []
        # Remove parentheses if any
        l_str = str(l_str)
        groups = []
        # usually space separated
        parts = l_str.split()
        for p in parts:
            # Extract digits using regex to avoid noise
            import re
            digits = [int(c) for c in re.findall(r'\d', p)]
            if digits:
                groups.append(digits)
        return groups

    def is_same_line(c1, c2, line_groups):
        if not line_groups: return False
        for g in line_groups:
            if c1 in g and c2 in g:
                return True
        return False

    # 4. Process History
    stats = {
        'total_races': 0,
        'ai_same_line': 0, 'ai_separate': 0,
        'res_same_line': 0, 'res_separate': 0,
        'ai_same_line_hit': 0, 'ai_separate_hit': 0
    }
    
    # Pre-process DB into Map
    race_map = {}
    
    def clean_rank_local(x):
        try: return int(float(str(x).replace('ç€','').replace('éƒ¨',''))) 
        except: return 99
        
    df_db['rank_val'] = df_db['ç€é †'].apply(clean_rank_local)
    
    for rid, grp in df_db.groupby('race_id'):
        # Get Line Info (first row)
        l_str = grp.iloc[0]['ãƒ©ã‚¤ãƒ³']
        l_groups = parse_line_str(l_str)
        
        sorted_grp = grp.sort_values('rank_val')
        valid = sorted_grp[sorted_grp['rank_val'] <= 2]
        
        res_pair = None
        if len(valid) >= 2:
            try:
                r1 = int(valid.iloc[0]['è»Šç•ª'])
                r2 = int(valid.iloc[1]['è»Šç•ª'])
                res_pair = (r1, r2)
            except: pass
            
        race_map[rid] = {'lines': l_groups, 'result': res_pair}

    for h in history_data:
        r_num = str(h.get('race_num','')).replace('R','') + 'R'
        rid = f"{h.get('place')}_{h.get('date')}_{r_num}"
        
        if rid not in race_map: continue
        
        r_info = race_map[rid]
        lines = r_info['lines']
        res_pair = r_info['result']
        
        if not lines: continue # Can't analyze without lines
        
        # Skip pending races (avoid counting as miss)
        if not res_pair: continue
        
        stats['total_races'] += 1
        
        # Analyze Result Bias
        res_is_same = False
        if res_pair:
            if is_same_line(res_pair[0], res_pair[1], lines):
                stats['res_same_line'] += 1
                res_is_same = True
            else:
                stats['res_separate'] += 1
        
        # Analyze AI Bias (Check tickets)
        tickets = h.get('tickets', [])
        
        # Determine AI Dominant Strategy for this race
        # By sampling predicted pairs
        predicted_pairs = set()
        
        import re
        for t in tickets:
            # Simple heuristic parsing: "2è»Šå˜: 1 â†’ 2"
            body = t.split(':')[-1].strip()
            # Split by arrow or hyphen
            if 'â†’' in body: parts = body.split('â†’')
            elif '-' in body: parts = body.split('-')
            else: parts = []
            
            if len(parts) >= 2:
                p1_str = parts[0]
                p2_str = parts[1]
                
                def expand(s):
                    return [int(x) for x in re.findall(r'\d+', s)]
                
                g1 = expand(p1_str)
                g2 = expand(p2_str)
                
                for c1 in g1:
                    for c2 in g2:
                        if c1 != c2: predicted_pairs.add((c1, c2))
        
        if predicted_pairs:
            same_cnt = 0
            sep_cnt = 0
            for (c1, c2) in predicted_pairs:
                if is_same_line(c1, c2, lines):
                    same_cnt += 1
                else:
                    sep_cnt += 1
            
            # Majority Vote for "Did AI bet Same Line or Separate?"
            if same_cnt >= sep_cnt and same_cnt > 0:
                stats['ai_same_line'] += 1
                if res_is_same: stats['ai_same_line_hit'] += 1
            elif sep_cnt > same_cnt:
                stats['ai_separate'] += 1
                if not res_is_same and res_pair: stats['ai_separate_hit'] += 1

    return stats

# ==========================================
# 7. AI Score Analysis Logic
# ==========================================
def analyze_ai_score_performance(history_data, db_path=db_utils.DB_PATH):
    """
    Analyze performance of AI Top Score Player.
    - Win Rate / Ren-tai Rate of Top AI Check
    - Relation between Top AI Rank and Competition Score Rank
    - Impact of Score Gap (1st vs 2nd) on Win Rate
    """
    if not history_data:
        return {}

    # 1. Identify Target IDs
    target_ids = []
    for h in history_data:
        r_num = str(h.get('race_num','')).replace('R','') + 'R'
        rid = f"{h.get('place')}_{h.get('date')}_{r_num}"
        target_ids.append(rid)
    
    target_ids = list(set(target_ids))
    if not target_ids:
        return {}

    # 2. Load DB Results (including Competition Score if available? No, scraper saves it but DB schema might not have it in race_result)
    # Actually scraper saves 'ç«¶èµ°å¾—ç‚¹' in race_result table?
    # Let's check schema by assuming it's there or fetched separately.
    # Logic v2 `load_and_process_data` loads 'ç«¶èµ°å¾—ç‚¹' from `race_result` (cols usually ç«¶èµ°å¾—ç‚¹).
    # We need to fetch 'è»Šç•ª', 'ç€é †', 'ç«¶èµ°å¾—ç‚¹' for all players in the race to rank them.
    
    conn = sqlite3.connect(db_path)
    df_db_list = []
    try:
        chunk_size = 900
        for i in range(0, len(target_ids), chunk_size):
            chunk = target_ids[i:i+chunk_size]
            placeholders = ','.join(['?'] * len(chunk))
            
            # Use limited columns to be safe, assuming 'ç«¶èµ°å¾—ç‚¹' exists
            # Note: DB column names can be tricky. existing logic uses 'ç«¶èµ°å¾—ç‚¹'.
            query = f"""
            SELECT race_id, ç€é †, è»Šç•ª, ç«¶èµ°å¾—ç‚¹
            FROM race_result
            WHERE race_id IN ({placeholders})
            """
            try:
                chunk_df = pd.read_sql(query, conn, params=chunk)
                if not chunk_df.empty:
                    df_db_list.append(chunk_df)
            except:
                pass 

        if df_db_list:
            df_db = pd.concat(df_db_list, ignore_index=True)
        else:
            return {}
            
    except Exception as e:
        print(f"Score Analysis DB Error: {e}")
        conn.close()
        return {}
    # Note: conn stays open for bonus calculation later
        
    if df_db.empty or 'ç«¶èµ°å¾—ç‚¹' not in df_db.columns:
        return {}

    # 3. Process
    
    # Clean Data
    def clean_rank_local(x):
        try: return int(float(str(x).replace('ç€','').replace('éƒ¨',''))) 
        except: return 99
    
    def clean_score_local(x):
        try: return float(x)
        except: return 0.0

    df_db['rank_val'] = df_db['ç€é †'].apply(clean_rank_local)
    df_db['score_val'] = df_db['ç«¶èµ°å¾—ç‚¹'].apply(clean_score_local)
    df_db['car_num'] = pd.to_numeric(df_db['è»Šç•ª'], errors='coerce').fillna(0).astype(int)

    race_db_map = {}
    for rid, grp in df_db.groupby('race_id'):
        # Calculate Competition Score Ranks
        # Sort by score desc
        s_grp = grp.sort_values('score_val', ascending=False).reset_index(drop=True)
        # Map Car Num -> Rank (0-based or 1-based)
        comp_rank_map = {}
        for idx, row in s_grp.iterrows():
            c = row['car_num']
            comp_rank_map[c] = idx + 1 # 1st, 2nd...
            
        # Get Winner
        winner = grp[grp['rank_val'] == 1]['car_num'].values
        winner_car = winner[0] if len(winner) > 0 else None
        
        race_db_map[rid] = {
            'comp_ranks': comp_rank_map,
            'winner': winner_car,
            'top2_cars': grp[grp['rank_val'] <= 2]['car_num'].tolist(),
            'top3_cars': grp[grp['rank_val'] <= 3]['car_num'].tolist()
        }

    stats = {
        'total_races': 0,
        'ai_top_win': 0, 'ai_top_rentai': 0, 'ai_top_fukusho': 0,
        'ai_2nd_win': 0, 'ai_2nd_rentai': 0, 'ai_2nd_fukusho': 0,
        
        # Distribution of Competition Rank for AI Top Pick
        # Key: Rank (1, 2, 3...), Value: Count
        'comp_rank_dist': {},
        
        # Score Gap Analysis
        # List of (Gap, IsWin, IsRentai) tuples
        'gap_data': [],
        
        # Bonus Analysis Data
        'bonus_data': []
    }

    for h in history_data:
        r_num = str(h.get('race_num','')).replace('R','') + 'R'
        rid = f"{h.get('place')}_{h.get('date')}_{r_num}"
        
        if rid not in race_db_map: continue
        
        db_info = race_db_map[rid]
        
        # Skip if Pending (No Winner)
        if db_info['winner'] is None: continue
        
        # Parse AI Indices to find AI Top Pick and 2nd Pick
        ai_indices = h.get('ai_indices', [])
        
        if not ai_indices: continue
        
        # ensure numeric
        for item in ai_indices:
            try: item['s'] = float(item.get('final_score', 0))
            except: item['s'] = 0
            
        sorted_ai = sorted(ai_indices, key=lambda x: x['s'], reverse=True)
        
        if not sorted_ai: continue
        
        # AI Top Pick
        ai_top_car = int(sorted_ai[0].get('è»Šç•ª', 0))
        ai_top_score = sorted_ai[0]['s']
        
        # AI 2nd Pick (for gap)
        ai_2nd_car = -1
        ai_2nd_score = 0
        if len(sorted_ai) > 1:
            ai_2nd_car = int(sorted_ai[1].get('è»Šç•ª', 0))
            ai_2nd_score = sorted_ai[1]['s']
            
        gap = ai_top_score - ai_2nd_score
        
        # Stats Update
        stats['total_races'] += 1
        
        # 1. Performance - Top Pick
        is_win_1 = (ai_top_car == db_info['winner'])
        is_rentai_1 = (ai_top_car in db_info['top2_cars'])
        is_fukusho_1 = (ai_top_car in db_info['top3_cars'])
        
        if is_win_1: stats['ai_top_win'] += 1
        if is_rentai_1: stats['ai_top_rentai'] += 1
        if is_fukusho_1: stats['ai_top_fukusho'] += 1

        # 1b. Performance - 2nd Pick
        if ai_2nd_car != -1:
            if ai_2nd_car == db_info['winner']: stats['ai_2nd_win'] += 1
            if ai_2nd_car in db_info['top2_cars']: stats['ai_2nd_rentai'] += 1
            if ai_2nd_car in db_info['top3_cars']: stats['ai_2nd_fukusho'] += 1
        
        # 2. Comp Rank
        c_rank = db_info['comp_ranks'].get(ai_top_car, 99)
        stats['comp_rank_dist'][c_rank] = stats['comp_rank_dist'].get(c_rank, 0) + 1
        
        # 3. Gap Data
        stats['gap_data'].append({'gap': gap, 'is_win': is_win_1, 'is_rentai': is_rentai_1, 'is_fukusho': is_fukusho_1})
        
        # 4. Bonus Analysis - Recalculate bonus for this race
        # Load race data and recalculate
        try:
            query_race = "SELECT * FROM race_result WHERE race_id = ?"
            df_race = pd.read_sql(query_race, conn, params=[rid])
            if not df_race.empty:
                df_scored = calculate_ai_score(df_race)
                if 'base_score' in df_scored.columns and 'ai_score' in df_scored.columns:
                    df_scored['bonus'] = df_scored['ai_score'] - df_scored['base_score']
                    # Safe rank calculation - handle NaN
                    df_scored['comp_rank'] = df_scored['base_score'].rank(ascending=False, method='min')
                    df_scored['comp_rank'] = df_scored['comp_rank'].fillna(99).astype(int)
                    
                    # Find max bonus player
                    df_sorted = df_scored.sort_values('bonus', ascending=False)
                    top_bonus_rec = df_sorted.iloc[0]
                    max_bonus = top_bonus_rec['bonus']
                    bonus_player_rank = top_bonus_rec['comp_rank']
                    
                    # Safe car number conversion
                    try:
                        bonus_player_car = int(float(str(top_bonus_rec['è»Šç•ª']).replace('nan','0')))
                    except:
                        bonus_player_car = 0
                    
                    # Skip if NaN values
                    if pd.isna(max_bonus) or pd.isna(bonus_player_rank):
                        pass
                    else:
                        # Check result
                        def clean_rank_bonus(x):
                            try: return int(float(str(x).replace('ç€','').replace('éƒ¨',''))) 
                            except: return 99
                        
                        finish_rank = clean_rank_bonus(top_bonus_rec['ç€é †'])
                        
                        stats['bonus_data'].append({
                            'bonus': max_bonus,
                            'comp_rank': int(bonus_player_rank),
                            'is_win': 1 if finish_rank == 1 else 0,
                            'is_rentai': 1 if finish_rank <= 2 else 0,
                            'is_fukusho': 1 if finish_rank <= 3 else 0
                        })
        except:
            pass

    conn.close()
    return stats

